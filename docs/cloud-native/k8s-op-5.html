<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Kubernetes operation and maintenance records (5) - English version of the website https://www.backendcloud.cn/</title>


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../style.css">

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');
                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }
                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../SUMMARY.html"><strong aria-hidden="true">1.</strong> Catalog: Cloud Native</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../cloud-native/k3s-install.html"><strong aria-hidden="true">1.1.</strong> k3s deployment and simple use and Longhorn deployment and use</a></li><li class="chapter-item expanded "><a href="../cloud-native/k8s-op-5.html" class="active"><strong aria-hidden="true">1.2.</strong> Kubernetes operation and maintenance records (5)</a></li><li class="chapter-item expanded "><a href="../cloud-native/k8s-op-4.html"><strong aria-hidden="true">1.3.</strong> Kubernetes operation and maintenance records (4)</a></li><li class="chapter-item expanded "><a href="../cloud-native/k8s-op-3.html"><strong aria-hidden="true">1.4.</strong> Kubernetes operation and maintenance records (3)</a></li><li class="chapter-item expanded "><a href="../cloud-native/k8s-op-2.html"><strong aria-hidden="true">1.5.</strong> Kubernetes operation and maintenance records (2)</a></li><li class="chapter-item expanded "><a href="../cloud-native/k8s-op-1.html"><strong aria-hidden="true">1.6.</strong> Kubernetes operation and maintenance records (1)</a></li><li class="chapter-item expanded "><a href="../cloud-native/docker-storage-driver.html"><strong aria-hidden="true">1.7.</strong> Docker Storage Driver - Overlay2</a></li><li class="chapter-item expanded "><a href="../cloud-native/k8s-service.html"><strong aria-hidden="true">1.8.</strong> Kubernetes Service</a></li><li class="chapter-item expanded "><a href="../cloud-native/cilium-replace-kube-proxy.html"><strong aria-hidden="true">1.9.</strong> Cilium completely replaces kube-proxy</a></li><li class="chapter-item expanded "><a href="../cloud-native/cilium-install.html"><strong aria-hidden="true">1.10.</strong> Cilium install</a></li><li class="chapter-item expanded "><a href="../cloud-native/kubevirtci-for-chinanet.html"><strong aria-hidden="true">1.11.</strong> KubeVirt CI adapts to China's network</a></li><li class="chapter-item expanded "><a href="../cloud-native/web-terminal.html"><strong aria-hidden="true">1.12.</strong> Web Terminal</a></li><li class="chapter-item expanded "><a href="../cloud-native/deploy-kubevirt.html"><strong aria-hidden="true">1.13.</strong> Deploy Kubernetes + KubeVirt and the basic use of KubeVirt</a></li><li class="chapter-item expanded "><a href="../cloud-native/docker-java-demo.html"><strong aria-hidden="true">1.14.</strong> docker hello-world project</a></li><li class="chapter-item expanded "><a href="../cloud-native/deploy-cinder-csi-plugin.html"><strong aria-hidden="true">1.15.</strong> Several problems encountered in deploying cinder-csi-plugin</a></li><li class="chapter-item expanded "><a href="../cloud-native/go1.18-workspace.html"><strong aria-hidden="true">1.16.</strong> New in Go 1.18 - Workspaces</a></li><li class="chapter-item expanded "><a href="../cloud-native/spring-boot-async-demo.html"><strong aria-hidden="true">1.17.</strong> spring-boot asynchronous example</a></li><li class="chapter-item expanded "><a href="../cloud-native/plg.html"><strong aria-hidden="true">1.18.</strong> PLG implements Kubernetes Pod log collection and display</a></li><li class="chapter-item expanded "><a href="../cloud-native/fluentd.html"><strong aria-hidden="true">1.19.</strong> Fluentd implements Kubernetes Pod log collection</a></li><li class="chapter-item expanded "><a href="../cloud-native/java-dev-memo-1.html"><strong aria-hidden="true">1.20.</strong> Little records in Java project development (1)</a></li><li class="chapter-item expanded "><a href="../cloud-native/jdk-bug.html"><strong aria-hidden="true">1.21.</strong> JDK bug - Encrypt Private Key failed unrecognized algorithm name PBEWithSHA1AndDESede</a></li><li class="chapter-item expanded "><a href="../cloud-native/tenant-cmp.html"><strong aria-hidden="true">1.22.</strong> Implementation of the function of creating resource pool tenants on the cloud management platform</a></li><li class="chapter-item expanded "><a href="../cloud-native/test-service-performance-2.html"><strong aria-hidden="true">1.23.</strong> iperf3 tests the four-layer performance of Kubernetes Service (Part 2)</a></li><li class="chapter-item expanded "><a href="../cloud-native/test-service-performance-1.html"><strong aria-hidden="true">1.24.</strong> iperf3 tests the four-layer performance of Kubernetes Service (Part 1)</a></li><li class="chapter-item expanded "><a href="../cloud-native/k8s-cmd.html"><strong aria-hidden="true">1.25.</strong> k8s common command</a></li><li class="chapter-item expanded "><a href="../cloud-native/cpu-binding.html"><strong aria-hidden="true">1.26.</strong> k8s supports container core binding</a></li><li class="chapter-item expanded "><a href="../cloud-native/capability.html"><strong aria-hidden="true">1.27.</strong> k8s supports Capability mechanism</a></li><li class="chapter-item expanded "><a href="../cloud-native/oom.html"><strong aria-hidden="true">1.28.</strong> k8s OOMkilled container exceeding memory limit</a></li></ol></li><li class="chapter-item expanded "><a href="../SUMMARY.html"><strong aria-hidden="true">2.</strong> Catalog: Openstack</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../openstack/deploy-openstack-stein.html"><strong aria-hidden="true">2.1.</strong> Problems encountered in Openstack Stein deployment</a></li><li class="chapter-item expanded "><a href="../openstack/sriov2ovs.html"><strong aria-hidden="true">2.2.</strong> sriov computing node to ovs computing node script</a></li><li class="chapter-item expanded "><a href="../openstack/placement.html"><strong aria-hidden="true">2.3.</strong> Openstack Placement</a></li><li class="chapter-item expanded "><a href="../openstack/power-arch.html"><strong aria-hidden="true">2.4.</strong> POWER architecture server as computing node</a></li><li class="chapter-item expanded "><a href="../openstack/sriov.html"><strong aria-hidden="true">2.5.</strong> OpenStack practices SR-IOV computing nodes</a></li><li class="chapter-item expanded "><a href="../openstack/openstack-op-5.html"><strong aria-hidden="true">2.6.</strong> Openstack operation and maintenance records (5)</a></li><li class="chapter-item expanded "><a href="../openstack/openstack-op-4.html"><strong aria-hidden="true">2.7.</strong> Openstack operation and maintenance records (4)</a></li><li class="chapter-item expanded "><a href="../openstack/openstack-op-3.html"><strong aria-hidden="true">2.8.</strong> Openstack operation and maintenance records (3)</a></li><li class="chapter-item expanded "><a href="../openstack/openstack-op-2.html"><strong aria-hidden="true">2.9.</strong> Openstack operation and maintenance records (2)</a></li><li class="chapter-item expanded "><a href="../openstack/openstack-op-1.html"><strong aria-hidden="true">2.10.</strong> Openstack operation and maintenance records (1)</a></li><li class="chapter-item expanded "><a href="../openstack/novnc-problem.html"><strong aria-hidden="true">2.11.</strong> OpenStack Pike dashboard noVNC cannot be accessed problem</a></li><li class="chapter-item expanded "><a href="../openstack/openstack-local-yum.html"><strong aria-hidden="true">2.12.</strong> Openstack Pike local yum source construction</a></li><li class="chapter-item expanded "><a href="../openstack/cpu-binding.html"><strong aria-hidden="true">2.13.</strong> CPU binding</a></li><li class="chapter-item expanded "><a href="../openstack/resize-fail.html"><strong aria-hidden="true">2.14.</strong> Investigation of the cause of resize failure</a></li><li class="chapter-item expanded "><a href="../openstack/mount-cloud-disk.html"><strong aria-hidden="true">2.15.</strong> Mount cloud disk</a></li><li class="chapter-item expanded "><a href="../openstack/ocata-nova-evacuate-bug.html"><strong aria-hidden="true">2.16.</strong> Ocata nova evacuate bug</a></li><li class="chapter-item expanded "><a href="../openstack/compute-node-ha.html"><strong aria-hidden="true">2.17.</strong> compute node ha mainstream open source implementation</a></li><li class="chapter-item expanded "><a href="../openstack/collectd-influxdb.html"><strong aria-hidden="true">2.18.</strong> Deployment and usage of Collectd and InfluxDB</a></li><li class="chapter-item expanded "><a href="../openstack/live-migration-local.html"><strong aria-hidden="true">2.19.</strong> Live Migration on Local Storage</a></li><li class="chapter-item expanded "><a href="../openstack/fast-evacuation.html"><strong aria-hidden="true">2.20.</strong> Add fast evacuation function to VM HA procedure</a></li><li class="chapter-item expanded "><a href="../openstack/vm-activation-failure.html"><strong aria-hidden="true">2.21.</strong> Windows virtual machine activation failure problem</a></li></ol></li><li class="chapter-item expanded "><a href="../SUMMARY.html"><strong aria-hidden="true">3.</strong> Catalog: Point of view</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../point-of-view/huawei-and-operators.html"><strong aria-hidden="true">3.1.</strong> Huawei and telecom operators</a></li></ol></li><li class="chapter-item expanded "><a href="../SUMMARY.html"><strong aria-hidden="true">4.</strong> Open source small project</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../open-source-small-project/firewall-tool.html"><strong aria-hidden="true">4.1.</strong> Open source a node.js firewall tool</a></li></ol></li><li class="chapter-item expanded "><a href="../SUMMARY.html"><strong aria-hidden="true">5.</strong> Catalog: Tools</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../tools/github-action.html"><strong aria-hidden="true">5.1.</strong> Github Action Supplementary Introduction</a></li><li class="chapter-item expanded "><a href="../tools/blog-cicd.html"><strong aria-hidden="true">5.2.</strong> Github Action Upgrade backend cloud website CICD process</a></li><li class="chapter-item expanded "><a href="../tools/github-action-demo.html"><strong aria-hidden="true">5.3.</strong> Try the Github Action CI/CD process (create a React project and package it for deployment)</a></li><li class="chapter-item expanded "><a href="../tools/notion-widget.html"><strong aria-hidden="true">5.4.</strong> Notion's page insert widget Widget</a></li><li class="chapter-item expanded "><a href="../tools/git-dir.html"><strong aria-hidden="true">5.5.</strong> Internal structure of .git directory</a></li><li class="chapter-item expanded "><a href="../tools/xshell-skill.html"><strong aria-hidden="true">5.6.</strong> Some Skills of using xshell tools</a></li><li class="chapter-item expanded "><a href="../tools/gerrit-install.html"><strong aria-hidden="true">5.7.</strong> gerrit install</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">English version of the website https://www.backendcloud.cn/</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <!-- Page table of contents -->
                        <div class="sidetoc"><nav class="pagetoc"></nav></div>

                        <p>release time :2023-01-11 13:01</p>
<h1 id="reasonably-set-request-and-limit"><a class="header" href="#reasonably-set-request-and-limit">Reasonably set Request and Limit</a></h1>
<p>All containers should set request</p>
<p>The value of request does not refer to the actual resource size allocated to the container, it is only for the scheduler to see, the scheduler will &quot;observe&quot; how many resources each node can allocate, and know that each node has been allocated How many resources. The size of the allocated resources is the sum of the container requests defined in all Pods on the node, which can calculate how much resources the node has left that can be allocated (allocated resources minus the sum of allocated requests). If it is found that the size of the remaining allocatable resources of the node is smaller than the reuqest of the currently scheduled Pod, then this node will not be considered for scheduling, otherwise, scheduling is possible. Therefore, if the request is not configured, the scheduler cannot know how many resources are allocated to the node, and the scheduler cannot obtain accurate information, so it cannot make reasonable scheduling decisions, which can easily cause unreasonable scheduling, and some nodes may Very idle, while some nodes may be very busy, even NotReady.</p>
<p>When the node resources are insufficient, automatic eviction will be triggered, and some low-priority Pods will be deleted to release resources and allow the node to heal itself. Pods without request and limit have the lowest priority and are easy to be evicted; those with request not equal to limit have the second priority; Pods with request equal to limit have higher priority and are not easy to be evicted. Therefore, if it is an important online application and you do not want to be evicted when the node fails and the online business will be affected, it is recommended to set the request and limit to be consistent.</p>
<p>Therefore, the suggestion is to set a request for all containers, so that the scheduler can perceive how many resources are allocated to the nodes, so as to make reasonable scheduling decisions, so that the resources of the cluster nodes can be allocated and used reasonably, and avoid falling into the situation of uneven resource allocation. Some accidents happen.</p>
<p>Sometimes we forget to set request and limit for some containers. In fact, we can use LimitRange to set the default request and limit value of namespace, and it can also be used to limit the minimum and maximum request and limit. Example:</p>
<pre><code>apiVersion: v1
kind: LimitRange
metadata:
name: mem-limit-range
namespace: test
spec:
limits:
- default:
    memory: 512Mi
    cpu: 500m
    defaultRequest:
    memory: 256Mi
    cpu: 100m
    type: Container
</code></pre>
<p>Try to avoid using too large request and limit</p>
<p>If your service uses a single copy or a small number of copies, give a large request and limit, and let it allocate enough resources to support the business, then a copy failure may have a greater impact on the business, and due to the request Larger, when the resource allocation in the cluster is relatively fragmented, if the node where the Pod is located hangs up, and none of the other nodes has enough remaining allocable resources to meet the request of the Pod, the Pod cannot drift, and it cannot Self-healing, aggravating the impact on the business.</p>
<p>On the contrary, it is recommended to reduce the request and limit as much as possible, and expand your service support capacity horizontally by adding copies to make your system more flexible and reliable.</p>
<p>If the production cluster has a namespace for testing, if it is not restricted, the cluster load may be too high, thereby affecting the production business. You can use ResourceQuota to limit the total size of the request and limit of the test namespace. Example:</p>
<pre><code>apiVersion: v1
kind: ResourceQuota
metadata:
name: quota-test
namespace: test
spec:
hard:
    requests.cpu: &quot;1&quot;
    requests.memory: 1Gi
    limits.cpu: &quot;2&quot;
    limits.memory: 2Gi
</code></pre>
<h1 id="pod-cpu-binding-core-and-binding-numa-affinity"><a class="header" href="#pod-cpu-binding-core-and-binding-numa-affinity">pod cpu binding core and binding numa affinity</a></h1>
<pre><code># Evict nodes
kubectl drain &lt;NODE_NAME&gt;
# stop kubelet
systemctl stop kubelet
# Modify kubelet parameters
# If you only need pod cpu binding core, just configure the following parameter
--cpu-manager-policy=static
# If you bind numa affinity, you need to configure the following two parameters
--cpu-manager-policy=static
--topology-manager-policy=single-numa-node
# delete old CPU manager state files
rm var/lib/kubelet/cpu_manager_state
# start kubelet
systemctl start kubelet
</code></pre>
<p>When creating a pod, the cpu request is strictly consistent with the limit.</p>
<p>Verification can be done by creating a process in the pod, so that all the CPUs of the pod can be used at 100%. Then log in to the node where the pod is located, and check it with cpustats. The CPUs with 100% CPU load are distributed on the same numa.</p>
<p>If you continue to create a pod, even if the CPU resources are sufficient, if the numa resources are not enough (that is, no numa can meet the number of CPUs required by the pod), the pod cannot be started, and the status will change to TopologyAffinityError.</p>
<h1 id="for-high-concurrency-scenarios-expand-the-range-of-source-ports"><a class="header" href="#for-high-concurrency-scenarios-expand-the-range-of-source-ports">For high concurrency scenarios, expand the range of source ports</a></h1>
<p>In high-concurrency scenarios, a large number of source ports will be used for the client. The source port range is randomly selected from the interval defined in the kernel parameter net.ipv4.ip_local_port_range. In a high-concurrency environment, a small port range will easily lead to exhaustion of source ports, making Some connections are abnormal. Usually, the Pod source port range is 32768-60999 by default. It is recommended to expand it to 1024-65535: sysctl -w net.ipv4.ip_local_port_range=&quot;1024 65535&quot;.</p>
<p>Two versions:</p>
<ol>
<li>Most articles say that this value determines the number of ports available for one IP of the client, that is, one IP can only create a little more than 60K connections (1025-65535), if you want to break through this limit, you need to bind multiple IPs to the client machine .</li>
<li>Some articles also said that this value determines the number of local ports in the socket quadruple, that is, one ip can create a maximum of 60K connections to the same target ip+port, as long as the target ip or port is different, it can be used The same local port does not necessarily require multiple client ips to break through the limit on the number of ports.</li>
</ol>
<p>Verification experiment:</p>
<p>First set the value of ip_local_port_range to a very small range:</p>
<pre><code>$ echo &quot;61000 61001&quot; | sudo tee /proc/sys/net/ipv4/ip_local_port_range
61000 61001

$ cat /proc/sys/net/ipv4/ip_local_port_range
61000       61001
</code></pre>
<p><em>Experiment 1: Limit the number of ports under the same target ip and same target port</em></p>
<pre><code>$ nohup nc 123.125.114.144 80 -v &amp;
[1] 16196
$ nohup: ignoring input and appending output to 'nohup.out'

$ nohup nc 123.125.114.144 80 -v &amp;
[2] 16197
$ nohup: ignoring input and appending output to 'nohup.out'

$ ss -ant |grep 10.0.2.15:61
ESTAB   0        0                10.0.2.15:61001       123.125.114.144:80
ESTAB   0        0                10.0.2.15:61000       123.125.114.144:80


$ nc 123.125.114.144 80 -v
nc: connect to 123.125.114.144 port 80 (tcp) failed: Cannot assign requested address
</code></pre>
<p><em>Experiment 2: Same target ip, different target ports</em></p>
<pre><code>$ nohup nc 123.125.114.144 443 -v &amp;
[3] 16215
$ nohup: ignoring input and appending output to 'nohup.out'

$ nohup nc 123.125.114.144 443 -v &amp;
[4] 16216
$ nohup: ignoring input and appending output to 'nohup.out'

$ ss -ant |grep 10.0.2.15:61
ESTAB   0        0                10.0.2.15:61001       123.125.114.144:443
ESTAB   0        0                10.0.2.15:61001       123.125.114.144:80
ESTAB   0        0                10.0.2.15:61000       123.125.114.144:443
ESTAB   0        0                10.0.2.15:61000       123.125.114.144:80

$ nc 123.125.114.144 443 -v
nc: connect to 123.125.114.144 port 443 (tcp) failed: Cannot assign requested address
</code></pre>
<p><em>Experiment 3: multiple target ip same target port</em></p>
<pre><code>$ nohup nc 220.181.57.216 80 -v &amp;
[5] 16222
$ nohup: ignoring input and appending output to 'nohup.out'

$ nohup nc 220.181.57.216 80 -v &amp;
[6] 16223
$ nohup: ignoring input and appending output to 'nohup.out'

$ nc 220.181.57.216 80 -v
nc: connect to 220.181.57.216 port 80 (tcp) failed: Cannot assign requested address

$ ss -ant |grep :80
SYN-SENT  0        1               10.0.2.15:61001       220.181.57.216:80
SYN-SENT  0        1               10.0.2.15:61000       220.181.57.216:80
SYN-SENT  0        1               10.0.2.15:61001      123.125.114.144:80
SYN-SENT  0        1               10.0.2.15:61000      123.125.114.144:80
</code></pre>
<p><em>Experiment 4: multiple target ip different target ports</em></p>
<pre><code>$ nohup nc 123.125.114.144 80 -v &amp;

$ nohup nc 123.125.114.144 80 -v &amp;

$ nc 123.125.114.144 80 -v
nc: connect to 123.125.114.144 port 80 (tcp) failed: Cannot assign requested address

$ nohup nc 123.125.114.144 443 -v &amp;

$ nohup nc 123.125.114.144 443 -v &amp;

$ nc 123.125.114.144 443 -v
nc: connect to 123.125.114.144 port 443 (tcp) failed: Cannot assign requested address

$ nohup nc 220.181.57.216 80 -v &amp;

$ nohup nc 220.181.57.216 80 -v &amp;

$ nc 220.181.57.216 80 -v
nc: connect to 220.181.57.216 port 80 (tcp) failed: Cannot assign requested address

$ nohup nc 220.181.57.216 443 -v &amp;

$ nohup nc 220.181.57.216 443 -v &amp;

$ nc 220.181.57.216 443 -v
nc: connect to 220.181.57.216 port 443 (tcp) failed: Cannot assign requested address

$ ss -ant |grep 10.0.2.15:61
SYN-SENT  0        1               10.0.2.15:61001       220.181.57.216:80
ESTAB     0        0               10.0.2.15:61001      123.125.114.144:443
ESTAB     0        0               10.0.2.15:61000       220.181.57.216:443
SYN-SENT  0        1               10.0.2.15:61000       220.181.57.216:80
SYN-SENT  0        1               10.0.2.15:61001      123.125.114.144:80
ESTAB     0        0               10.0.2.15:61000      123.125.114.144:443
SYN-SENT  0        1               10.0.2.15:61000      123.125.114.144:80
ESTAB     0        0               10.0.2.15:61001       220.181.57.216:443
</code></pre>
<p><em>Summarize</em></p>
<p>So can it be said that the first statement in the preface is wrong? After checking the information, it cannot be said that the first statement is wrong:</p>
<ol>
<li>The first statement is correct when the system's kernel version is less than 3.2</li>
<li>When the kernel version of the system is greater than or equal to 3.2, the second statement is correct</li>
</ol>
<p>The previous experiments were all operated in an environment with a kernel version greater than 3.2, so the second statement was confirmed.</p>
<h1 id="pod-affinity-and-anti-affinity-pod-topology-distribution-constraints"><a class="header" href="#pod-affinity-and-anti-affinity-pod-topology-distribution-constraints">Pod affinity and anti-affinity, Pod topology distribution constraints</a></h1>
<p>Scattering and scheduling Pods to different places can avoid service unavailability due to factors such as software and hardware failures, fiber failures, power outages, or natural disasters, so as to achieve high-availability deployment of services.</p>
<p>Kubernetes supports two ways to break up pod scheduling:</p>
<ul>
<li>Pod Anti-Affinity</li>
<li>*Pod Topology Spread Constraints</li>
</ul>
<p>topologySpreadConstraints is more powerful than podAntiAffinity and provides finer scheduling control. We can understand that topologySpreadConstraints is an upgraded version of podAntiAffinity.</p>
<p>Force pods to be scattered and scheduled to different nodes (strong anti-affinity) to avoid single point of failure</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
name: nginx
spec:
replicas: 1
selector:
    matchLabels:
    app: nginx
template:
    metadata:
    labels:
        app: nginx
    spec:
    affinity:
        podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: kubernetes.io/hostname
            labelSelector:
            matchLabels:
                app: nginx
    containers:
    - name: nginx
        image: nginx
</code></pre>
<ul>
<li>
<p>labelSelector.matchLabelsReplace it with the label actually used by the selected Pod.</p>
</li>
<li>
<p>topologyKey: The key of a certain label of the node, which can represent the topological domain of the node. Well-Known Labels can be used , and the * commonly used ones are kubernetes.io/hostname(node ​​dimension), topology.kubernetes.io/zone(availability zone/machine room dimension). You can also * manually add a custom label to the node to define the topology domain, such as rack(rack dimension), machine(physical machine dimension), switch(switch * dimension).</p>
</li>
<li>
<p>If you don't want to use coercion, you can use weak anti-affinity to let Pods be scheduled to different nodes as much as possible:</p>
<p>podAntiAffinity:
preferredDuringSchedulingIgnoredDuringExecution:</p>
<ul>
<li>podAffinityTerm:
topologyKey: kubernetes.io/hostname
weight: 100</li>
</ul>
</li>
</ul>
<p>Forcibly disperse and schedule Pods to different availability zones (computer rooms) to achieve cross-computer room disaster recovery</p>
<p>kubernetes.io/hostnameReplace topology.kubernetes.io/zonewith , and the rest are the same as above.</p>
<p>Disperse and schedule Pods to each node as evenly as possible</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
name: nginx
spec:
replicas: 1
selector:
    matchLabels:
    app: nginx
template:
    metadata:
    labels:
        app: nginx
    spec:
    topologySpreadConstraints:
    - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
        - matchLabels:
            app: nginx
    containers:
    - name: nginx
        image: nginx
</code></pre>
<ul>
<li>topologyKey: Similar to the configuration in podAntiAffinity.</li>
<li>labelSelector: Similar to the configuration in podAntiAffinity, except that multiple groups of pod labels can be selected here.</li>
<li>maxSkew: Must be an integer greater than zero, indicating the maximum value that can tolerate the difference in the number of Pods in different topology domains. Here 1 means that only 1 Pod difference is allowed.</li>
<li>whenUnsatisfiable: Indicates what to do when the condition is not met. DoNotScheduleNo scheduling (keep pending), similar to strong anti-affinity; ScheduleAnywaymeans to schedule, similar to weak anti-affinity;</li>
</ul>
<p>The above configurations are combined to explain: All nginx Pods are strictly evenly dispersed and scheduled to different nodes. The number of copies of nginx on different nodes can only differ by 1 at most. If there are nodes that cannot schedule more Pods due to other factors (such as resource Insufficient), then let the remaining nginx copy Pending.</p>
<p>Disperse and schedule Pods on each node as evenly as possible, without forcing (change DoNotSchedule to ScheduleAnyway):</p>
<pre><code>spec:
topologySpreadConstraints:
- maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
    - matchLabels:
        app: nginx
</code></pre>
<p>If the cluster nodes support cross-availability zones, Pods can also be distributed and scheduled to each availability zone as evenly as possible to achieve a higher level of high availability (topologyKey is changed topology.kubernetes.io/zone):</p>
<pre><code>spec:
topologySpreadConstraints:
- maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
    - matchLabels:
        app: nginx
</code></pre>
<p>Furthermore, while pods can be distributed and scheduled to each availability zone as evenly as possible, each node in the availability zone should also be dispersed as much as possible :</p>
<pre><code>spec:
topologySpreadConstraints:
- maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
    - matchLabels:
        app: nginx
- maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
    - matchLabels:
        app: nginx
</code></pre>
<p>References:</p>
<ul>
<li>https://github.com/imroc/kubernetes-guide</li>
<li>https://mozillazg.com/2019/05/linux-what-net.ipv4.ip_local_port_range-effect-or-mean.html</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../cloud-native/k3s-install.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../cloud-native/k8s-op-4.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../cloud-native/k3s-install.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../cloud-native/k8s-op-4.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="../sidebar.js"></script>


    </body>
</html>