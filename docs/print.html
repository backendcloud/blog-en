<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>English version of the website https://www.backendcloud.cn/</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="style.css">
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');
                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }
                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="SUMMARY.html"><strong aria-hidden="true">1.</strong> Openstack</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="openstack/mount-cloud-disk.html"><strong aria-hidden="true">1.1.</strong> Mount cloud disk</a></li><li class="chapter-item expanded "><a href="openstack/ocata-nova-evacuate-bug.html"><strong aria-hidden="true">1.2.</strong> Ocata nova evacuate bug</a></li><li class="chapter-item expanded "><a href="openstack/compute-node-ha.html"><strong aria-hidden="true">1.3.</strong> compute node ha mainstream open source implementation</a></li><li class="chapter-item expanded "><a href="openstack/collectd-influxdb.html"><strong aria-hidden="true">1.4.</strong> Deployment and usage of Collectd and InfluxDB</a></li><li class="chapter-item expanded "><a href="openstack/live-migration-local.html"><strong aria-hidden="true">1.5.</strong> Live Migration on Local Storage</a></li><li class="chapter-item expanded "><a href="openstack/fast-evacuation.html"><strong aria-hidden="true">1.6.</strong> Add fast evacuation function to VM HA procedure</a></li><li class="chapter-item expanded "><a href="openstack/vm-activation-failure.html"><strong aria-hidden="true">1.7.</strong> Windows virtual machine activation failure problem</a></li></ol></li><li class="chapter-item expanded "><a href="SUMMARY.html"><strong aria-hidden="true">2.</strong> Tools</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tools/xshell-skill.html"><strong aria-hidden="true">2.1.</strong> Some Skills of using xshell tools</a></li></ol></li><li class="chapter-item expanded "><a href="SUMMARY.html"><strong aria-hidden="true">3.</strong> point of view</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tools/gerrit-install.html"><strong aria-hidden="true">3.1.</strong> gerrit install</a></li><li class="chapter-item expanded "><a href="point-of-view/huawei-and-operators.html"><strong aria-hidden="true">3.2.</strong> Huawei and telecom operators</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">English version of the website https://www.backendcloud.cn/</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <!-- Page table of contents -->
                        <div class="sidetoc"><nav class="pagetoc"></nav></div>

                        <h1 id="summary"><a class="header" href="#summary">Summary</a></h1>
<ul>
<li>
<p><a href="./SUMMARY.html">Openstack</a></p>
<ul>
<li><a href="./openstack/mount-cloud-disk.html">Mount cloud disk</a></li>
<li><a href="./openstack/ocata-nova-evacuate-bug.html">Ocata nova evacuate bug</a></li>
<li><a href="./openstack/compute-node-ha.html">compute node ha mainstream open source implementation</a></li>
<li><a href="./openstack/collectd-influxdb.html">Deployment and usage of Collectd and InfluxDB</a></li>
<li><a href="./openstack/live-migration-local.html">Live Migration on Local Storage</a></li>
<li><a href="./openstack/fast-evacuation.html">Add fast evacuation function to VM HA procedure</a></li>
<li><a href="./openstack/vm-activation-failure.html">Windows virtual machine activation failure problem</a></li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Tools</a></p>
<ul>
<li><a href="./tools/xshell-skill.html">Some Skills of using xshell tools</a></li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">point of view</a></p>
<ul>
<li><a href="./tools/gerrit-install.html">gerrit install</a></li>
<li><a href="./point-of-view/huawei-and-operators.html">Huawei and telecom operators</a></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-30 05:16</p>
<h1 id="create-order-a-cloud-disk"><a class="header" href="#create-order-a-cloud-disk">Create (order) a cloud disk</a></h1>
<pre><code>[root@NFJD-PSC-BCEC-SV3 deployer]# cinder create 500 --volume-type fujitsu-ipsan --name 0630
+---------------------------------------+--------------------------------------+
|                Property               |                Value                 |
+---------------------------------------+--------------------------------------+
|              attachments              |                  []                  |
|           availability_zone           |                 nova                 |
|                bootable               |                false                 |
|          consistencygroup_id          |                 None                 |
|               created_at              |      2017-06-30T08:23:05.000000      |
|              description              |                 None                 |
|               encrypted               |                False                 |
|                   id                  | 08d3018c-b1cd-4652-87ed-45a48a14b6f3 |
|                metadata               |                  {}                  |
|              multiattach              |                False                 |
|                  name                 |                 0630                 |
|      os-vol-tenant-attr:tenant_id     |   597854e23bfe46abb6178f786af12391   |
|   os-volume-replication:driver_data   |                 None                 |
| os-volume-replication:extended_status |                 None                 |
|           replication_status          |               disabled               |
|                  size                 |                 500                  |
|              snapshot_id              |                 None                 |
|              source_volid             |                 None                 |
|                 status                |               creating               |
|                user_id                |   622684c00210427091c7493f36aaa6cf   |
|              volume_type              |            fujitsu-ipsan             |
+---------------------------------------+--------------------------------------+
</code></pre>
<h1 id="mount-cloud-disk"><a class="header" href="#mount-cloud-disk">Mount cloud disk</a></h1>
<p>Existing cloud host id 27b31829-326f-4029-a537-bb327303a32c</p>
<p>Mount the cloud disk to the cloud host</p>
<pre><code>[root@NFJD-PSC-BCEC-SV3 deployer]# nova volume-attach 27b31829-326f-4029-a537-bb327303a32c 08d3018c-b1cd-4652-87ed-45a48a14b6f3
+----------+--------------------------------------+
| Property | Value                                |
+----------+--------------------------------------+
| device   | /dev/vdb                             |
| id       | 08d3018c-b1cd-4652-87ed-45a48a14b6f3 |
| serverId | 27b31829-326f-4029-a537-bb327303a32c |
| volumeId | 08d3018c-b1cd-4652-87ed-45a48a14b6f3 |
+----------+--------------------------------------+
</code></pre>
<p>Comparing the execution results of the fdisk -l command before and after mounting, it is found that after the cloud disk is mounted, there is an extra vdb</p>
<p><img src="openstack/2023-01-17-13-45-58.png" alt="" /></p>
<h1 id="artition"><a class="header" href="#artition">artition</a></h1>
<p>Run fdisk /dev/vdb to partition the data disk. According to the prompt, enter n, p, 1 in sequence, press Enter twice, wq, and the partition starts.</p>
<p><img src="openstack/2023-01-17-13-46-13.png" alt="" /></p>
<p>Run the fdisk -l command to view the new partition. The new partition vdb1 has been created. Like /dev/xvdb in the example below.</p>
<p><img src="openstack/2023-01-17-13-46-26.png" alt="" /></p>
<h1 id="format"><a class="header" href="#format">format</a></h1>
<p>Run mkfs.ext3 /dev/xvdb1 to format the new partition. The time required for formatting depends on the size of the data disk. You can also decide to choose other file formats, such as ext4, etc.</p>
<p><img src="openstack/2023-01-17-13-46-39.png" alt="" /></p>
<p>Run echo /dev/xvdb1 /mnt ext3 defaults 0 0 &gt;&gt; /etc/fstab to write new partition information. Once complete, you can use the cat /etc/fstab command to view.</p>
<p><img src="openstack/2023-01-17-13-46-51.png" alt="" /></p>
<p>Through the command mount -a, after modifying /etc/fstab, it will take effect without restarting. Then execute df -h to view the partitions. If the data disk information appears, it means that the mount is successful and the new partition can be used.</p>
<p><img src="openstack/2023-01-17-13-47-02.png" alt="" /></p>
<h1 id="linux-uses-the-dd-command-to-quickly-generate-large-files"><a class="header" href="#linux-uses-the-dd-command-to-quickly-generate-large-files">Linux uses the dd command to quickly generate large files</a></h1>
<p><img src="openstack/2023-01-17-13-47-16.png" alt="" /></p>
<p>The dd command can easily create a file of a specified size, such as</p>
<pre><code>dd if=/dev/zero of=test bs=1M count=1000
</code></pre>
<p>A 1000M test file will be generated, and the content of the file is all 0 (because it is read from /dev/zero, /dev/zero is the source of 0)</p>
<p>But this is for actually writing to the hard disk. The speed of file generation depends on the read and write speed of the hard disk. If you want to generate very large files, the speed is very slow.</p>
<p>In a certain scenario, we just want the file system to think that there is a very large file here, but we don’t actually write it to the hard disk</p>
<p>then you can</p>
<pre><code>dd if=/dev/zero of=test bs=1M count=0 seek=100000
</code></pre>
<p>The displayed size of the file created at this time in the file system is 100000MB, but it does not actually occupy a block, so the creation speed is comparable to the memory speed</p>
<p>The function of seek is to skip the part of the specified size in the output file, which achieves the purpose of creating a large file, but not actually writing</p>
<blockquote>
<p>Of course, because you don’t actually write to the hard disk, you can create 100G of such files on a hard disk with a capacity of only 10G.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-28 18:24</p>
<h1 id="phenomenon"><a class="header" href="#phenomenon">Phenomenon</a></h1>
<p>Execute the nova evacuate operation, but there is a problem during the rebuild. After a certain step, the error &quot;rebuild virtual machine has been deleted&quot; is reported.</p>
<h1 id="reason"><a class="header" href="#reason">reason</a></h1>
<p>When an instance is evacuated from one node to another, an InstanceNotFound exception is expected to occur during the execution of the check_instance_exists method during the rebuild process.</p>
<p>The check_instance_exists method is used to ensure that the instance does not exist on the new node, and if it exists, an exception will be thrown.</p>
<p>The check_instance_exists in the actual code reported this exception when it should not report the InstanceNotFound exception, and the exception of the method was not added to capture the InstanceNotFound exception.</p>
<h1 id="community-fixes"><a class="header" href="#community-fixes">community fixes</a></h1>
<p>nova/virt/libvirt/driver.py</p>
<pre><code class="language-python">def instance_exists(self, instance):
    &quot;&quot;&quot;Efficient override of base instance_exists method.&quot;&quot;&quot;
    try:
        self._host.get_guest(instance)
        return True
    except exception.InternalError:
        return False
</code></pre>
<p>change to:</p>
<pre><code class="language-python">def instance_exists(self, instance):
    &quot;&quot;&quot;Efficient override of base instance_exists method.&quot;&quot;&quot;
    try:
        self._host.get_guest(instance)
        return True
    except (exception.InternalError, exception.InstanceNotFound):
        return False
</code></pre>
<h1 id="unit-test"><a class="header" href="#unit-test">unit test</a></h1>
<pre><code class="language-python">@mock.patch.object(host.Host, 'get_guest')
def test_instance_exists(self, mock_get_guest):
    drvr = libvirt_driver.LibvirtDriver(fake.FakeVirtAPI(), False)
    self.assertTrue(drvr.instance_exists(None))

    mock_get_guest.side_effect = exception.InstanceNotFound
    self.assertFalse(drvr.instance_exists(None))

    mock_get_guest.side_effect = exception.InternalError
    self.assertFalse(drvr.instance_exists(None))
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-22 09:16</p>
<h1 id="nova-evacuate"><a class="header" href="#nova-evacuate">nova evacuate</a></h1>
<p>Various implementation schemes basically call nova's evacuate interface without exception.</p>
<p>nova evacuate is very similar to hot migration. They all want to transfer instances from one node to another. The difference is mainly that thermal migration is performed under normal conditions, while evacuation is performed under abnormal conditions. A vivid example is that thermal migration occurs before an earthquake to escape from a building, and evacuation occurs after an earthquake occurs by escaping from a destroyed building.</p>
<p>But strictly speaking, the above analogy is not rigorous enough. To be precise, it is not escaped from the destroyed building, but resurrected.</p>
<p>So the translation of nova evacuate into resurrection is more accurate than evacuation.</p>
<h1 id="masakari"><a class="header" href="#masakari">masakari</a></h1>
<p>NTT NTT's open source project masakari has become an independent project of Openstack, which is dedicated to compute node ha.</p>
<p>The Japanese word for masakari is まさかり, meaning an axe. To be precise, it's an axe. It is the ax used by Zhou Xingchi's movie Ax Gang. It is not known whether the initiators of the project, several Japanese, were affected by this.</p>
<p>The name masakari is to find the location of the fault, use the ax to help the highest stunt, and throw the ax accurately to isolate and cut off the fault.</p>
<h1 id="detect-3-type-of-vm-down"><a class="header" href="#detect-3-type-of-vm-down">detect 3 type of vm down</a></h1>
<ul>
<li>unexpected vm down (monitoring libvirt's events)</li>
<li>vm manager down (monitoring manager process)</li>
<li>host down (using pacemaker)</li>
</ul>
<h1 id="new-features-that-masakari-is-working-on"><a class="header" href="#new-features-that-masakari-is-working-on">New features that masakari is working on</a></h1>
<ol>
<li>
<p>Now it only supports evacuation of vm in active, stopped, and error states.
Later, it can be added to evacuate vm in more states, such as shelved, rescued, paused and suspended</p>
</li>
<li>
<p>Now the process after the fault is detected is fixed, and the only way to change it is to change the code.
In the future, users can customize the process by writing yaml files.</p>
</li>
</ol>
<h1 id="compute-node-ha-other-technologies-used"><a class="header" href="#compute-node-ha-other-technologies-used">compute node ha other technologies used</a></h1>
<ul>
<li>consul</li>
<li>raft</li>
<li>gossip</li>
</ul>
<h1 id="compute-node-ha-other-related-open-source-projects"><a class="header" href="#compute-node-ha-other-related-open-source-projects">compute node ha other related open source projects</a></h1>
<ul>
<li>Openstack Congress (Policy as a Service)</li>
<li>pacemaker-remote (host monitor solution)</li>
<li>mistral-evacuate (workflow)</li>
</ul>
<h1 id="appendix-1-gossip-protocol"><a class="header" href="#appendix-1-gossip-protocol">Appendix 1: Gossip protocol</a></h1>
<p>A Gossip protocol runs between all Consul Agents (both server and normal). Both server nodes and ordinary Agents will join this Gossip cluster to send and receive Gossip messages. Every once in a while, each node will randomly select several nodes to send Gossip messages, and other nodes will randomly select other nodes to relay and send messages again. After such a period of time, the entire cluster can receive this message.</p>
<p>At first glance, it seems that the efficiency of this sending method is very low, but there have been papers in mathematics that have demonstrated its feasibility, and the Gossip protocol is already a relatively mature protocol in the P2P network. You can check the introduction of Gossip, which contains a simulator that can tell you the time and bandwidth required for messages to propagate in the cluster. The biggest advantage of the Gossip protocol is that even if the number of cluster nodes increases, the load on each node will not increase much and is almost constant. This allows Consul-managed clusters to scale out to thousands of nodes.</p>
<h1 id="appendix-2-path-algorithms"><a class="header" href="#appendix-2-path-algorithms">Appendix 2: Path Algorithms</a></h1>
<p>Find the optimal path for evacuate.</p>
<p>Not only for compute node HA, but also for load optimization and balancing</p>
<p>Try to design the migration path algorithm to optimize the performance of the node where the VM is located to maximize the return on hardware investment.</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-15 06:47</p>
<h1 id="update-package"><a class="header" href="#update-package">update package</a></h1>
<pre><code>$ sudo apt-get update
$ sudo apt-get upgrade
$ sudo reboot
</code></pre>
<h1 id="install-influxdb"><a class="header" href="#install-influxdb">install influxdb</a></h1>
<pre><code>hanwei@ubuntu-lab:~$ wget https://dl.influxdata.com/influxdb/releases/influxdb_1.2.4_amd64.deb
--2017-06-14 16:37:09--  https://dl.influxdata.com/influxdb/releases/influxdb_1.2.4_amd64.deb
Resolving dl.influxdata.com (dl.influxdata.com)... 52.84.167.167, 52.84.167.39, 52.84.167.178, ...
Connecting to dl.influxdata.com (dl.influxdata.com)|52.84.167.167|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 17305080 (17M) [application/x-debian-package]
Saving to: ‘influxdb_1.2.4_amd64.deb’

influxdb_1.2.4_amd64.deb         100%[========================================================&gt;]  16.50M   875KB/s    in 19s     

2017-06-14 16:37:29 (874 KB/s) - ‘influxdb_1.2.4_amd64.deb’ saved [17305080/17305080]

hanwei@ubuntu-lab:~$ sudo dpkg -i influxdb_1.2.4_amd64.deb
[sudo] password for hanwei: 
Selecting previously unselected package influxdb.
(Reading database ... 63342 files and directories currently installed.)
Preparing to unpack influxdb_1.2.4_amd64.deb ...
Unpacking influxdb (1.2.4-1) ...
Setting up influxdb (1.2.4-1) ...
Created symlink from /etc/systemd/system/influxd.service to /lib/systemd/system/influxdb.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/influxdb.service to /lib/systemd/system/influxdb.service.
Processing triggers for man-db (2.7.5-1) ...
hanwei@ubuntu-lab:~$ service influxdb start
==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ===
Authentication is required to start 'influxdb.service'.
Authenticating as: hanwei,,, (hanwei)
Password: 
==== AUTHENTICATION COMPLETE ===
hanwei@ubuntu-lab:~$ service influxdb status
● influxdb.service - InfluxDB is an open-source, distributed, time series database
Loaded: loaded (/lib/systemd/system/influxdb.service; enabled; vendor preset: enabled)
Active: active (running) since Wed 2017-06-14 16:50:05 CST; 4s ago
    Docs: https://docs.influxdata.com/influxdb/
Main PID: 2156 (influxd)
    Tasks: 7
Memory: 4.9M
    CPU: 49ms
CGroup: /system.slice/influxdb.service
        └─2156 /usr/bin/influxd -config /etc/influxdb/influxdb.conf 
</code></pre>
<h1 id="modify-the-influxdb-configuration-file"><a class="header" href="#modify-the-influxdb-configuration-file">Modify the influxdb configuration file</a></h1>
<pre><code>hanwei@ubuntu-lab:~$ vim /etc/influxdb/influxdb.conf
[admin]
# Determines whether the admin service is enabled.
# enabled = false

# The default bind address used by the admin service.
# bind-address = &quot;:8083&quot;

# Whether the admin service should use HTTPS.
# https-enabled = false
</code></pre>
<p>change to: </p>
<pre><code>enabled = true
bind-address = &quot;:8083&quot;
</code></pre>
<h1 id="create-database-collectd-in-infuxdb"><a class="header" href="#create-database-collectd-in-infuxdb">Create database Collectd in infuxdb</a></h1>
<p>Two ways CLI or web ui</p>
<p>http://192.168.206.144:8086/query?q=CREATE+DATABASE+%22collectd%22&amp;db=collectd</p>
<p>or use web ui</p>
<p><img src="openstack/2023-01-17-11-27-16.png" alt="" /></p>
<h1 id="install-collectd"><a class="header" href="#install-collectd">Install collectd</a></h1>
<pre><code>$ sudo apt-get install collectd
</code></pre>
<p>Configure Collectd as the client, and send the collected data directly to InfluxDB</p>
<pre><code>$ sudo vi /etc/collectd/collectd.conf
...
LoadPlugin network
...
&lt;Plugin network&gt;
        Server &quot;192.168.2.183&quot; &quot;25826&quot;
&lt;/Plugin&gt;
...
</code></pre>
<p>Restart collectd:</p>
<pre><code>$ sudo /etc/init.d/collectd restart
</code></pre>
<h1 id="configure-the-collectd-plugin-that-comes-with-influxdb"><a class="header" href="#configure-the-collectd-plugin-that-comes-with-influxdb">Configure the Collectd plugin that comes with InfluxDB</a></h1>
<p>The Collectd plug-in that comes with InfluxDB is disabled by default. It needs to be manually configured to enable enabled = true, and fill in the line database = &quot;collectd&quot;. The &quot;collectd&quot; here is the database we created above. Remember to restart InfluxDB after changing the configuration</p>
<pre><code>$ sudo vim /etc/influxdb/shared/influxdb.conf
...
# Configure the collectd api
[input_plugins.collectd]
enabled = true
# address = &quot;0.0.0.0&quot; # If not set, is actually set to bind-address.
# port = 25826
database = &quot;collectd&quot;
# types.db can be found in a collectd installation or on github:
# https://github.com/collectd/collectd/blob/master/src/types.db
# typesdb = &quot;/usr/share/collectd/types.db&quot; # The path to the collectd types.db file
...
hanwei@ubuntu-lab:~$ sudo service influxdb restart
</code></pre>
<p>Check the port opened on the server and you will find that the influxdb plug-in has started a 25826 port. If you find that there is no (collected) data in the InfluxDB database, be sure to check whether the 25826 port is started normally</p>
<pre><code>hanwei@ubuntu-lab:~$ sudo netstat -tupln
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 192.168.122.1:53        0.0.0.0:*               LISTEN      1649/dnsmasq    
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1308/sshd       
tcp        0      0 127.0.0.1:5432          0.0.0.0:*               LISTEN      1437/postgres   
tcp        0      0 127.0.0.1:6010          0.0.0.0:*               LISTEN      1740/0          
tcp        0      0 127.0.0.1:6011          0.0.0.0:*               LISTEN      10471/1         
tcp6       0      0 :::8083                 :::*                    LISTEN      10490/influxd   
tcp6       0      0 :::8086                 :::*                    LISTEN      10490/influxd   
tcp6       0      0 :::22                   :::*                    LISTEN      1308/sshd       
tcp6       0      0 :::8088                 :::*                    LISTEN      10490/influxd   
tcp6       0      0 ::1:5432                :::*                    LISTEN      1437/postgres   
tcp6       0      0 ::1:6010                :::*                    LISTEN      1740/0          
tcp6       0      0 ::1:6011                :::*                    LISTEN      10471/1         
udp        0      0 0.0.0.0:53466           0.0.0.0:*                           10239/collectd  
udp        0      0 192.168.122.1:53        0.0.0.0:*                           1649/dnsmasq    
udp        0      0 0.0.0.0:67              0.0.0.0:*                           1649/dnsmasq    
udp        0      0 0.0.0.0:68              0.0.0.0:*                           1250/dhclient   
udp6       0      0 :::25826                :::*                                10490/influxd
</code></pre>
<h1 id="check-if-influxdb-has-data-from-collectd"><a class="header" href="#check-if-influxdb-has-data-from-collectd">Check if InfluxDB has data from Collectd</a></h1>
<p>InfluxDB is ready to accept and process data from Collectd. Use the command line or the web management interface to verify whether there is data in the database</p>
<pre><code>hanwei@ubuntu-lab:~$ influx
Connected to http://localhost:8086 version 1.2.4
InfluxDB shell version: 1.2.4
&gt; show databases;
name: databases
name
----
_internal
mydb
&gt; use collectd
Using database collectd
&gt; show measurements
name: measurements
name
----
cpu_value
df_value
disk_io_time
disk_read
disk_value
disk_weighted_io_time
disk_write
entropy_value
interface_rx
interface_tx
irq_value
load_longterm
load_midterm
load_shortterm
memory_value
processes_value
swap_value
users_value
&gt; SELECT * from cpu_value
...
7435180464451600 ubuntu-lab.localdomain 0        cpu  nice          722
1497435180464452775 ubuntu-lab.localdomain 0        cpu  interrupt     0
1497435180464454083 ubuntu-lab.localdomain 0        cpu  softirq       339
1497435180464454592 ubuntu-lab.localdomain 0        cpu  steal         0
1497435180464455103 ubuntu-lab.localdomain 0        cpu  idle          592750
1497435190462593838 ubuntu-lab.localdomain 0        cpu  user          4281
1497435190462598610 ubuntu-lab.localdomain 0        cpu  system        5599
1497435190462600580 ubuntu-lab.localdomain 0        cpu  wait          7799
1497435190462602296 ubuntu-lab.localdomain 0        cpu  nice          722
1497435190462603649 ubuntu-lab.localdomain 0        cpu  interrupt     0
1497435190462604905 ubuntu-lab.localdomain 0        cpu  softirq       339
1497435190462605510 ubuntu-lab.localdomain 0        cpu  steal         0
1497435190462606004 ubuntu-lab.localdomain 0        cpu  idle          593745
1497435200463552177 ubuntu-lab.localdomain 0        cpu  user          4285
1497435200463557277 ubuntu-lab.localdomain 0        cpu  system        5608
1497435200463558626 ubuntu-lab.localdomain 0        cpu  wait          7799
1497435200463560045 ubuntu-lab.localdomain 0        cpu  nice          722
1497435200463561496 ubuntu-lab.localdomain 0        cpu  interrupt     0
1497435200463562866 ubuntu-lab.localdomain 0        cpu  softirq       339
...
</code></pre>
<p>or web ui</p>
<p><img src="openstack/2023-01-17-11-29-03.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-14 15:25</p>
<blockquote>
<p>nova live-migration --block-migrate</p>
</blockquote>
<h1 id="the-role-of-virtual-machine-live-migration"><a class="header" href="#the-role-of-virtual-machine-live-migration">The role of virtual machine live migration</a></h1>
<p>Every reader may ask such a question, the virtual machine is used well, why should it be migrated? That is, where is the value and purpose of migration. In the daily operation and maintenance of the data center, the following scenarios and requirements are often dealt with. After understanding these requirements, this question will have an answer.</p>
<ul>
<li>Requirement 1: Maintenance, fault repair and upgrade of the physical machine hardware system, but the virtual machine running on this physical machine cannot be shut down, because the user's important services run on it.</li>
<li>Requirement 2: The software system of the physical machine is upgraded and patched. In order not to affect the virtual machine running on it, the virtual machine needs to be migrated to another physical machine before upgrading and patching.</li>
<li>Requirement 3: The load on a physical machine is too heavy, and some virtual machines need to be reduced to release resources.</li>
<li>Requirement 4: In a cluster, some physical machines have too many virtual machines, and some physical machines have too few virtual machines. Resource balance needs to be done.</li>
</ul>
<p>In addition to the above four main requirements, from a service point of view, Live migration has the following two benefits:</p>
<ul>
<li>Benefit 1: The maintenance and upgrade of software and hardware systems will not affect the key services of users, which improves the high availability of services and users satisfaction.</li>
<li>Benefit 2: The system administrator does not need to work overtime, and upgrades the system in the middle of the night. This work can be completed during normal working hours, reducing the company's maintenance costs.</li>
</ul>
<h1 id="basic-concept"><a class="header" href="#basic-concept">basic concept</a></h1>
<p>Before understanding live migration, you must understand the image file format QCOW2. Qcow2 is currently the image format recommended by QEMU. It supports sparse files to save storage space, supports encryption to improve the security of image files, and supports zlib-based compression. Qcow2 image can be used to save the changes of another image file, it does not modify the original image file, which is also called backing_file. Only the image file that is different from the original image file is recorded. This kind of image file is called copy-on-write image. Although it is a separate image file, most of its data comes from the original image, and only based on the original image Only the incremental part of the file will be recorded. The virtual machines mentioned in this article are all created by OpenStack using Qcow2 format image files, as shown in the figure, including two parts.</p>
<ol>
<li>Back-end image (libvirt base)</li>
<li>Separate incremental image file of virtual machine (libvirt instance disks), copy-on-write image</li>
</ol>
<p><img src="openstack/2023-01-17-11-10-35.png" alt="" /></p>
<pre><code>[root@NFJD-TESTN-COMPUTE-1 ~]# cd /var/lib/nova/instances/
[root@NFJD-TESTN-COMPUTE-1 ~]# tree
├── b9530e7b-2309-4eb5-bec3-180241c1e3a2
│   ├── console.log
│   ├── disk
│   ├── disk.config
│   ├── disk.info
│   └── libvirt.xml
├── ...
├── _base
│   ├── 0522bc602608d45758d815b01a6899ff3e1e3e27
│   ├── 05252f6d26980b56fbf93a14c5e70910f18b412c
│   ├── ...
</code></pre>
<p>Use qemu-img to view the information of the virtual machine's separate incremental image file, we can see that his backing file is the image file in the _base directory</p>
<pre><code>[root@NFJD-TESTN-COMPUTE-1 ~]# cd /var/lib/nova/instances/
[root@NFJD-TESTN-COMPUTE-1 instances]# cd 852e1a26-bd49-4149-bd24-552eb4b37034
[root@NFJD-TESTN-COMPUTE-1 852e1a26-bd49-4149-bd24-552eb4b37034]# ls
console.log  disk  disk.config  disk.info  libvirt.xml
[root@NFJD-TESTN-COMPUTE-1 852e1a26-bd49-4149-bd24-552eb4b37034]# qemu-img info disk
image: disk
file format: qcow2
virtual size: 20G (21474836480 bytes)
disk size: 281M
cluster_size: 65536
backing file: /var/lib/nova/instances/_base/731d527f50917ff3364203ebbcf8d4c22dc7919c
Format specific information:
    compat: 1.1
    lazy refcounts: false
    refcount bits: 16
    corrupt: false
</code></pre>
<p>After spending so much time introducing QCOW2, you may wonder, what is the purpose? In fact, the back-end image (libvirt Base) and the incremental image file (libvirt instance disks) of the virtual machine described above are the data to be migrated. The ultimate goal of live migration is to completely migrate them from the source physical host to the target physical host. In addition to the two of them, there is another object that needs to be migrated, which is the data of the virtual machine running in memory.</p>
<p>The transfer of data involves the transmission of data, and the transmission of data needs to go through the network. This article introduces the use of TCP network protocol to realize dynamic migration. Libvirt does not support the TCP protocol by default, you need to modify the configuration of libvirt to enable libvirt to support the TCP protocol, and the following chapters will introduce how to configure it in detail. During the migration process, the libvirtd process running in the destination physical host (Dest Host) needs to create a URI based on the address and port. The URI is used by the destination physical host to receive data and send data back to the source physical host (Source Host). libvirtd process.</p>
<p>On the destination physical host and the source physical host, as long as the following commands can be executed, it means that data can be transferred.</p>
<p><strong>Execute on compute01:</strong></p>
<pre><code>[root@compute01]# virsh -c qemu+tcp://nova@compute02/system
</code></pre>
<p><strong>Execute on compute02:</strong></p>
<pre><code>[root@compute01]# virsh -c qemu+tcp://nova@compute01/system
</code></pre>
<p>Such as:</p>
<pre><code>[root@NFJD-TESTN-COMPUTE-1 instances]# virsh -c qemu+tcp://nova@NFJD-TESTN-COMPUTE-2/system
欢迎使用 virsh，虚拟化的交互式终端。

输入：'help' 来获得命令的帮助信息
    'quit' 退出

virsh # list
Id    名称                         状态
----------------------------------------------------
9     instance-000057c2              暂停
10    instance-000057c6              running
24    instance-0000581a              暂停
25    instance-0000581f              running
31    instance-00005833              running
...
</code></pre>
<h1 id="migration-steps"><a class="header" href="#migration-steps">Migration steps</a></h1>
<p>Now that the basic concept of migration is clarified, let's continue to introduce the steps of migration. A normal process of OpenStack live migration mainly includes four parts: pre-migration condition check, pre-migration preprocessing, migration, and post-migration processing.</p>
<h2 id="condition-checks-before-migration"><a class="header" href="#condition-checks-before-migration">Condition checks before migration</a></h2>
<p>For live migration to execute successfully, some conditions must be met, so some condition checks must be done before performing the migration.</p>
<ol>
<li>Permission check, whether the user performing the migration has sufficient permissions to perform dynamic migration.</li>
<li>Parameter check, whether the parameters passed to the API are sufficient and correct, such as whether the block-migrate parameter is specified.</li>
<li>Check whether the target physical host exists.</li>
<li>Check whether the migrated virtual machine is running.</li>
<li>Check whether the nova-compute service on the source and destination physical hosts is running normally.</li>
<li>Check whether the destination physical host and the source physical host are the same machine.</li>
<li>Check whether the destination physical host has enough memory.</li>
<li>Check whether the hypervisor and hypervisor versions of the destination and source physical host machines are the same.</li>
</ol>
<h2 id="preprocessing-before-migration"><a class="header" href="#preprocessing-before-migration">Preprocessing before migration</a></h2>
<p>Before actually performing the migration, you must do some warm-up and do some preparatory work.</p>
<ol>
<li>Obtain and prepare the block device (volume) mounted by the virtual machine on the target physical host.</li>
<li>Set up the network (networks) of the virtual machine on the destination physical host.</li>
<li>Set up a virtual machine firewall (fireware) on the destination physical host.</li>
</ol>
<h2 id="migrate"><a class="header" href="#migrate">migrate</a></h2>
<p>After the conditions are met and the preprocessing work is completed, live migration can be performed. The main steps are as follows:</p>
<ol>
<li>Call the libvirt python interface migrateToURI to migrate the source host to the destination host.</li>
</ol>
<ul>
<li>dom.migrateToURI(CONF.live_migration_uri % dest,logical_sum,None,CONF.live_migration_bandwidth)</li>
<li>live_migration_uri: This URI is defined by the libvirtd process introduced in 3.2.2.</li>
<li>live_migration_bandwidth: This parameter defines the maximum bandwidth used during the migration.</li>
</ul>
<ol start="2">
<li>Call the wait_for_live_migration method cyclically at a certain time interval (0.5) to detect the status of the virtual machine migration until the virtual machine is successfully migrated.</li>
</ol>
<h2 id="post-migration-processing"><a class="header" href="#post-migration-processing">Post-migration processing</a></h2>
<p>After the virtual machine migration is complete, some aftermath work needs to be done.</p>
<ol>
<li>Detach volume on the source physical host.</li>
<li>Release the security group ingress rule on the source physical host.</li>
<li>Update the status of the virtual machine in the database on the destination physical host.</li>
<li>Delete the virtual machine on the source physical host.</li>
</ol>
<p>After the above four steps are completed normally, the virtual machine is successfully migrated from the source physical host to the destination physical host. The system administrator can then perform the administrative tasks listed in Chapter 2.</p>
<h1 id="configuration-for-live-migration"><a class="header" href="#configuration-for-live-migration">Configuration for Live Migration</a></h1>
<p>This section lists the configurations that support live migration. You must ensure that the configurations on all physical hosts are correct before live migration can be successfully completed.</p>
<h2 id="libvirt"><a class="header" href="#libvirt">libvirt</a></h2>
<p>By default, libvirt supports the TLS protocol for remote connections, but does not support the TCP protocol, so listen_tls=0 listen_tcp=1 enables libvirt to support the TCP protocol.</p>
<ol>
<li>
<p>Modify the /etc/sysconfig/libvirtd file.</p>
<p>LIBVIRTD_ARGS=&quot;--listen&quot;</p>
</li>
<li>
<p>Do the following configuration in the /etc/libvirt/libvirtd.conf file.</p>
<p>listen_tls=0
listen_tcp=1
auth_tcp=&quot;none&quot;</p>
</li>
<li>
<p>Restart the libvirtd service</p>
</li>
</ol>
<h2 id="firewall"><a class="header" href="#firewall">firewall</a></h2>
<p>Configure /etc/sysconfig/iptables to open TCP port 16509.</p>
<pre><code>-A INPUT -p tcp -m multiport --ports 16509 -m comment --comment &quot;libvirt&quot; -j ACCEPT
</code></pre>
<h2 id="openstack-nova"><a class="header" href="#openstack-nova">OpenStack Nova</a></h2>
<p>Configure the live_migration flag in the /etc/nova/nova.conf file.</p>
<pre><code>live_migration_flag=VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-10 03:27</p>
<h1 id="relevant-blueprints"><a class="header" href="#relevant-blueprints">Relevant blueprints:</a></h1>
<ul>
<li>https://blueprints.launchpad.net/nova/+spec/mark-host-down</li>
<li>https://blueprints.launchpad.net/python-novaclient/+spec/support-force-down-service</li>
</ul>
<h1 id="why-add-this-api"><a class="header" href="#why-add-this-api">Why add this API</a></h1>
<p>With this API, external fault monitoring can get the nova-compute service down faster, which will directly improve the fault recovery time of VM HA (the time to evacuate a VM from a faulty node to a normal node).
external fault monitoring system a possibility of telling OpenStack Nova fast that compute host is down. This will immediately enable calling of evacuation of any VM on host and further enabling faster HA actions.</p>
<h1 id="rest-api-for-forced-down"><a class="header" href="#rest-api-for-forced-down">REST API for forced down</a></h1>
<p>request: PUT /v2.1/{tenant_id}/os-services/force-down { “binary”: “nova-compute”, “host”: “compute1”, “forced_down”: true }</p>
<p>response: 200 OK { “service”: { “host”: “compute1”, “binary”: “nova-compute”, “forced_down”: true } }</p>
<p>Example: curl -g -i -X PUT http://{service_host_ip}:8774/v2.1/{tenant_id}/os-services /force-down -H “Content-Type: application/json” -H “Accept: application/json ” -H “X-OpenStack-Nova-API-Version: 2.11” -H “X-Auth-Token: {token}” -d ‘{“b inary”: “nova-compute”, “host”: “compute1”, “forced_down”: true}’</p>
<h1 id="cli-for-forced-down"><a class="header" href="#cli-for-forced-down">CLI for forced down</a></h1>
<p>nova service-force-down nova-compute</p>
<p>Example: nova service-force-down compute1 nova-compute</p>
<h1 id="rest-api-for-disabling-forced-down"><a class="header" href="#rest-api-for-disabling-forced-down">REST API for disabling forced down</a></h1>
<p>request: PUT /v2.1/{tenant_id}/os-services/force-down { “binary”: “nova-compute”, “host”: “compute1”, “forced_down”: false }</p>
<p>response: 200 OK { “service”: { “host”: “compute1”, “binary”: “nova-compute”, “forced_down”: false } }</p>
<p>Example: curl -g -i -X PUT http://{service_host_ip}:8774/v2.1/{tenant_id}/os-services /force-down -H “Content-Type: application/json” -H “Accept: application/json ” -H “X-OpenStack-Nova-API-Version: 2.11” -H “X-Auth-Token: {token}” -d ‘{“b inary”: “nova-compute”, “host”: “compute1”, “forced_down”: false}’</p>
<h1 id="cli-for-disabling-forced-down"><a class="header" href="#cli-for-disabling-forced-down">CLI for disabling forced down</a></h1>
<p>nova service-force-down –unset nova-compute</p>
<p>Example: nova service-force-down –unset compute1 nova-compute</p>
<h1 id="add-fast-evacuation-function-to-vm-ha-procedure"><a class="header" href="#add-fast-evacuation-function-to-vm-ha-procedure">Add fast evacuation function to VM HA procedure</a></h1>
<p>Now add fast evacuation function in the developed VM HA program</p>
<ol>
<li>Modify the configuration file and add the quick_evacuate_level option</li>
</ol>
<ul>
<li>level 1: (fastest) Do not shut down, just force down nova-compute-service</li>
<li>level 2: (faster) (default) Shut down first, then force down nova-compute-service</li>
<li>level 3: (disable quick_evacuate)</li>
</ul>
<ol start="2">
<li>change python-novaclient&gt;=6.0.0 to support force-service-down</li>
<li>Modify the processing logic of the evacuation process and add rapid evacuation</li>
</ol>
<p>The original evacuation process is to shut down the faulty computing node after finding the faulty node, and then wait for the faulty node to cause nova service state down due to heartbeat loss, and then perform evacuation.</p>
<p>Now it is changed to:</p>
<ul>
<li>level 1: (fastest) does not shut down, only force down nova-compute -service</li>
<li>level 2: (faster) (default) Shut down first, then force down after shutdown introduction nova-compute-service</li>
<li>level 3: (disable quick_evacuate) do nothing, same as the original process</li>
</ul>
<p>The following code def _ipmi_handle, before the change, shuts down the faulty node. After the change, at level 1, force down nova service state and skip the shutdown process.</p>
<pre><code class="language-python">def _ipmi_handle(self, node):
    # cmd = 'ssh %s systemctl stop openstack-nova-compute' % node
    # return os.system(cmd)
    ret = 1
################  add  ################
    if CONF.quick_evacuate_level == 1:
        try:
            self.nova_client.services.force_down(host=node,
                                                 binary='nova-compute',
                                                 force_down=True,
                                                )
            service = self.nova_client.services.list(
                host=node,
                binary='nova-compute')
            if service:
                if service[0].state == 'down':
                    return ret
            raise
        except Exception as e:
            content = node + ' nova-compute service force down failed!'
            title = 'force down failed'
            self.db.add_or_update_guardian_log(**{'title': title,
                                                  'detail': content,
                                                  'level': 'ERROR'})
            LOG.error(content)
            LOG.error(e)
################  add  ################
    LOG.info('IPMI enabled,need to check power status of %s' % node)
    ipmi_info = self.db.get_ipmiInfo(node)
    if ipmi_info is None:
        LOG.warning(
            'can not find ipmi info for %s, will ignore evacuate...'
            % node)
        return 0
    LOG.debug('get ipmi info (ip %s user %s, password %s)' % (
        ipmi_info.ip, ipmi_info.username, ipmi_info.password))
    ipmi_util = IPMIUtil(ipmi_info.ip, ipmi_info.username,
                         ipmi_info.password)
    power_status = ipmi_util.get_power_status()
    LOG.info('power status of %s is %s' % (node, power_status))

    if 'off' != power_status:
        LOG.info('power status of %s is not off,power off it...' % node)
        ipmi_util.do_power_off()
        for i in range(1, CONF.ipmi_check_max_count + 1):
            power_status = ipmi_util.get_power_status()
            if 'off' != power_status:
                LOG.info(
                    'power status of %s is not off,wait for %s seconds...'
                    % (node, CONF.ipmi_check_interval))
                time.sleep(CONF.ipmi_check_interval)
            else:
                break
        power_status = ipmi_util.get_power_status()
        if 'off' != power_status:
            LOG.info(
                'after %s check,can not confirm power status of '
                '%s is off,will ignore evacuate...'
                % (node,
                   CONF.ipmi_check_max_count*CONF.ipmi_check_interval))
            ret = 0
    return ret
</code></pre>
<p>When the following code is at level 2, it does not skip the shutdown process of the above code, and force down before checking the nova service state to achieve rapid evacuation (do not check the heartbeat process). Level 3 is to do nothing, the same as before.</p>
<pre><code class="language-python">################  add  ################
if CONF.quick_evacuate_level == 2:
    try:
        self.nova_client.services.force_down(host=node,
                                             binary='nova-compute',
                                             force_down=True,
                                            )
        service = self.nova_client.services.list(
            host=node,
            binary='nova-compute')
        if service:
            if service[0].state != 'down':
                raise
    except Exception as e:
        content = node + ' nova-compute service force down failed!'
        title = 'force down failed'
        self.db.add_or_update_guardian_log(**{'title': title,
                                              'detail': content,
                                              'level': 'ERROR'})
        LOG.error(content)
        LOG.error(e)
################  add  ################

if self._wait_for_service_to_down(node):
    for server in servers:
        can_evacuate = self._evacuate_predict(server)
        if can_evacuate:
            try:
                # just evacuate, let nova scheduler
                # choose one host if not in drbd env
                respone = server.evacuate(
                    host=evacuated_to,
                    on_shared_storage=CONF.on_share_storage)
                LOG.info(
                    'evacuate vm info:[id:%s,name:%s,status:%s],'
                    'reponse is %s'
                    % (server.id, server.name, server.status,
                        respone))
                content = '虚拟机被疏散:虚拟机ID: %s' % server.id
                self.send_snmp(content=content)
            except Exception as e:
                content = ('evacuate vm %s failed' % server.id)
                title = ('疏散虚拟机%s失败' % server.id)
                self.db.add_or_update_guardian_log(
                    **{'title': title,
                       'detail': content,
                       'level': 'ERROR'})
                content = '虚拟机疏散调用API失败:虚拟机ID: %s' % server.id
                self.send_snmp(content=content)
                LOG.error(content)
                LOG.error(e)

            # add to db
            try:
                self.db.add_evacuate_history(
                    instance_id=server.id,
                    instance_name=server.name,
                    host=getattr(server, 'OS-EXT-SRV-ATTR:host'))

                content = (
                    'add_evacuate_history '
                    '[instance_id:%s,instance_name:%s,host:%s]'
                    % (server.id, server.name,
                       getattr(server, 'OS-EXT-SRV-ATTR:host')))
                LOG.info(content)
                title = ('增加疏散记录%s' % server.name)
                self.db.add_or_update_guardian_log(
                    **{'title': title,
                       'detail': content,
                       'level': 'INFO'})
            except Exception as e:
                content = (
                    'add_evacuate_history for vm %s failed'
                    % server.id)
                title = ('增加疏散记录%s失败' % server.id)
                self.db.add_or_update_guardian_log(
                    **{'title': title,
                       'detail': content,
                       'level': 'ERROR'})
                LOG.error(content)
                LOG.error(e)
        else:
            content = '虚拟机没有被疏散:虚拟机ID: %s' % server.id
            self.send_snmp(content=content)
            LOG.info('server %s can not evacuate' % server.id)

    # disable this service
    try:
        self.nova_client.services.disable(node, 'nova-compute')
    except Exception as e:
        content = (
            'failed to disable compute service on node %s' % node)
        title = ('disable节点%s服务失败' % node)
        self.db.add_or_update_guardian_log(
            **{'title': title,
               'detail': content,
               'level': 'ERROR'})
        LOG.error(content)
        LOG.error(e)
        raise

    heart_beat = self.db.get_heartbeat_by_name(node)
    heart_beat.has_evacuated = 1
    heart_beat.status = 'done'
    heart_beat.save(self.db.session)
    LOG.info('node %s update with [has_evacuated: 1]' % node)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-05-04 22:21</p>
<h1 id="question-survey"><a class="header" href="#question-survey">question survey</a></h1>
<h2 id="direct-cause"><a class="header" href="#direct-cause">direct cause</a></h2>
<p>After investigation, we found that due to the cloud environment, There is a deviation between the time of the windows virtual machine and the time of the physical machine, which causes the activation to fail.</p>
<h2 id="specific-time-deviation"><a class="header" href="#specific-time-deviation">Specific time deviation</a></h2>
<p>Windows instance sometimes finds that the operating system time is always 8 hours behind. Even if the time and time zone are manually adjusted, the difference will be 8 hours after the next restart of the instance.</p>
<h2 id="why-is-there-a-time-deviation"><a class="header" href="#why-is-there-a-time-deviation">Why is there a time deviation?</a></h2>
<p>KVM handles system time differently for Linux and Windows virtual machines, and Windows requires some additional settings.</p>
<p>Why do Linux and Windows virtual machines handle system time differently? To understand the following three concepts.</p>
<p>###UTC time and local time</p>
<p>UTC time: Also known as the world standard time and the unified world time, UTC is calibrated by atomic clocks, and other parts of the world use this as the reference time plus their own time zone to set their local time</p>
<p>Local time: Due to different time zones, local time is generally different from UTC, and the conversion method is</p>
<p>Local time = UTC + time zone or UTC = local time - time</p>
<p>zone The east of the time zone is positive, and the west is negative. In China, the time zone is the east eighth district, which is +8 district. The local time uses Beijing time, which is displayed on linux as CST , so CST=UTC+(+8 hours) or UTC=CST-(+8 hours)</p>
<h3 id="guest-os-time-keeping"><a class="header" href="#guest-os-time-keeping">guest OS time keeping</a></h3>
<p>The kvm technology is fully virtualized, and the guest OS can run directly without modification. However, there are problems in timing. One way of guest OS timing is to count through clock interrupts and then convert them, but the clock interrupts generated by the host Can't reach all guest OS in time, because interrupt in guest OS is not really hardware interrupt, it's interrupt injected by host</p>
<p>Many network applications, session verification in the web, etc., will call the system time. If the time in the guest OS is wrong, it will cause a program error. For this reason, kvm provides a paravirtualized clock (kvm-clock) for the guest vms. In RHEL5.5 and above, kvm-clock is used as the default clock source, you can use the following command in the guest to check whether kvm-clock is used</p>
<pre><code>cat /sys/devices/system/clocksource/clocksource0/current_clocksource
kvm-clock
</code></pre>
<p>In the kvm-clock mode, the guest OS cannot directly access the host clock, and the host writes the system time into a memory page that the guest can read, so that the guest can read this memory page to set its own hardware time, but the host does not update it in real time The time is up to this memory page, but it is updated when a vm event (vm shutdown, restart, etc.) occurs, so this method cannot keep the guest time accurate</p>
<h3 id="set-the-virtual-machine-hardware-clock-in-libvirt"><a class="header" href="#set-the-virtual-machine-hardware-clock-in-libvirt">Set the virtual machine hardware clock in libvirt</a></h3>
<p>The kvm virtual machine is generally managed by libvirt. In the xml file of the virtual machine configuration, there are items related to the hardware clock setting of the virtual machine.</p>
<pre><code>&lt;clock offset='localtime'&gt;
&lt;/clock&gt;
</code></pre>
<p>The offset attribute of the clock has four options: &quot;utc&quot;, &quot;localtime&quot;, &quot;timezone&quot;, and &quot;variable&quot;.</p>
<p>If the guest OS is a Linux system, &quot;utc&quot; should be selected. The guest OS will synchronize the utc time with the host once when it starts, and then calculate the system time according to the time zone set in /etc/localtime</p>
<p>If the guest OS is a windows system, &quot;localtime&quot; should be selected, and the guest OS will synchronize the system time with the host once at startup</p>
<h1 id="countermeasures-in-the-openstack-cloud-environment"><a class="header" href="#countermeasures-in-the-openstack-cloud-environment">Countermeasures in the openstack cloud environment</a></h1>
<h2 id="for-new-windows-virtual-machines"><a class="header" href="#for-new-windows-virtual-machines">For new windows virtual machines</a></h2>
<p>It is to add os_type=&quot;windows&quot; information to the existing windows image and the new windows image created in the future</p>
<pre><code>[root@build localhost]# glance image-update  --property os_type=&quot;windows&quot; 1d2afd0f-9162-415c-ace3-97de254b6c9c
1d2afd0f-9162-415c-ace3-97de254b6c9c 是vm id
</code></pre>
<h2 id="for-existing-windows-virtual-machines"><a class="header" href="#for-existing-windows-virtual-machines">For existing windows virtual machines</a></h2>
<p>To modify the database through violence, the specific plan:</p>
<ol>
<li>Find out the uuid information of all the existing windows virtual machines to be modified</li>
<li>Enter the nova database, modify the os_type field of the windows virtual machine in the instances table, and modify the field For windows, for example, modify the os_type field of the vm whose vm id is f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd to windows</li>
<li>update instances set os_type='windows' where uuid='f64bf83d-a9f0-4ec5-b481-e363de19bd' Check whether the os_type field is changed to 'windows', for example, select * from instances where uuid='f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd';</li>
<li>Hard restart the virtual machine, you need to contact the customer in advance
<code># nova reboot f64bf83d-a9f0-4ec5 -b481-e3e76e19b6bd –hard –poll</code>
restart Reason: KVM will obtain the modified database information, update the XML configuration, and ensure time synchronization.</li>
</ol>
<h1 id="test-whether-the-vm-generated-by-nova-boot-will-have-the-keyword-of-image"><a class="header" href="#test-whether-the-vm-generated-by-nova-boot-will-have-the-keyword-of-image">Test: Whether the vm generated by nova boot will have the keyword of image</a></h1>
<pre><code>[root@NFJD-TESTVM-CORE-API-1 ~]# glance image-update --property os_type=&quot;windows&quot; fb5bc637-c8ab-4999-9e88-da28561fed21
+---------------------+--------------------------------------------------+
| Property | Value |
+---------------------+--------------------------------------------------+
| checksum | 8049db08971bec254be86067025d0c32 |
| container_format | bare |
| created_at | 2016-07-12T06:16:50Z |
| direct_url | sheepdog://fb5bc637-c8ab-4999-9e88-da28561fed21 |
| disk_format | qcow2 |
| hw_qemu_guest_agent | yes |
| id | fb5bc637-c8ab-4999-9e88-da28561fed21 |
| image.os_type | windows |
| min_disk | 0 |
| min_ram | 0 |
| name | Windows-Server-2008-R2-Enterprise-64bit-20160706 |
| os_distro | windows |
| os_type | windows |
| owner | 6c149dcd3cf64171b8dd972dd03bbac0 |
| protected | False |
| size | 10015539200 |
| status | active |
| tags | [] |
| updated_at | 2017-05-05T03:42:50Z |
| virtual_size | None |
| visibility | public |
+---------------------+--------------------------------------------------+
[root@NFJD-TESTVM-CORE-API-1 ~]# nova image-show fb5bc637-c8ab-4999-9e88-da28561fed21
+------------------------------+--------------------------------------------------+
| Property | Value |
+------------------------------+--------------------------------------------------+
| OS-EXT-IMG-SIZE:size | 10015539200 |
| created | 2016-07-12T06:16:50Z |
| id | fb5bc637-c8ab-4999-9e88-da28561fed21 |
| metadata hw_qemu_guest_agent | yes |
| metadata image.os_type | windows |
| metadata os_distro | windows |
| metadata os_type | windows |
| minDisk | 0 |
| minRam | 0 |
| name | Windows-Server-2008-R2-Enterprise-64bit-20160706 |
| progress | 100 |
| status | ACTIVE |
| updated | 2017-05-05T03:42:50Z |
+------------------------------+--------------------------------------------------+
[root@NFJD-TESTVM-CORE-API-1 ~]# nova boot --image fb5bc637-c8ab-4999-9e88-da28561fed21 --flavor 3 --nic net-id=ee98ec6a-e1ed-43eb-9d0a-26080d277484 t4 --availability-zone nova:NFJD-TESTN-COMPUTE-3
+--------------------------------------+-----------------------------------------------------------------------------------------+
| Property | Value |
+--------------------------------------+-----------------------------------------------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | nova |
| OS-EXT-SRV-ATTR:host | - |
| OS-EXT-SRV-ATTR:hypervisor_hostname | - |
| OS-EXT-SRV-ATTR:instance_name | instance-000056dc |
| OS-EXT-STS:power_state | 0 |
| OS-EXT-STS:task_state | scheduling |
| OS-EXT-STS:vm_state | building |
| OS-SRV-USG:launched_at | - |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| adminPass | DoLL3d65w7cN |
| config_drive | |
| created | 2017-05-05T03:46:42Z |
| flavor | m1.medium (3) |
| hostId | |
| id | f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd |
| image | Windows-Server-2008-R2-Enterprise-64bit-20160706 (fb5bc637-c8ab-4999-9e88-da28561fed21) |
| key_name | - |
| metadata | {} |
| name | t4 |
| os-extended-volumes:volumes_attached | [] |
| progress | 0 |
| security_groups | default |
| status | BUILD |
| tenant_id | 6c149dcd3cf64171b8dd972dd03bbac0 |
| updated | 2017-05-05T03:46:43Z |
| user_id | 62f52135115f4898bd0d82c1f0cd632b |
+--------------------------------------+-----------------------------------------------------------------------------------------+
[root@NFJD-TESTVM-CORE-API-1 ~]# nova show t4
+--------------------------------------+-----------------------------------------------------------------------------------------+
| Property | Value |
+--------------------------------------+-----------------------------------------------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | nova |
| OS-EXT-SRV-ATTR:host | NFJD-TESTN-COMPUTE-3 |
| OS-EXT-SRV-ATTR:hypervisor_hostname | NFJD-TESTN-COMPUTE-3 |
| OS-EXT-SRV-ATTR:instance_name | instance-000056dc |
| OS-EXT-STS:power_state | 1 |
| OS-EXT-STS:task_state | - |
| OS-EXT-STS:vm_state | active |
| OS-SRV-USG:launched_at | 2017-05-05T03:46:50.000000 |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| config_drive | True |
| created | 2017-05-05T03:46:42Z |
| flavor | m1.medium (3) |
| hostId | c34578f3d9c097be0d91fda237706199673ca7e5a87051f5aad24637 |
| id | f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd |
| image | Windows-Server-2008-R2-Enterprise-64bit-20160706 (fb5bc637-c8ab-4999-9e88-da28561fed21) |
| key_name | - |
| metadata | {} |
| name | t4 |
| net001 network | 192.168.2.92 |
| os-extended-volumes:volumes_attached | [] |
| progress | 0 |
| security_groups | default |
| status | ACTIVE |
| tenant_id | 6c149dcd3cf64171b8dd972dd03bbac0 |
| updated | 2017-05-05T03:46:51Z |
| user_id | 62f52135115f4898bd0d82c1f0cd632b |
+--------------------------------------+-----------------------------------------------------------------------------------------+
MySQL [nova]&gt; select * from instances where uuid='f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd';
+---------------------+---------------------+------------+-------+-------------+----------------------------------+----------------------------------+--------------------------------------+-----------+------------+--------------+----------+----------+-------------+----------+-----------+-------+----------+----------------------+-----------+----------------+--------------+---------------------+---------------+--------------+---------------------+-------------------+--------+---------+----------------------+------------------+---------+--------------------------------------+--------------+------------------+--------------+--------------+--------------+------------+--------------------------+---------------------+----------+------------------+--------------------+-------------------+---------+--------------+-----------+----------------------+---------+-----------+---------+--------------------+
| created_at | updated_at | deleted_at | id | internal_id | user_id | project_id | image_ref | kernel_id | ramdisk_id | launch_index | key_name | key_data | power_state | vm_state | memory_mb | vcpus | hostname | host | user_data | reservation_id | scheduled_at | launched_at | terminated_at | display_name | display_description | availability_zone | locked | os_type | launched_on | instance_type_id | vm_mode | uuid | architecture | root_device_name | access_ip_v4 | access_ip_v6 | config_drive | task_state | default_ephemeral_device | default_swap_device | progress | auto_disk_config | shutdown_terminate | disable_terminate | root_gb | ephemeral_gb | cell_name | node | deleted | locked_by | cleaned | ephemeral_key_uuid |
+---------------------+---------------------+------------+-------+-------------+----------------------------------+----------------------------------+--------------------------------------+-----------+------------+--------------+----------+----------+-------------+----------+-----------+-------+----------+----------------------+-----------+----------------+--------------+---------------------+---------------+--------------+---------------------+-------------------+--------+---------+----------------------+------------------+---------+--------------------------------------+--------------+------------------+--------------+--------------+--------------+------------+--------------------------+---------------------+----------+------------------+--------------------+-------------------+---------+--------------+-----------+----------------------+---------+-----------+---------+--------------------+
| 2017-05-05 03:46:42 | 2017-05-05 03:46:51 | NULL | 22236 | NULL | 62f52135115f4898bd0d82c1f0cd632b | 6c149dcd3cf64171b8dd972dd03bbac0 | fb5bc637-c8ab-4999-9e88-da28561fed21 | | | 0 | NULL | NULL | 1 | active | 4096 | 2 | t4 | NFJD-TESTN-COMPUTE-3 | NULL | r-ae2dhvll | NULL | 2017-05-05 03:46:50 | NULL | t4 | t4 | nova | 0 | windows | NFJD-TESTN-COMPUTE-3 | 3 | NULL | f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd | NULL | /dev/vda | NULL | NULL | True | NULL | NULL | NULL | 0 | 0 | 0 | 0 | 40 | 0 | NULL | NFJD-TESTN-COMPUTE-3 | 0 | NULL | 0 | NULL |
+---------------------+---------------------+------------+-------+-------------+----------------------------------+----------------------------------+--------------------------------------+-----------+------------+--------------+----------+----------+-------------+----------+-----------+-------+----------+----------------------+-----------+----------------+--------------+---------------------+---------------+--------------+---------------------+-------------------+--------+---------+----------------------+------------------+---------+--------------------------------------+--------------+------------------+--------------+--------------+--------------+------------+--------------------------+---------------------+----------+------------------+--------------------+-------------------+---------+--------------+-----------+----------------------+---------+-----------+---------+--------------------+
1 row in set (0.00 sec)

the instances table of database nova os_type='windows'
</code></pre>
<h1 id="similarly-for-virtual-machines-that-cannot-obtain-monitoring-data-use-the-following-methods-to-modify-the-virtual-machine-configuration"><a class="header" href="#similarly-for-virtual-machines-that-cannot-obtain-monitoring-data-use-the-following-methods-to-modify-the-virtual-machine-configuration">Similarly, for virtual machines that cannot obtain monitoring data, use the following methods to modify the virtual machine configuration:</a></h1>
<p>(1) Contact the colleague who provided the image to confirm that qemu-guest-agent has been installed in the virtual machine</p>
<p>(2) Modify the nova database to change the virtual machine field:</p>
<pre><code>MariaDB [(none)]&gt; use nova;
MariaDB [nova]&gt; insert into instance_system_metadata values(NULL,NULL,NULL,NULL,'2cf79497-d1ba-42c3-adf5-3b45c6927dfb','image_hw_qemu_guest_agent','yes',0);
注： 2cf79497-d1ba-42c3-adf5-3b45c6927dfb 为对应instance的uuid
检查：select * from instance_system_metadata where instance_uuid='3ce9d8b0-a0a3-48ed-ba54-415883f717e5';
</code></pre>
<p>(3) Modify the image used by the virtual machine</p>
<pre><code># glance image-update xxx --property hw_qemu_guest_agent=yes --property hw_ovirt_guest_agent=yes
</code></pre>
<p>(4) To hard restart the virtual machine, you need to contact the customer in advance</p>
<pre><code># nova reboot 2cf79497-d1ba-42c3-adf5-3b45c6927dfb --hard --poll
</code></pre>
<h1 id="why-modifying-os_typewindows-will-affect-the-time-of-windows-virtual-machine"><a class="header" href="#why-modifying-os_typewindows-will-affect-the-time-of-windows-virtual-machine">Why modifying os_type='windows' will affect the time of windows virtual machine</a></h1>
<h2 id="openstack-will-determine-the-xml-content-required-for-the-vm-of-the-underlying-kvm-according-to-the-content-of-os_type"><a class="header" href="#openstack-will-determine-the-xml-content-required-for-the-vm-of-the-underlying-kvm-according-to-the-content-of-os_type">openstack will determine the xml content required for the vm of the underlying kvm according to the content of os_type</a></h2>
<p>By querying a windows virtual machine and a linux virtual machine to compare the xml content, it is true that windows vm is clock offset='localtime' linux vm is clock offset='utc'</p>
<p>The <clock> element is used to determine how the guest virtual machine clock is synchronized with the host physical machine clock. The clock element has the following attributes:</p>
<p>offset determines how the guest virtual machine clock is offset from the host physical machine clock.</p>
<h3 id="windows-virtual-machine"><a class="header" href="#windows-virtual-machine">windows virtual machine</a></h3>
<pre><code>[root@NFJD-TESTVM-CORE-API-1 ~]# nova show t4
+--------------------------------------+-----------------------------------------------------------------------------------------+
| Property | Value |
+--------------------------------------+-----------------------------------------------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | nova |
| OS-EXT-SRV-ATTR:host | NFJD-TESTN-COMPUTE-3 |
| OS-EXT-SRV-ATTR:hypervisor_hostname | NFJD-TESTN-COMPUTE-3 |
| OS-EXT-SRV-ATTR:instance_name | instance-000056dc |
| OS-EXT-STS:power_state | 1 |
| OS-EXT-STS:task_state | - |
| OS-EXT-STS:vm_state | active |
| OS-SRV-USG:launched_at | 2017-05-05T03:46:50.000000 |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| config_drive | True |
| created | 2017-05-05T03:46:42Z |
| flavor | m1.medium (3) |
| hostId | c34578f3d9c097be0d91fda237706199673ca7e5a87051f5aad24637 |
| id | f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd |
| image | Windows-Server-2008-R2-Enterprise-64bit-20160706 (fb5bc637-c8ab-4999-9e88-da28561fed21) |
| key_name | - |
| metadata | {} |
| name | t4 |
| net001 network | 192.168.2.92 |
| os-extended-volumes:volumes_attached | [] |
| progress | 0 |
| security_groups | default |
| status | ACTIVE |
| tenant_id | 6c149dcd3cf64171b8dd972dd03bbac0 |
| updated | 2017-05-05T03:46:51Z |
| user_id | 62f52135115f4898bd0d82c1f0cd632b |
+--------------------------------------+-----------------------------------------------------------------------------------------+
[root@NFJD-TESTN-COMPUTE-3 ~]# virsh dumpxml instance-000056dc
...
&lt;clock offset='localtime'&gt;
&lt;timer name='pit' tickpolicy='delay'/&gt;
&lt;timer name='rtc' tickpolicy='catchup'/&gt;
&lt;timer name='hpet' present='no'/&gt;
&lt;/clock&gt;
...
</code></pre>
<h3 id="linux-virtual-machine"><a class="header" href="#linux-virtual-machine">linux virtual machine</a></h3>
<pre><code>[root@NFJD-TESTVM-CORE-API-1 ~]# nova show t1
+--------------------------------------+-------------------------------------------------------------------+
| Property | Value |
+--------------------------------------+-------------------------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | nova |
| OS-EXT-SRV-ATTR:host | NFJD-TESTN-COMPUTE-1 |
| OS-EXT-SRV-ATTR:hypervisor_hostname | NFJD-TESTN-COMPUTE-1 |
| OS-EXT-SRV-ATTR:instance_name | instance-000056d9 |
| OS-EXT-STS:power_state | 1 |
| OS-EXT-STS:task_state | - |
| OS-EXT-STS:vm_state | active |
| OS-SRV-USG:launched_at | 2017-05-05T02:44:20.000000 |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| config_drive | True |
| created | 2017-05-05T02:44:11Z |
| flavor | m1.medium (3) |
| hostId | f091880caebe68654030e5df4825e49548af0037454e5723961bfc3d |
| id | 6316dc6e-2fb0-4d72-8e99-ad5749efc295 |
| image | CentOS-7.1-x86_64-20161018 (8aaf1759-9fb4-4ba9-8cff-1743eb824c5f) |
| key_name | - |
| metadata | {} |
| name | t1 |
| net001 network | 192.168.2.90 |
| os-extended-volumes:volumes_attached | [] |
| progress | 0 |
| security_groups | default |
| status | ACTIVE |
| tenant_id | 6c149dcd3cf64171b8dd972dd03bbac0 |
| updated | 2017-05-05T03:36:09Z |
| user_id | 62f52135115f4898bd0d82c1f0cd632b |
+--------------------------------------+-------------------------------------------------------------------+
[root@NFJD-TESTN-COMPUTE-1 ~]# virsh dumpxml instance-000056d9
...
&lt;clock offset='utc'&gt;
&lt;timer name='pit' tickpolicy='delay'/&gt;
&lt;timer name='rtc' tickpolicy='catchup'/&gt;
&lt;timer name='hpet' present='no'/&gt;
&lt;/clock&gt;
...
</code></pre>
<h1 id="related-nova-code"><a class="header" href="#related-nova-code">Related nova code</a></h1>
<pre><code class="language-python"># nova/virt/libvirt/driver.py

def _set_clock(self, guest, os_type, image_meta, virt_type):
    # NOTE(mikal): Microsoft Windows expects the clock to be in
    # &quot;localtime&quot;. If the clock is set to UTC, then you can use a
    # registry key to let windows know, but Microsoft says this is
    # buggy in http://support.microsoft.com/kb/2687252
    clk = vconfig.LibvirtConfigGuestClock()
    if os_type == 'windows':
        LOG.info(_LI('Configuring timezone for windows instance to '
                     'localtime'))
        clk.offset = 'localtime'  # 在os_type为windows的时候clk.offset设置为localtime
    else:
        clk.offset = 'utc'
    guest.set_clock(clk)  # 在os_type为linux的时候clk.offset设置为utc

    if virt_type == &quot;kvm&quot;:
        self._set_kvm_timers(clk, os_type, image_meta)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="summary-1"><a class="header" href="#summary-1">Summary</a></h1>
<ul>
<li>
<p><a href="./SUMMARY.html">Openstack</a></p>
<ul>
<li><a href="./openstack/mount-cloud-disk.html">Mount cloud disk</a></li>
<li><a href="./openstack/ocata-nova-evacuate-bug.html">Ocata nova evacuate bug</a></li>
<li><a href="./openstack/compute-node-ha.html">compute node ha mainstream open source implementation</a></li>
<li><a href="./openstack/collectd-influxdb.html">Deployment and usage of Collectd and InfluxDB</a></li>
<li><a href="./openstack/live-migration-local.html">Live Migration on Local Storage</a></li>
<li><a href="./openstack/fast-evacuation.html">Add fast evacuation function to VM HA procedure</a></li>
<li><a href="./openstack/vm-activation-failure.html">Windows virtual machine activation failure problem</a></li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Tools</a></p>
<ul>
<li><a href="./tools/xshell-skill.html">Some Skills of using xshell tools</a></li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">point of view</a></p>
<ul>
<li><a href="./tools/gerrit-install.html">gerrit install</a></li>
<li><a href="./point-of-view/huawei-and-operators.html">Huawei and telecom operators</a></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2016-07-05 22:57</p>
<h1 id="paste"><a class="header" href="#paste">Paste</a></h1>
<p>For example, openstack vm_id:</p>
<pre><code>424288e4-23a7-45de-bb5d-742bd6c54561
</code></pre>
<p>If the default delimiter is used, double-clicking can only select a part, you need to press and hold the mouse to drag, or more or less, or it will take a little time.</p>
<p>After changing the settings:</p>
<p>In the &quot;Keyboard and Mouse&quot; tab of &quot;Options&quot;</p>
<ol>
<li>Remove the &quot;-&quot; in the delimiter.</li>
<li>Check &quot;Automatically copy the selected text to the clipboard&quot; and vm_id can be selected by double-clicking. There is no need to select copy and paste. It has been copied at the same time as it is selected. At this time, only the middle mouse button is needed to complete the paste.</li>
</ol>
<h1 id="split-screen"><a class="header" href="#split-screen">Split Screen</a></h1>
<p>Only 5.0 or above is supported, and it can be realized by dragging the label to a certain position on the screen with the mouse. It is useful in some cases, such as analyzing networks and comparing jos.</p>
<h1 id="multi-level-jump"><a class="header" href="#multi-level-jump">multi-level jump</a></h1>
<p>In the internal environment of an enterprise, not every node has an external network ip, often through the Jump Server, and then the Jump Server login to other nodes. In a complex environment, there may be more than two levels of login, or even three or more levels of login. It can be easily achieved with xshell.</p>
<p>When creating a new session, or click the properties of the created session, select &quot;Login Script&quot; in &quot;Connection&quot; in &quot;Category&quot;</p>
<p>Select the &quot;Execute the following wait and send rules&quot; check box to activate the two columns below Expect and Send to display interactive functions similar to tcl's expect or python pexpect package.</p>
<table><thead><tr><th>Expect</th><th>Send</th></tr></thead><tbody>
<tr><td>$</td><td>ssh deployer@xx.xx.xx.xx</td></tr>
<tr><td>password:</td><td>xxxxxx</td></tr>
</tbody></table>
<p>The above is two levels of login, and more levels of login can be added later.</p>
<h1 id="tunnel-forwarding"><a class="header" href="#tunnel-forwarding">tunnel forwarding</a></h1>
<p>Select &quot;Tunnel&quot; in &quot;SSH&quot; in &quot;Connection&quot; in &quot;Category&quot; in the properties of the session.</p>
<p>Two commonly used methods Local (Outgoing) and Dynamic (SOCKS4/5)</p>
<p>Take access to the openstack dashboard on the intranet as an example:</p>
<h2 id="localoutgoing"><a class="header" href="#localoutgoing">Local(Outgoing)</a></h2>
<pre><code>(http)
Source host: localhost
Listening port: xx
Destination host: xx.xx.xx.xx
Destination port: 80
(novnc)
Source host: localhost
Listening port: xx
Destination host: xx.xx.xx.xx
Destination port : 6080
</code></pre>
<p>The browser does not need to set up a proxy when accessing, just need to enter <code>http://localhost:port</code> in the address bar.</p>
<h2 id="dynamicsocks45"><a class="header" href="#dynamicsocks45">Dynamic(SOCKS4/5)</a></h2>
<p>Listening port: xx</p>
<p>You need to set SOCKS4 or SOCKS5 proxy when accessing the browser, and you need to enter the url address of the intranet in the address bar.</p>
<p>General browser supports SOCKS4/5 proxy, also you can use some proxy tool such as the Proxy SwitchyOmega plug-in of chrome.</p>
<p>The two tunnel forwarding methods have their own characteristics. I always use the latter, because there are few settings. If you want to access other ports or other services, you only need to set up the jump server.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="summary-2"><a class="header" href="#summary-2">Summary</a></h1>
<ul>
<li>
<p><a href="./SUMMARY.html">Openstack</a></p>
<ul>
<li><a href="./openstack/mount-cloud-disk.html">Mount cloud disk</a></li>
<li><a href="./openstack/ocata-nova-evacuate-bug.html">Ocata nova evacuate bug</a></li>
<li><a href="./openstack/compute-node-ha.html">compute node ha mainstream open source implementation</a></li>
<li><a href="./openstack/collectd-influxdb.html">Deployment and usage of Collectd and InfluxDB</a></li>
<li><a href="./openstack/live-migration-local.html">Live Migration on Local Storage</a></li>
<li><a href="./openstack/fast-evacuation.html">Add fast evacuation function to VM HA procedure</a></li>
<li><a href="./openstack/vm-activation-failure.html">Windows virtual machine activation failure problem</a></li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Tools</a></p>
<ul>
<li><a href="./tools/xshell-skill.html">Some Skills of using xshell tools</a></li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">point of view</a></p>
<ul>
<li><a href="./tools/gerrit-install.html">gerrit install</a></li>
<li><a href="./point-of-view/huawei-and-operators.html">Huawei and telecom operators</a></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-28 15:51</p>
<p><img src="tools/2023-01-17-13-39-31.png" alt="" /></p>
<p>Gerrit is the current mainstream code review tool. It directly adds a layer of code review verification to code push and code merge, and it is also an important link and tool in the continuous integration tool chain.</p>
<p>The gerrit deployment process is as follows:</p>
<ol>
<li>Install jdk1.8</li>
<li>install httpd</li>
</ol>
<p>Download gerrit, such as: gerrit-2.12.4.war</p>
<pre><code>Gerrit 2.12.4 https://www.gerritcodereview.com/download/gerrit-2.12.4.war
</code></pre>
<p>gerrit management account</p>
<pre><code>sudo adduser gerrit
sudo passwd gerrit
</code></pre>
<p>And add gerrit to sudo permission</p>
<pre><code>sudo visudo
gerrit  ALL=(ALL:ALL) ALL
</code></pre>
<p>install gerrit</p>
<pre><code>java -jar gerrit-2.11.3.war init
</code></pre>
<p>Start the gerrit service</p>
<pre><code>[gerrit@promote review2]$ /etc/init.d/gerrit.sh start
** ERROR: GERRIT_SITE not set
[gerrit@promote review2]$ pwd
/home/gerrit/review2
[gerrit@promote review2]$ /etc/init.d/gerrit.sh start
** ERROR: GERRIT_SITE not set
[gerrit@promote review2]$ export GERRIT_SITE=/home/gerrit/review2
[gerrit@promote review2]$ /etc/init.d/gerrit.sh status
Checking arguments to Gerrit Code Review:
GERRIT_SITE     =  /home/gerrit/review2
GERRIT_CONFIG   =  /home/gerrit/review2/etc/gerrit.config
GERRIT_PID      =  /home/gerrit/review2/logs/gerrit.pid
GERRIT_TMP      =  /home/gerrit/review2/tmp
GERRIT_WAR      =  /home/gerrit/review2/bin/gerrit.war
GERRIT_FDS      =  1024
GERRIT_USER     =  gerrit
JAVA            =  /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.141-2.6.10.1.el7_3.x86_64/jre/bin/java
JAVA_OPTIONS    =  
RUN_EXEC        =  /usr/bin/perl -e '$x=$ENV{JAVA};exec $x @ARGV;die $!' -- GerritCodeReview
RUN_ARGS        =  -jar /home/gerrit/review2/bin/gerrit.war daemon -d /home/gerrit/review2

[gerrit@promote review2]$ /etc/init.d/gerrit.sh start
Starting Gerrit Code Review: OK
[root@promote etc]# cd /etc/httpd/
[root@promote httpd]# ls
conf  conf.d  conf.modules.d  logs  modules  run
[root@promote httpd]# cd conf.d/
[root@promote conf.d]# ls
autoindex.conf  gerrit.conf  README  userdir.conf  welcome.conf
</code></pre>
<p>Modify the configuration file as needed</p>
<pre><code>[root@promote conf.d]# vim gerrit.conf 
</code></pre>
<p>Configure the gerrit account password</p>
<pre><code>[root@promote ~]# htpasswd -m /etc/gerrit.passwd hanwei
htpasswd: cannot modify file /etc/gerrit.passwd; use '-c' to create it
[root@promote ~]# touch /etc/gerrit.passwd
[root@promote ~]# htpasswd -m /etc/gerrit.passwd hanwei
New password: 
Re-type new password: 
Adding password for user hanwei
</code></pre>
<p>Open the browser, open the main interface of gerrit, and log in to gerrit with the account just created.</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-03-10 21:25</p>
<h1 id="huawei-and-chinas-telecom-operators"><a class="header" href="#huawei-and-chinas-telecom-operators">Huawei and China's telecom operators</a></h1>
<p>Over the past few decades, operators such as Huawei and China's telecom operators have reached a long-term tacit understanding, that is, operators are responsible for operations, and Huawei provides solutions, equipment, and technologies. </p>
<p>With the development of the times and the popularity of open source, technical barriers have been broken, and operators have also started to develop technology. Operators first broke this harmony, cut off Huawei's financial resources, and gave priority to internal procurement when bidding, destroying the rules of fair competition. Huawei is not a vegetarian either. It also does what operators do, starting a public cloud.</p>
<p>In the past, Huawei did not participate in the operation of the public cloud. Huawei originally wanted to make money from technology, while operators made money from operations. Huawei's technology and global resources are by no means comparable to ordinary companies or even operators. There is nothing Huawei can't do if it wants to do something. As long as Huawei can do it, you need to know how many opponents Huawei has already put down in today's technology-heavy world.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="sidebar.js"></script>
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>