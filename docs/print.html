<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>English version of the website https://www.backendcloud.cn/</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="style.css">
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');
                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }
                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="SUMMARY.html"><strong aria-hidden="true">1.</strong> Catalog: Cloud Native</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="cloud-native/deploy-kubevirt.html"><strong aria-hidden="true">1.1.</strong> Deploy Kubernetes + KubeVirt and the basic use of KubeVirt</a></li><li class="chapter-item expanded "><a href="cloud-native/docker-java-demo.html"><strong aria-hidden="true">1.2.</strong> docker hello-world project</a></li><li class="chapter-item expanded "><a href="cloud-native/deploy-cinder-csi-plugin.html"><strong aria-hidden="true">1.3.</strong> Several problems encountered in deploying cinder-csi-plugin</a></li><li class="chapter-item expanded "><a href="cloud-native/go1.18-workspace.html"><strong aria-hidden="true">1.4.</strong> New in Go 1.18 - Workspaces</a></li><li class="chapter-item expanded "><a href="cloud-native/spring-boot-async-demo.html"><strong aria-hidden="true">1.5.</strong> spring-boot asynchronous example</a></li><li class="chapter-item expanded "><a href="cloud-native/plg.html"><strong aria-hidden="true">1.6.</strong> PLG implements Kubernetes Pod log collection and display</a></li><li class="chapter-item expanded "><a href="cloud-native/fluentd.html"><strong aria-hidden="true">1.7.</strong> Fluentd implements Kubernetes Pod log collection</a></li><li class="chapter-item expanded "><a href="cloud-native/java-dev-memo-1.html"><strong aria-hidden="true">1.8.</strong> Little records in Java project development (1)</a></li><li class="chapter-item expanded "><a href="cloud-native/jdk-bug.html"><strong aria-hidden="true">1.9.</strong> JDK bug - Encrypt Private Key failed unrecognized algorithm name PBEWithSHA1AndDESede</a></li><li class="chapter-item expanded "><a href="cloud-native/tenant-cmp.html"><strong aria-hidden="true">1.10.</strong> Implementation of the function of creating resource pool tenants on the cloud management platform</a></li><li class="chapter-item expanded "><a href="cloud-native/test-service-performance-2.html"><strong aria-hidden="true">1.11.</strong> iperf3 tests the four-layer performance of Kubernetes Service (Part 2)</a></li><li class="chapter-item expanded "><a href="cloud-native/test-service-performance-1.html"><strong aria-hidden="true">1.12.</strong> iperf3 tests the four-layer performance of Kubernetes Service (Part 1)</a></li><li class="chapter-item expanded "><a href="cloud-native/k8s-cmd.html"><strong aria-hidden="true">1.13.</strong> k8s common command</a></li><li class="chapter-item expanded "><a href="cloud-native/cpu-binding.html"><strong aria-hidden="true">1.14.</strong> k8s supports container core binding</a></li><li class="chapter-item expanded "><a href="cloud-native/capability.html"><strong aria-hidden="true">1.15.</strong> k8s supports Capability mechanism</a></li><li class="chapter-item expanded "><a href="cloud-native/oom.html"><strong aria-hidden="true">1.16.</strong> k8s OOMkilled container exceeding memory limit</a></li></ol></li><li class="chapter-item expanded "><a href="SUMMARY.html"><strong aria-hidden="true">2.</strong> Catalog: Openstack</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="openstack/deploy-openstack-stein.html"><strong aria-hidden="true">2.1.</strong> Problems encountered in Openstack Stein deployment</a></li><li class="chapter-item expanded "><a href="openstack/sriov2ovs.html"><strong aria-hidden="true">2.2.</strong> sriov computing node to ovs computing node script</a></li><li class="chapter-item expanded "><a href="openstack/placement.html"><strong aria-hidden="true">2.3.</strong> Openstack Placement</a></li><li class="chapter-item expanded "><a href="openstack/power-arch.html"><strong aria-hidden="true">2.4.</strong> POWER architecture server as computing node</a></li><li class="chapter-item expanded "><a href="openstack/sriov.html"><strong aria-hidden="true">2.5.</strong> OpenStack practices SR-IOV computing nodes</a></li><li class="chapter-item expanded "><a href="openstack/openstack-op-4.html"><strong aria-hidden="true">2.6.</strong> Openstack operation and maintenance records (4)</a></li><li class="chapter-item expanded "><a href="openstack/openstack-op-3.html"><strong aria-hidden="true">2.7.</strong> Openstack operation and maintenance records (3)</a></li><li class="chapter-item expanded "><a href="openstack/openstack-op-2.html"><strong aria-hidden="true">2.8.</strong> Openstack operation and maintenance records (2)</a></li><li class="chapter-item expanded "><a href="openstack/openstack-op-1.html"><strong aria-hidden="true">2.9.</strong> Openstack operation and maintenance records (1)</a></li><li class="chapter-item expanded "><a href="openstack/novnc-problem.html"><strong aria-hidden="true">2.10.</strong> OpenStack Pike dashboard noVNC cannot be accessed problem</a></li><li class="chapter-item expanded "><a href="openstack/openstack-local-yum.html"><strong aria-hidden="true">2.11.</strong> Openstack Pike local yum source construction</a></li><li class="chapter-item expanded "><a href="openstack/cpu-binding.html"><strong aria-hidden="true">2.12.</strong> CPU binding</a></li><li class="chapter-item expanded "><a href="openstack/resize-fail.html"><strong aria-hidden="true">2.13.</strong> Investigation of the cause of resize failure</a></li><li class="chapter-item expanded "><a href="openstack/mount-cloud-disk.html"><strong aria-hidden="true">2.14.</strong> Mount cloud disk</a></li><li class="chapter-item expanded "><a href="openstack/ocata-nova-evacuate-bug.html"><strong aria-hidden="true">2.15.</strong> Ocata nova evacuate bug</a></li><li class="chapter-item expanded "><a href="openstack/compute-node-ha.html"><strong aria-hidden="true">2.16.</strong> compute node ha mainstream open source implementation</a></li><li class="chapter-item expanded "><a href="openstack/collectd-influxdb.html"><strong aria-hidden="true">2.17.</strong> Deployment and usage of Collectd and InfluxDB</a></li><li class="chapter-item expanded "><a href="openstack/live-migration-local.html"><strong aria-hidden="true">2.18.</strong> Live Migration on Local Storage</a></li><li class="chapter-item expanded "><a href="openstack/fast-evacuation.html"><strong aria-hidden="true">2.19.</strong> Add fast evacuation function to VM HA procedure</a></li><li class="chapter-item expanded "><a href="openstack/vm-activation-failure.html"><strong aria-hidden="true">2.20.</strong> Windows virtual machine activation failure problem</a></li></ol></li><li class="chapter-item expanded "><a href="SUMMARY.html"><strong aria-hidden="true">3.</strong> Catalog: Point of view</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="point-of-view/huawei-and-operators.html"><strong aria-hidden="true">3.1.</strong> Huawei and telecom operators</a></li></ol></li><li class="chapter-item expanded "><a href="SUMMARY.html"><strong aria-hidden="true">4.</strong> Open source small project</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="open-source-small-project/firewall-tool.html"><strong aria-hidden="true">4.1.</strong> Open source a node.js firewall tool</a></li></ol></li><li class="chapter-item expanded "><a href="SUMMARY.html"><strong aria-hidden="true">5.</strong> Catalog: Tools</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tools/github-action.html"><strong aria-hidden="true">5.1.</strong> Github Action Supplementary Introduction</a></li><li class="chapter-item expanded "><a href="tools/blog-cicd.html"><strong aria-hidden="true">5.2.</strong> Github Action Upgrade backend cloud website CICD process</a></li><li class="chapter-item expanded "><a href="tools/github-action-demo.html"><strong aria-hidden="true">5.3.</strong> Try the Github Action CI/CD process (create a React project and package it for deployment)</a></li><li class="chapter-item expanded "><a href="tools/notion-widget.html"><strong aria-hidden="true">5.4.</strong> Notion's page insert widget Widget</a></li><li class="chapter-item expanded "><a href="tools/git-dir.html"><strong aria-hidden="true">5.5.</strong> Internal structure of .git directory</a></li><li class="chapter-item expanded "><a href="tools/xshell-skill.html"><strong aria-hidden="true">5.6.</strong> Some Skills of using xshell tools</a></li><li class="chapter-item expanded "><a href="tools/gerrit-install.html"><strong aria-hidden="true">5.7.</strong> gerrit install</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">English version of the website https://www.backendcloud.cn/</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <!-- Page table of contents -->
                        <div class="sidetoc"><nav class="pagetoc"></nav></div>

                        <h1 id="summary"><a class="header" href="#summary">Summary</a></h1>
<ul>
<li>
<p><a href="./SUMMARY.html">Catalog: Cloud Native</a></p>
<ul>
<li>
<p><a href="./cloud-native/deploy-kubevirt.html">Deploy Kubernetes + KubeVirt and the basic use of KubeVirt</a> ------ release time :2022-05-06 14:17</p>
</li>
<li>
<p><a href="./cloud-native/docker-java-demo.html">docker hello-world project</a> ------ release time :2022-05-02 10:13</p>
</li>
<li>
<p><a href="./cloud-native/deploy-cinder-csi-plugin.html">Several problems encountered in deploying cinder-csi-plugin</a> ------ release time :2022-04-24 11:00</p>
</li>
<li>
<p><a href="./cloud-native/go1.18-workspace.html">New in Go 1.18 - Workspaces</a> ------ release time :2022-04-13 20:17</p>
</li>
<li>
<p><a href="./cloud-native/spring-boot-async-demo.html">spring-boot asynchronous example</a> ------ release time :2022-03-25 11:24</p>
</li>
<li>
<p><a href="./cloud-native/plg.html">PLG implements Kubernetes Pod log collection and display</a> ------ release time :2021-12-01 17:48</p>
</li>
<li>
<p><a href="./cloud-native/fluentd.html">Fluentd implements Kubernetes Pod log collection</a> ------ release time :2021-11-30 16:36</p>
</li>
<li>
<p><a href="./cloud-native/java-dev-memo-1.html">Little records in Java project development (1)</a> ------ release time :2021-11-23 15:04</p>
</li>
<li>
<p><a href="./cloud-native/jdk-bug.html">JDK bug - Encrypt Private Key failed unrecognized algorithm name PBEWithSHA1AndDESede</a> ------ release time :2021-11-23 14:22</p>
</li>
<li>
<p><a href="./cloud-native/tenant-cmp.html">Implementation of the function of creating resource pool tenants on the cloud management platform</a> ------ release time :2021-09-13 14:22</p>
</li>
<li>
<p><a href="./cloud-native/test-service-performance-2.html">iperf3 tests the four-layer performance of Kubernetes Service (Part 2)</a> ------ release time :2021-08-10 07:06</p>
</li>
<li>
<p><a href="./cloud-native/test-service-performance-1.html">iperf3 tests the four-layer performance of Kubernetes Service (Part 1)</a> ------ release time :2021-08-09 17:06</p>
</li>
<li>
<p><a href="./cloud-native/k8s-cmd.html">k8s common command</a> ------ release time :2020-05-27 17:32</p>
</li>
<li>
<p><a href="./cloud-native/cpu-binding.html">k8s supports container core binding</a> ------ release time :2020-05-22 16:24</p>
</li>
<li>
<p><a href="./cloud-native/capability.html">k8s supports Capability mechanism</a> ------ release time :2020-05-22 15:18</p>
</li>
<li>
<p><a href="./cloud-native/oom.html">k8s OOMkilled container exceeding memory limit</a> ------ release time :2020-05-22 13:52</p>
</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Openstack</a></p>
<ul>
<li>
<p><a href="./openstack/deploy-openstack-stein.html">Problems encountered in Openstack Stein deployment</a> ------ release time :2019-09-20 18:49</p>
</li>
<li>
<p><a href="./openstack/sriov2ovs.html">sriov computing node to ovs computing node script</a> ------ release time :2019-04-19 16:50</p>
</li>
<li>
<p><a href="./openstack/placement.html">Openstack Placement</a> ------ release time :2019-04-18 09:14</p>
</li>
<li>
<p><a href="./openstack/power-arch.html">POWER architecture server as computing node</a> ------ release time :2018-10-21 16:21</p>
</li>
<li>
<p><a href="./openstack/sriov.html">OpenStack practices SR-IOV computing nodes</a> ------ release time :2018-10-14 05:15</p>
</li>
<li>
<p><a href="./openstack/openstack-op-4.html">Openstack operation and maintenance records (4)</a> ------ release time :2020-01-01 19:13</p>
</li>
<li>
<p><a href="./openstack/openstack-op-3.html">Openstack operation and maintenance records (3)</a> ------ release time :2018-09-29 20:28</p>
</li>
<li>
<p><a href="./openstack/openstack-op-2.html">Openstack operation and maintenance records (2)</a> ------ release time :2018-08-28 14:48</p>
</li>
<li>
<p><a href="./openstack/openstack-op-1.html">Openstack operation and maintenance records (1)</a> ------ release time :2018-08-26 18:38</p>
</li>
<li>
<p><a href="./openstack/novnc-problem.html">OpenStack Pike dashboard noVNC cannot be accessed problem</a> ------ release time :2018-04-14 05:18</p>
</li>
<li>
<p><a href="./openstack/openstack-local-yum.html">Openstack Pike local yum source construction</a> ------ release time :2017-12-28 22:20</p>
</li>
<li>
<p><a href="./openstack/cpu-binding.html">CPU binding</a> ------ release time :2017-07-20 18:30</p>
</li>
<li>
<p><a href="./openstack/resize-fail.html">Investigation of the cause of resize failure</a> ------ release time :2017-07-04 10:20</p>
</li>
<li>
<p><a href="./openstack/mount-cloud-disk.html">Mount cloud disk</a> ------ release time :2017-06-30 05:16</p>
</li>
<li>
<p><a href="./openstack/ocata-nova-evacuate-bug.html">Ocata nova evacuate bug</a> ------ release time :2017-06-28 18:24</p>
</li>
<li>
<p><a href="./openstack/compute-node-ha.html">compute node ha mainstream open source implementation</a> ------ release time :2017-06-22 09:16</p>
</li>
<li>
<p><a href="./openstack/collectd-influxdb.html">Deployment and usage of Collectd and InfluxDB</a> ------ release time :2017-06-15 06:47</p>
</li>
<li>
<p><a href="./openstack/live-migration-local.html">Live Migration on Local Storage</a> ------ release time :2017-06-14 15:25</p>
</li>
<li>
<p><a href="./openstack/fast-evacuation.html">Add fast evacuation function to VM HA procedure</a> ------ release time :2017-06-10 03:27</p>
</li>
<li>
<p><a href="./openstack/vm-activation-failure.html">Windows virtual machine activation failure problem</a> ------ release time :2017-05-04 22:21</p>
</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Point of view</a></p>
<ul>
<li><a href="./point-of-view/huawei-and-operators.html">Huawei and telecom operators</a> ------ release time :2017-03-10 21:25</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Open source small project</a></p>
<ul>
<li><a href="./open-source-small-project/firewall-tool.html">Open source a node.js firewall tool</a> ------ release time :2022-05-07 09:41</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Tools</a></p>
<ul>
<li>
<p><a href="./tools/github-action.html">Github Action Supplementary Introduction</a> ------ release time :2022-04-12 13:04</p>
</li>
<li>
<p><a href="./tools/blog-cicd.html">Github Action Upgrade backend cloud website CICD process</a> ------ release time :2022-04-03 00:17</p>
</li>
<li>
<p><a href="./tools/github-action-demo.html">Try the Github Action CI/CD process (create a React project and package it for deployment)</a> ------ release time :2022-02-25 11:54</p>
</li>
<li>
<p><a href="./tools/notion-widget.html">Notion's page insert widget Widget</a> ------ release time :2022-01-06 11:06</p>
</li>
<li>
<p><a href="./tools/git-dir.html">Internal structure of .git directory</a> ------ release time :2023-01-18 18:37</p>
</li>
<li>
<p><a href="./tools/xshell-skill.html">Some Skills of using xshell tools</a> ------ release time :2016-07-05 22:57</p>
</li>
<li>
<p><a href="./tools/gerrit-install.html">gerrit install</a> ------ release time :2017-06-28 15:51</p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2022-05-06 14:17</p>
<p>The KubeVirt project allows virtual machines to be managed by Kubernetes like containers.</p>
<h1 id="deploy-kubernetes"><a class="header" href="#deploy-kubernetes">deploy Kubernetes</a></h1>
<p>Basic environment - execute on all machines</p>
<pre><code>hostnamectl set-hostname xxxx

systemctl stop firewalld
systemctl disable firewalld

sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

swapoff -a  
sed -ri 's/.*swap.*/#&amp;/' /etc/fstab

cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system

yum install -y yum-utils
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce-20.10.7 docker-ce-cli-20.10.7  containerd.io-1.4.6
systemctl start docker &amp;&amp; systemctl enable docker &amp;&amp; systemctl status docker

mkdir -p /etc/docker
tee /etc/docker/daemon.json &lt;&lt;-'EOF'
{
&quot;registry-mirrors&quot;: [&quot;https://82m9ar63.mirror.aliyuncs.com&quot;],
&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
&quot;log-driver&quot;: &quot;json-file&quot;,
&quot;log-opts&quot;: {
&quot;max-size&quot;: &quot;100m&quot;
},
&quot;storage-driver&quot;: &quot;overlay2&quot;
}
EOF
systemctl daemon-reload
systemctl restart docker
</code></pre>
<p>Install kubelet, kubeadm, kubectl - execute on all machines</p>
<pre><code>cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF


sudo yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes=kubernetes

sudo systemctl enable --now kubelet
</code></pre>
<p>Download the images required by each machine - execute on all machines</p>
<pre><code>sudo tee ./images.sh &lt;&lt;-'EOF'
#!/bin/bash
images=(
kube-apiserver:v1.20.9
kube-proxy:v1.20.9
kube-controller-manager:v1.20.9
kube-scheduler:v1.20.9
coredns:1.7.0
etcd:3.4.13-0
pause:3.2
)
for imageName in ${images[@]} ; do
docker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/$imageName
done
EOF

chmod +x ./images.sh &amp;&amp; ./images.sh

echo &quot;192.168.159.133  cluster-endpoint k8s-master&quot; &gt;&gt; /etc/hosts
echo &quot;192.168.159.134  k8s-node01&quot; &gt;&gt; /etc/hosts
</code></pre>
<p>Initialize the master node - execute on the master node</p>
<pre><code>kubeadm init \
--apiserver-advertise-address=192.168.159.133 \
--control-plane-endpoint=cluster-endpoint \
--image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \
--kubernetes-version v1.20.9 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=192.168.0.0/16
</code></pre>
<pre><code class="language-bash">[init] Using Kubernetes version: v1.20.9
[preflight] Running pre-flight checks
        [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
        [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/
        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.7. Latest validated version: 19.03
        [WARNING Hostname]: hostname &quot;k8s-master&quot; could not be reached
        [WARNING Hostname]: hostname &quot;k8s-master&quot;: lookup k8s-master on 192.168.159.2:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;
[certs] Generating &quot;ca&quot; certificate and key
[certs] Generating &quot;apiserver&quot; certificate and key
[certs] apiserver serving cert is signed for DNS names [cluster-endpoint k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.159.133]
[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key
[certs] Generating &quot;front-proxy-ca&quot; certificate and key
[certs] Generating &quot;front-proxy-client&quot; certificate and key
[certs] Generating &quot;etcd/ca&quot; certificate and key
[certs] Generating &quot;etcd/server&quot; certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.159.133 127.0.0.1 ::1]
[certs] Generating &quot;etcd/peer&quot; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.159.133 127.0.0.1 ::1]
[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key
[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key
[certs] Generating &quot;sa&quot; key and public key
[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;
[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;
[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;
[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;
[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;
[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;
[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.503023 seconds
[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace
[kubelet] Creating a ConfigMap &quot;kubelet-config-1.20&quot; in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master as control-plane by adding the labels &quot;node-role.kubernetes.io/master=''&quot; and &quot;node-role.kubernetes.io/control-plane='' (deprecated)&quot;
[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 6ibx3k.dwc77g1lgrmsdd3b
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace
[kubelet-finalize] Updating &quot;/etc/kubernetes/kubelet.conf&quot; to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join cluster-endpoint:6443 --token 6ibx3k.dwc77g1lgrmsdd3b \
    --discovery-token-ca-cert-hash sha256:241901cd93dd1e7c08f43316f2d6d6c968ac2d38e463509c234f301025d8d191 \
    --control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join cluster-endpoint:6443 --token 6ibx3k.dwc77g1lgrmsdd3b \
    --discovery-token-ca-cert-hash sha256:241901cd93dd1e7c08f43316f2d6d6c968ac2d38e463509c234f301025d8d191

[root@localhost ~]# mkdir -p $HOME/.kube
[root@localhost ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@localhost ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config
[root@localhost ~]# kubectl get nodes
NAME         STATUS     ROLES                  AGE     VERSION
k8s-master   NotReady   control-plane,master   2m10s   v1.20.9
[root@localhost ~]# kubectl get pods -A
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE
kube-system   coredns-5897cd56c4-gnrrz             0/1     Pending   0          2m52s
kube-system   coredns-5897cd56c4-hwbc8             0/1     Pending   0          2m52s
kube-system   etcd-k8s-master                      1/1     Running   0          3m6s
kube-system   kube-apiserver-k8s-master            1/1     Running   0          3m6s
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          3m6s
kube-system   kube-proxy-kklkn                     1/1     Running   0          2m52s
kube-system   kube-scheduler-k8s-master            1/1     Running   0          3m8s
[root@localhost ~]# 
</code></pre>
<h1 id="install-network-components---executed-on-the-master-node"><a class="header" href="#install-network-components---executed-on-the-master-node">Install network components - executed on the master node</a></h1>
<pre><code class="language-bash">[root@localhost ~]# curl https://docs.projectcalico.org/manifests/calico.yaml -O
[root@localhost ~]# ls
anaconda-ks.cfg  calico.yaml  images.sh
[root@localhost ~]# cat calico.yaml |grep 192.168
            #   value: &quot;192.168.0.0/16&quot;
[root@localhost ~]# vi calico.yaml 
</code></pre>
<pre><code>        # - name: CALICO_IPV4POOL_CIDR
        #   value: &quot;192.168.0.0/16&quot;
</code></pre>
<p>changed to</p>
<pre><code>        - name: CALICO_IPV4POOL_CIDR
        value: &quot;10.97.0.0/16&quot;
</code></pre>
<pre><code class="language-bash">[root@localhost ~]# kubectl apply -f calico.yaml 
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created

[root@localhost ~]# kubectl get node
NAME         STATUS   ROLES                  AGE   VERSION
k8s-master   Ready    control-plane,master   14m   v1.20.9
[root@localhost ~]# kubectl get pod -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6fcb5c5bcf-wghrm   1/1     Running   0          3m23s
kube-system   calico-node-pj9rm                          1/1     Running   0          3m24s
kube-system   coredns-5897cd56c4-gnrrz                   1/1     Running   0          14m
kube-system   coredns-5897cd56c4-hwbc8                   1/1     Running   0          14m
kube-system   etcd-k8s-master                            1/1     Running   0          14m
kube-system   kube-apiserver-k8s-master                  1/1     Running   0          14m
kube-system   kube-controller-manager-k8s-master         1/1     Running   0          14m
kube-system   kube-proxy-kklkn                           1/1     Running   0          14m
kube-system   kube-scheduler-k8s-master                  1/1     Running   0          14m
[root@localhost ~]# 
</code></pre>
<p>Join the node node</p>
<pre><code>[root@localhost ~]# kubeadm join cluster-endpoint:6443 --token 6ibx3k.dwc77g1lgrmsdd3b \
    --discovery-token-ca-cert-hash sha256:241901cd93dd1e7c08f43316f2d6d6c968ac2d38e463509c234f301025d8d191

[root@k8s-node01 ~]# kubeadm join cluster-endpoint:6443 --token 6ibx3k.dwc77g1lgrmsdd3b     --discovery-token-ca-cert-hash sha256:241901cd93dd1e7c08f43316f2d6d6c968ac2d38e463509c234f301025d8d191
[preflight] Running pre-flight checks
        [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/
        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.7. Latest validated version: 19.03
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;
[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

[root@k8s-master ~]# kubectl get node
NAME         STATUS   ROLES                  AGE   VERSION
k8s-master   Ready    control-plane,master   30m   v1.20.9
k8s-node01   Ready    &lt;none&gt;                 63s   v1.20.9
[root@k8s-master ~]# kubectl get pod -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6fcb5c5bcf-wghrm   1/1     Running   0          19m
kube-system   calico-node-6clkb                          1/1     Running   0          68s
kube-system   calico-node-pj9rm                          1/1     Running   0          19m
kube-system   coredns-5897cd56c4-gnrrz                   1/1     Running   0          30m
kube-system   coredns-5897cd56c4-hwbc8                   1/1     Running   0          30m
kube-system   etcd-k8s-master                            1/1     Running   0          30m
kube-system   kube-apiserver-k8s-master                  1/1     Running   0          30m
kube-system   kube-controller-manager-k8s-master         1/1     Running   0          30m
kube-system   kube-proxy-9zf4v                           1/1     Running   0          68s
kube-system   kube-proxy-kklkn                           1/1     Running   0          30m
kube-system   kube-scheduler-k8s-master                  1/1     Running   0          30m
[root@k8s-master ~]# 
[root@localhost ~]# kubeadm token create --print-join-command
</code></pre>
<h1 id="deploy-kubevirt"><a class="header" href="#deploy-kubevirt">deploy KubeVirt</a></h1>
<pre><code>controlplane $ export KUBEVIRT_VERSION=$(curl -s https://api.github.com/repos/kubevirt/kubevirt/releases/latest | jq -r .tag_name)
controlplane $ echo $KUBEVIRT_VERSION
v0.52.0
controlplane $ export KUBEVIRT_VERSION=v0.49.0

# deploy the KubeVirt Operator
controlplane $ kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/${KUBEVIRT_VERSION}/kubevirt-operator.yaml
namespace/kubevirt created
customresourcedefinition.apiextensions.k8s.io/kubevirts.kubevirt.io created
priorityclass.scheduling.k8s.io/kubevirt-cluster-critical created
clusterrole.rbac.authorization.k8s.io/kubevirt.io:operator created
serviceaccount/kubevirt-operator created
role.rbac.authorization.k8s.io/kubevirt-operator created
rolebinding.rbac.authorization.k8s.io/kubevirt-operator-rolebinding created
clusterrole.rbac.authorization.k8s.io/kubevirt-operator created
clusterrolebinding.rbac.authorization.k8s.io/kubevirt-operator created
deployment.apps/virt-operator created

# creating a Custom Resource
controlplane $ kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/${KUBEVIRT_VERSION}/kubevirt-cr.yaml
kubevirt.kubevirt.io/kubevirt created
controlplane $ kubectl -n kubevirt patch kubevirt kubevirt --type=merge --patch '{&quot;spec&quot;:{&quot;configuration&quot;:{&quot;developerConfiguration&quot;:{&quot;useEmulation&quot;:true}}}}'
kubevirt.kubevirt.io/kubevirt patched

controlplane $ kubectl get pods -n kubevirt
NAME                               READY   STATUS              RESTARTS   AGE
virt-api-8959bd8d6-h4wgf           1/1     Running             0          49s
virt-api-8959bd8d6-kq5gm           1/1     Running             0          50s
virt-controller-6fc6b9b7cf-4ftfc   0/1     ContainerCreating   0          19s
virt-controller-6fc6b9b7cf-8t8fw   0/1     Running             0          19s
virt-handler-2t4zb                 0/1     Init:0/1            0          19s
virt-operator-5c74687999-8769q     1/1     Running             0          2m9s
virt-operator-5c74687999-wp24r     1/1     Running             0          2m9s
controlplane $ kubectl get pods -n kubevirt
NAME                               READY   STATUS    RESTARTS   AGE
virt-api-8959bd8d6-h4wgf           1/1     Running   0          3m3s
virt-api-8959bd8d6-kq5gm           1/1     Running   0          3m4s
virt-controller-6fc6b9b7cf-4ftfc   1/1     Running   0          2m33s
virt-controller-6fc6b9b7cf-8t8fw   1/1     Running   0          2m33s
virt-handler-2t4zb                 1/1     Running   0          2m33s
virt-operator-5c74687999-8769q     1/1     Running   0          4m23s
virt-operator-5c74687999-wp24r     1/1     Running   0          4m23s

controlplane $ wget -O virtctl https://github.com/kubevirt/kubevirt/releases/download/${KUBEVIRT_VERSION}/virtctl-${KUBEVIRT_VERSION}-linux-amd64
--2022-05-06 06:15:07--  https://github.com/kubevirt/kubevirt/releases/download/v0.49.0/virtctl-v0.49.0-linux-amd64
Resolving github.com (github.com)... 140.82.121.4
Connecting to github.com (github.com)|140.82.121.4|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/76686583/1cacd2e3-3748-4498-a87b-6c0217cca458?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20220506T061509Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=fdd77e733da9bd98d2adc462718f125ca761d4a3c4325038bd63b61058483806&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=76686583&amp;response-content-disposition=attachment%3B%20filename%3Dvirtctl-v0.49.0-linux-amd64&amp;response-content-type=application%2Foctet-stream [following]
--2022-05-06 06:15:08--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/76686583/1cacd2e3-3748-4498-a87b-6c0217cca458?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20220506T061509Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=fdd77e733da9bd98d2adc462718f125ca761d4a3c4325038bd63b61058483806&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=76686583&amp;response-content-disposition=attachment%3B%20filename%3Dvirtctl-v0.49.0-linux-amd64&amp;response-content-type=application%2Foctet-stream
Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...
Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 52294614 (50M) [application/octet-stream]
Saving to: ‘virtctl’

virtctl                            100%[==============================================================&gt;]  49.87M  26.7MB/s    in 1.9s    

2022-05-06 06:15:10 (26.7 MB/s) - ‘virtctl’ saved [52294614/52294614]

controlplane $ chmod +x virtctl
controlplane $ ls
go  virtctl
</code></pre>
<h1 id="basic-use-of-kubevirt"><a class="header" href="#basic-use-of-kubevirt">Basic use of KubeVirt</a></h1>
<p>The purpose of KubeVirt is to allow the virtual machine to run in the container. Here are a few basic operations of KubeVirt:</p>
<ul>
<li>create &amp; start virtual machine</li>
<li>vnc login virtual machine</li>
<li>stop &amp; delete virtual machine</li>
</ul>
<pre><code class="language-bash">controlplane $ curl https://kubevirt.io/labs/manifests/vm.yaml
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: testvm
spec:
  running: false
  template:
    metadata:
      labels:
        kubevirt.io/size: small
        kubevirt.io/domain: testvm
    spec:
      domain:
        devices:
          disks:
            - name: containerdisk
              disk:
                bus: virtio
            - name: cloudinitdisk
              disk:
                bus: virtio
          interfaces:
          - name: default
            masquerade: {}
        resources:
          requests:
            memory: 64M
      networks:
      - name: default
        pod: {}
      volumes:
        - name: containerdisk
          containerDisk:
            image: quay.io/kubevirt/cirros-container-disk-demo
        - name: cloudinitdisk
          cloudInitNoCloud:
            userDataBase64: SGkuXG4=

controlplane $ kubectl apply -f https://kubevirt.io/labs/manifests/vm.yaml
virtualmachine.kubevirt.io/testvm created
controlplane $ kubectl get vms
NAME     AGE   STATUS    READY
testvm   2s    Stopped   False
controlplane $ kubectl get vms -o yaml testvm
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {&quot;apiVersion&quot;:&quot;kubevirt.io/v1&quot;,&quot;kind&quot;:&quot;VirtualMachine&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;testvm&quot;,&quot;namespace&quot;:&quot;default&quot;},&quot;spec&quot;:{&quot;running&quot;:false,&quot;template&quot;:{&quot;metadata&quot;:{&quot;labels&quot;:{&quot;kubevirt.io/domain&quot;:&quot;testvm&quot;,&quot;kubevirt.io/size&quot;:&quot;small&quot;}},&quot;spec&quot;:{&quot;domain&quot;:{&quot;devices&quot;:{&quot;disks&quot;:[{&quot;disk&quot;:{&quot;bus&quot;:&quot;virtio&quot;},&quot;name&quot;:&quot;containerdisk&quot;},{&quot;disk&quot;:{&quot;bus&quot;:&quot;virtio&quot;},&quot;name&quot;:&quot;cloudinitdisk&quot;}],&quot;interfaces&quot;:[{&quot;masquerade&quot;:{},&quot;name&quot;:&quot;default&quot;}]},&quot;resources&quot;:{&quot;requests&quot;:{&quot;memory&quot;:&quot;64M&quot;}}},&quot;networks&quot;:[{&quot;name&quot;:&quot;default&quot;,&quot;pod&quot;:{}}],&quot;volumes&quot;:[{&quot;containerDisk&quot;:{&quot;image&quot;:&quot;quay.io/kubevirt/cirros-container-disk-demo&quot;},&quot;name&quot;:&quot;containerdisk&quot;},{&quot;cloudInitNoCloud&quot;:{&quot;userDataBase64&quot;:&quot;SGkuXG4=&quot;},&quot;name&quot;:&quot;cloudinitdisk&quot;}]}}}}
    kubevirt.io/latest-observed-api-version: v1
    kubevirt.io/storage-observed-api-version: v1alpha3
  creationTimestamp: &quot;2022-05-06T06:32:17Z&quot;
  generation: 1
  managedFields:
  - apiVersion: kubevirt.io/v1alpha3
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          f:kubevirt.io/latest-observed-api-version: {}
          f:kubevirt.io/storage-observed-api-version: {}
      f:status:
        .: {}
        f:conditions: {}
        f:printableStatus: {}
        f:volumeSnapshotStatuses: {}
    manager: Go-http-client
    operation: Update
    time: &quot;2022-05-06T06:32:17Z&quot;
  - apiVersion: kubevirt.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:kubectl.kubernetes.io/last-applied-configuration: {}
      f:spec:
        .: {}
        f:running: {}
        f:template:
          .: {}
          f:metadata:
            .: {}
            f:labels:
              .: {}
              f:kubevirt.io/domain: {}
              f:kubevirt.io/size: {}
          f:spec:
            .: {}
            f:domain:
              .: {}
              f:devices:
                .: {}
                f:disks: {}
                f:interfaces: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:memory: {}
            f:networks: {}
            f:volumes: {}
    manager: kubectl
    operation: Update
    time: &quot;2022-05-06T06:32:17Z&quot;
  name: testvm
  namespace: default
  resourceVersion: &quot;5996&quot;
  selfLink: /apis/kubevirt.io/v1/namespaces/default/virtualmachines/testvm
  uid: 1b047ffb-6eeb-4fda-b081-169391886bfb
spec:
  running: false
  template:
    metadata:
      creationTimestamp: null
      labels:
        kubevirt.io/domain: testvm
        kubevirt.io/size: small
    spec:
      domain:
        devices:
          disks:
          - disk:
              bus: virtio
            name: containerdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
          interfaces:
          - masquerade: {}
            name: default
        machine:
          type: q35
        resources:
          requests:
            memory: 64M
      networks:
      - name: default
        pod: {}
      volumes:
      - containerDisk:
          image: quay.io/kubevirt/cirros-container-disk-demo
        name: containerdisk
      - cloudInitNoCloud:
          userDataBase64: SGkuXG4=
        name: cloudinitdisk
status:
  conditions:
  - lastProbeTime: &quot;2022-05-06T06:32:18Z&quot;
    lastTransitionTime: &quot;2022-05-06T06:32:18Z&quot;
    message: VMI does not exist
    reason: VMINotExists
    status: &quot;False&quot;
    type: Ready
  printableStatus: Stopped
  volumeSnapshotStatuses:
  - enabled: false
    name: containerdisk
    reason: Snapshot is not supported for this volumeSource type [containerdisk]
  - enabled: false
    name: cloudinitdisk
    reason: Snapshot is not supported for this volumeSource type [cloudinitdisk]

controlplane $ ./virtctl start testvm
VM testvm was scheduled to start

controlplane $ kubectl get vms
NAME     AGE   STATUS    READY
testvm   40s   Running   True

controlplane $ kubectl get vmis
NAME     AGE    PHASE     IP            NODENAME   READY
testvm   104s   Running   10.244.1.12   node01     True
controlplane $  kubectl get vmis -o yaml testvm
apiVersion: kubevirt.io/v1
kind: VirtualMachineInstance
metadata:
  annotations:
    kubevirt.io/latest-observed-api-version: v1
    kubevirt.io/storage-observed-api-version: v1alpha3
  creationTimestamp: &quot;2022-05-06T06:32:38Z&quot;
  finalizers:
  - kubevirt.io/virtualMachineControllerFinalize
  - foregroundDeleteVirtualMachine
  generation: 9
  labels:
    kubevirt.io/domain: testvm
    kubevirt.io/nodeName: node01
    kubevirt.io/size: small
  managedFields:
  - apiVersion: kubevirt.io/v1alpha3
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:kubevirt.io/latest-observed-api-version: {}
          f:kubevirt.io/storage-observed-api-version: {}
        f:finalizers: {}
        f:labels:
          .: {}
          f:kubevirt.io/domain: {}
          f:kubevirt.io/nodeName: {}
          f:kubevirt.io/size: {}
        f:ownerReferences: {}
      f:spec:
        .: {}
        f:domain:
          .: {}
          f:devices:
            .: {}
            f:disks: {}
            f:interfaces: {}
          f:firmware:
            .: {}
            f:uuid: {}
          f:machine:
            .: {}
            f:type: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:memory: {}
        f:networks: {}
        f:volumes: {}
      f:status:
        .: {}
        f:activePods:
          .: {}
          f:5ed78651-9f18-426f-a248-4bf7b2c8b003: {}
        f:conditions: {}
        f:guestOSInfo: {}
        f:interfaces: {}
        f:launcherContainerImageVersion: {}
        f:migrationMethod: {}
        f:migrationTransport: {}
        f:nodeName: {}
        f:phase: {}
        f:phaseTransitionTimestamps: {}
        f:qosClass: {}
        f:virtualMachineRevisionName: {}
        f:volumeStatus: {}
    manager: Go-http-client
    operation: Update
    time: &quot;2022-05-06T06:32:47Z&quot;
  name: testvm
  namespace: default
  ownerReferences:
  - apiVersion: kubevirt.io/v1
    blockOwnerDeletion: true
    controller: true
    kind: VirtualMachine
    name: testvm
    uid: 1b047ffb-6eeb-4fda-b081-169391886bfb
  resourceVersion: &quot;6141&quot;
  selfLink: /apis/kubevirt.io/v1/namespaces/default/virtualmachineinstances/testvm
  uid: 3721e713-37e9-4871-bb5e-aaad3ab5d44b
spec:
  domain:
    cpu:
      cores: 1
      model: host-model
      sockets: 1
      threads: 1
    devices:
      disks:
      - disk:
          bus: virtio
        name: containerdisk
      - disk:
          bus: virtio
        name: cloudinitdisk
      interfaces:
      - masquerade: {}
        name: default
    features:
      acpi:
        enabled: true
    firmware:
      uuid: 5a9fc181-957e-5c32-9e5a-2de5e9673531
    machine:
      type: q35
    resources:
      requests:
        memory: 64M
  networks:
  - name: default
    pod: {}
  volumes:
  - containerDisk:
      image: quay.io/kubevirt/cirros-container-disk-demo
      imagePullPolicy: Always
    name: containerdisk
  - cloudInitNoCloud:
      userDataBase64: SGkuXG4=
    name: cloudinitdisk
status:
  activePods:
    5ed78651-9f18-426f-a248-4bf7b2c8b003: node01
  conditions:
  - lastProbeTime: null
    lastTransitionTime: &quot;2022-05-06T06:32:46Z&quot;
    status: &quot;True&quot;
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: null
    status: &quot;True&quot;
    type: LiveMigratable
  guestOSInfo: {}
  interfaces:
  - ipAddress: 10.244.1.12
    ipAddresses:
    - 10.244.1.12
    mac: 52:54:00:f8:cf:ce
    name: default
  launcherContainerImageVersion: quay.io/kubevirt/virt-launcher:v0.49.0
  migrationMethod: BlockMigration
  migrationTransport: Unix
  nodeName: node01
  phase: Running
  phaseTransitionTimestamps:
  - phase: Pending
    phaseTransitionTimestamp: &quot;2022-05-06T06:32:39Z&quot;
  - phase: Scheduling
    phaseTransitionTimestamp: &quot;2022-05-06T06:32:39Z&quot;
  - phase: Scheduled
    phaseTransitionTimestamp: &quot;2022-05-06T06:32:46Z&quot;
  - phase: Running
    phaseTransitionTimestamp: &quot;2022-05-06T06:32:48Z&quot;
  qosClass: Burstable
  virtualMachineRevisionName: revision-start-vm-1b047ffb-6eeb-4fda-b081-169391886bfb-2
  volumeStatus:
  - name: cloudinitdisk
    size: 1048576
    target: vdb
  - name: containerdisk
    target: vda

controlplane $ ./virtctl console testvm
Successfully connected to testvm console. The escape sequence is ^]

login as 'cirros' user. default password: 'gocubsgo'. use 'sudo' for root.
testvm login: cirros
Password: 
$ pwd
/home/cirros
$ ls /
bin         home        lib64       mnt         root        tmp
boot        init        linuxrc     old-root    run         usr
dev         initrd.img  lost+found  opt         sbin        var
etc         lib         media       proc        sys         vmlinuz
$ exit

login as 'cirros' user. default password: 'gocubsgo'. use 'sudo' for root.
testvm login: controlplane $ 
controlplane $ ssh 10.224.1.12
^C
controlplane $ ./virtctl stop testvm
VM testvm was scheduled to stop
controlplane $ kubectl get vms
NAME     AGE     STATUS    READY
testvm   4m23s   Stopped   False
controlplane $ kubectl get vmis
NAME     AGE    PHASE       IP            NODENAME   READY
testvm   4m4s   Succeeded   10.244.1.12   node01     False
controlplane $ kubectl delete vms testvm
virtualmachine.kubevirt.io &quot;testvm&quot; deleted
controlplane $ kubectl get vms
No resources found in default namespace.
controlplane $ kubectl get vmis
No resources found in default namespace.
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2022-05-02 10:13</p>
<p>The docker hello-world project is the most basic docker hello-world, including: (compared with Android development)</p>
<ul>
<li>Make a mirror image of a simple java project using redis middleware - just like developing an Android application and packaging it</li>
<li>Upload your own image to docker hub (public image repository) - Upload to Android Market</li>
<li>Anyone and any machine can pull the previously uploaded image from the public mirror warehouse and run the previous java project normally - any mobile phone can go to the Android market to download the mobile application</li>
</ul>
<h1 id="install-docker-under-centos7"><a class="header" href="#install-docker-under-centos7">Install docker under centos7</a></h1>
<pre><code>yum install -y yum-utils
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce docker-ce-cli containerd.io
systemctl start docker &amp;&amp; systemctl enable docker &amp;&amp; systemctl status docker
</code></pre>
<h1 id="docker-image-acceleration"><a class="header" href="#docker-image-acceleration">docker image acceleration</a></h1>
<pre><code>mkdir -p /etc/docker
tee /etc/docker/daemon.json &lt;&lt;-'EOF'
{
&quot;registry-mirrors&quot;: [&quot;https://82m9ar63.mirror.aliyuncs.com&quot;],
&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
&quot;log-driver&quot;: &quot;json-file&quot;,
&quot;log-opts&quot;: {
&quot;max-size&quot;: &quot;100m&quot;
},
&quot;storage-driver&quot;: &quot;overlay2&quot;
}
EOF
systemctl daemon-reload
systemctl restart docker
</code></pre>
<h1 id="download-and-deploy-the-redis-image"><a class="header" href="#download-and-deploy-the-redis-image">Download and deploy the redis image</a></h1>
<pre><code>docker pull redis:latest
mkdir -p /data/redis/
tee /data/redis/redis.conf &lt;&lt;-'EOF'
appendonly yes
requirepass 666666
EOF
docker run -v /data/redis/redis.conf:/etc/redis/redis.conf \
-v /data/redis/data:/data \
-d --name myredis \
-p 6379:6379 \
redis:latest  redis-server /etc/redis/redis.conf
</code></pre>
<h1 id="java-docker-hello-project"><a class="header" href="#java-docker-hello-project">java docker-hello project</a></h1>
<p>pom.xml</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>
<p>application.properties</p>
<pre><code>spring.redis.host=192.168.159.132
spring.redis.password=666666
</code></pre>
<p>CounterController.java</p>
<pre><code>import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.redis.core.StringRedisTemplate;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class CounterController {

//    private int i = 0; //mysql，redis

    @Autowired
    StringRedisTemplate redisTemplate;

    @GetMapping(&quot;/hello&quot;)
    public String count(){

        Long increment = redisTemplate.opsForValue().increment(&quot;count-people&quot;);
        return &quot;有 【&quot;+ increment +&quot;】 人访问了这个页面&quot;;
    }
}
</code></pre>
<p><img src="cloud-native/2023-01-19-14-26-58.png" alt="" /></p>
<h1 id="make-a-mirror-for-the-docker-hello-project"><a class="header" href="#make-a-mirror-for-the-docker-hello-project">Make a mirror for the docker-hello project</a></h1>
<pre><code>[root@localhost tt]# cat Dockerfile 
FROM openjdk:8-jdk-slim
LABEL maintainer=leifengyang

COPY target/*.jar   /app.jar

ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;]

[root@localhost tt]# docker build -t docker-java-demo:v1.0 .
Sending build context to Docker daemon  27.58MB
Step 1/4 : FROM openjdk:8-jdk-slim
8-jdk-slim: Pulling from library/openjdk
1fe172e4850f: Already exists 
44d3aa8d0766: Pull complete 
81bea02f1eea: Pull complete 
ccd3c592d09e: Pull complete 
Digest: sha256:ace86b512a287ed6c65c4b0daffd895c1ad3d8489aa2ba5fedd9585a29ebb3de
Status: Downloaded newer image for openjdk:8-jdk-slim
---&gt; a07d7f1dd64b
Step 2/4 : LABEL maintainer=leifengyang
---&gt; Running in 0c95e08a46c1
Removing intermediate container 0c95e08a46c1
---&gt; 451513a02e4a
Step 3/4 : COPY target/*.jar   /app.jar
---&gt; bb79a201820b
Step 4/4 : ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;]
---&gt; Running in 7d08772be1eb
Removing intermediate container 7d08772be1eb
---&gt; 903bba8f3d19
Successfully built 903bba8f3d19
Successfully tagged docker-java-demo:v1.0
[root@localhost tt]# 
</code></pre>
<h1 id="upload-the-docker-hello-project-to-the-public-mirror-repository"><a class="header" href="#upload-the-docker-hello-project-to-the-public-mirror-repository">Upload the docker-hello project to the public mirror repository</a></h1>
<pre><code>[root@localhost tt]# docker images
REPOSITORY         TAG          IMAGE ID       CREATED         SIZE
docker-java-demo   v1.0         903bba8f3d19   2 minutes ago   323MB
redis              latest       a10f849e1540   4 days ago      117MB
openjdk            8-jdk-slim   a07d7f1dd64b   4 days ago      296MB
[root@localhost tt]# docker tag docker-java-demo:v1.0 backendcloud/docker-java-demo:v1.0
[root@localhost tt]# docker push backendcloud/docker-java-demo:v1.0
The push refers to repository [docker.io/backendcloud/docker-java-demo]
f30a1988cea2: Preparing 
e2118ceaebd1: Preparing 
8373e87e0617: Preparing 
13a34b6fff78: Preparing 
9c1b6dd6c1e6: Preparing 
denied: requested access to the resource is denied
[root@localhost tt]# docker login
Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.
Username: backendcloud
Password: 
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
[root@localhost tt]# docker push backendcloud/docker-java-demo:v1.0
The push refers to repository [docker.io/backendcloud/docker-java-demo]
f30a1988cea2: Pushed 
e2118ceaebd1: Mounted from library/openjdk 
8373e87e0617: Mounted from library/openjdk 
13a34b6fff78: Mounted from library/openjdk 
9c1b6dd6c1e6: Mounted from library/openjdk 
v1.0: digest: sha256:826e2d6fb61c9a93e06e4816a511051aa18504eb2f6b098c8fa4b3fc0c8ed44c size: 1372
[root@localhost tt]# docker images
REPOSITORY                      TAG          IMAGE ID       CREATED          SIZE
backendcloud/docker-java-demo   v1.0         903bba8f3d19   13 minutes ago   323MB
docker-java-demo                v1.0         903bba8f3d19   13 minutes ago   323MB
redis                           latest       a10f849e1540   4 days ago       117MB
openjdk                         8-jdk-slim   a07d7f1dd64b   4 days ago       296MB
[root@localhost tt]# docker run -d -p 8080:8080 backendcloud/docker-java-demo:v1.0 
311a62f586934059e4a18ed19460047efbe167979b316a0a9c17476e457e1c26
[root@localhost tt]# docker ps
CONTAINER ID   IMAGE                                COMMAND                  CREATED          STATUS          PORTS                                       NAMES
311a62f58693   backendcloud/docker-java-demo:v1.0   &quot;java -jar /app.jar&quot;     38 seconds ago   Up 37 seconds   0.0.0.0:8080-&gt;8080/tcp, :::8080-&gt;8080/tcp   brave_einstein
2883786f38ae   redis:latest                         &quot;docker-entrypoint.s…&quot;   42 minutes ago   Up 42 minutes   0.0.0.0:6379-&gt;6379/tcp, :::6379-&gt;6379/tcp   myredis
[root@localhost tt]# docker exec -it 311a62f586934059e4a18ed19460047efbe167979b316a0a9c17476e457e1c26 /bin/bash
root@311a62f58693:/# 
</code></pre>
<p><img src="cloud-native/2023-01-19-14-27-39.png" alt="" /></p>
<p><img src="cloud-native/2023-01-19-14-27-50.png" alt="" /></p>
<pre><code>[root@localhost tt]# docker logs 311a62f58693

.   ____          _            __ _ _
/\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
\\/  ___)| |_)| | | | | || (_| |  ) ) ) )
'  |____| .__|_| |_|_| |_\__, | / / / /
=========|_|==============|___/=/_/_/_/
:: Spring Boot ::                (v2.6.7)

2022-05-02 09:27:58.936  INFO 1 --- [           main] c.e.d.DockerJavaDemoApplication          : Starting DockerJavaDemoApplication v0.0.1-SNAPSHOT using Java 1.8.0_332 on 311a62f58693 with PID 1 (/app.jar started by root in /)
2022-05-02 09:27:58.942  INFO 1 --- [           main] c.e.d.DockerJavaDemoApplication          : No active profile set, falling back to 1 default profile: &quot;default&quot;
2022-05-02 09:27:59.925  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Multiple Spring Data modules found, entering strict repository configuration mode!
2022-05-02 09:27:59.931  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data Redis repositories in DEFAULT mode.
2022-05-02 09:27:59.958  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 3 ms. Found 0 Redis repository interfaces.
2022-05-02 09:28:00.506  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2022-05-02 09:28:00.520  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2022-05-02 09:28:00.520  INFO 1 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.62]
2022-05-02 09:28:00.596  INFO 1 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2022-05-02 09:28:00.596  INFO 1 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 1522 ms
2022-05-02 09:28:01.598  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''
2022-05-02 09:28:01.613  INFO 1 --- [           main] c.e.d.DockerJavaDemoApplication          : Started DockerJavaDemoApplication in 3.26 seconds (JVM running for 3.753)
2022-05-02 09:28:54.065  INFO 1 --- [nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2022-05-02 09:28:54.065  INFO 1 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2022-05-02 09:28:54.066  INFO 1 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 1 ms
[root@localhost tt]# 
</code></pre>
<blockquote>
<p>The source code of the docker-java-demo project is located at https://github.com/backendcloud/example/tree/master/docker-java-demo</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2022-04-24 11:00</p>
<p>The Pod of the Kubernetes cluster uses Openstack Cinder as the backend storage, and cinder-csi-plugin needs to be deployed</p>
<blockquote>
<p>https://github.com/kubernetes/cloud-provider-openstack</p>
</blockquote>
<h1 id="unable-to-connect-to-keystone-add-custom-domain-name-resolution-for-some-pods"><a class="header" href="#unable-to-connect-to-keystone-add-custom-domain-name-resolution-for-some-pods">Unable to connect to Keystone: Add custom domain name resolution for some Pods</a></h1>
<p>Keystone url uses a domain name, you need to add domain name resolution in /etc/hosts</p>
<p>The following is just an example to illustrate how to add custom domain name resolution for Pod</p>
<p><img src="cloud-native/2023-01-19-14-21-55.png" alt="" /></p>
<p>If some Pods depend on specific domain name resolution, you can use the hostAliases provided by K8S to add hosts for some workloads if you do not want to configure dns resolution:</p>
<pre><code>spec:
hostAliases:
- hostnames: [ &quot;harbor.example.com&quot; ]
    ip: &quot;10.10.10.10&quot;
</code></pre>
<p>After adding, you can see that hosts has been added to /etc/hosts in the container:</p>
<pre><code>$ cat /etc/hosts
...
# Entries added by HostAliases.
10.10.10.10    harboar.example.com
</code></pre>
<h1 id="the-metadata-service-is-not-up"><a class="header" href="#the-metadata-service-is-not-up">The metadata service is not up</a></h1>
<pre><code>[developer@localhost ~]$ kubectl logs csi-cinder-nodeplugin-t8hcx -nkube-system -c cinder-csi-plugin
I0424 11:10:56.531843       1 driver.go:73] Driver: cinder.csi.openstack.org
I0424 11:10:56.531889       1 driver.go:74] Driver version: 1.3.2@latest
I0424 11:10:56.531892       1 driver.go:75] CSI Spec version: 1.3.0
I0424 11:10:56.531898       1 driver.go:104] Enabling controller service capability: LIST_VOLUMES
I0424 11:10:56.531900       1 driver.go:104] Enabling controller service capability: CREATE_DELETE_VOLUME
I0424 11:10:56.531907       1 driver.go:104] Enabling controller service capability: PUBLISH_UNPUBLISH_VOLUME
I0424 11:10:56.531909       1 driver.go:104] Enabling controller service capability: CREATE_DELETE_SNAPSHOT
I0424 11:10:56.531911       1 driver.go:104] Enabling controller service capability: LIST_SNAPSHOTS
I0424 11:10:56.531913       1 driver.go:104] Enabling controller service capability: EXPAND_VOLUME
I0424 11:10:56.531915       1 driver.go:104] Enabling controller service capability: CLONE_VOLUME
I0424 11:10:56.531917       1 driver.go:104] Enabling controller service capability: LIST_VOLUMES_PUBLISHED_NODES
I0424 11:10:56.531919       1 driver.go:116] Enabling volume access mode: SINGLE_NODE_WRITER
I0424 11:10:56.531920       1 driver.go:126] Enabling node service capability: STAGE_UNSTAGE_VOLUME
I0424 11:10:56.531923       1 driver.go:126] Enabling node service capability: EXPAND_VOLUME
I0424 11:10:56.531928       1 driver.go:126] Enabling node service capability: GET_VOLUME_STATS
I0424 11:10:56.532069       1 openstack.go:90] Block storage opts: {0 false false}
I0424 11:10:57.181982       1 server.go:108] Listening for connections on address: &amp;net.UnixAddr{Name:&quot;/csi/csi.sock&quot;, Net:&quot;unix&quot;}
E0424 11:10:58.771149       1 utils.go:85] GRPC error: rpc error: code = Internal desc = [NodeGetInfo] unable to retrieve instance id of node error fetching http://169.254.169.254/openstack/latest/meta_data.json: Get &quot;http://169.254.169.254/openstack/latest/meta_data.json&quot;: dial tcp 169.254.169.254:80: connect: connection refused
E0424 11:11:00.768432       1 utils.go:85] GRPC error: rpc error: code = Internal desc = [NodeGetInfo] unable to retrieve instance id of node error fetching http://169.254.169.254/openstack/latest/meta_data.json: Get &quot;http://169.254.169.254/openstack/latest/meta_data.json&quot;: dial tcp 169.254.169.254:80: connect: connection refused
E0424 11:11:13.787632       1 utils.go:85] GRPC error: rpc error: code = Internal desc = [NodeGetInfo] unable to retrieve instance id of node error fetching http://169.254.169.254/openstack/latest/meta_data.json: Get &quot;http://169.254.169.254/openstack/latest/meta_data.json&quot;: dial tcp 169.254.169.254:80: connect: connection refused
</code></pre>
<blockquote>
<p>https://github.com/kubernetes/cloud-provider-openstack/issues/1127</p>
</blockquote>
<h1 id="after-devstack-restarts-cinder-reports-an-error-when-creating-a-volume"><a class="header" href="#after-devstack-restarts-cinder-reports-an-error-when-creating-a-volume">After devstack restarts, cinder reports an error when creating a volume</a></h1>
<blockquote>
<p>https://www.codetd.com/en/article/13087314</p>
</blockquote>
<h1 id="the-cinder-csi-plugin-node-service-is-not-up"><a class="header" href="#the-cinder-csi-plugin-node-service-is-not-up">The cinder-csi-plugin node service is not up</a></h1>
<pre><code>Normal   Provisioning          3m32s (x9 over 7m47s)  cinder.csi.openstack.org_csi-cinder-controllerplugin-667d467bf6-qfsnn_90ee191a-788a-4cda-82f2-eb61f37b20f5  External provisioner is provisioning volume for claim &quot;default/csi-pvc-cinderplugin&quot;
Warning  ProvisioningFailed    3m32s (x9 over 7m47s)  cinder.csi.openstack.org_csi-cinder-controllerplugin-667d467bf6-qfsnn_90ee191a-788a-4cda-82f2-eb61f37b20f5  failed to provision volume with StorageClass &quot;csi-sc-cinderplugin&quot;: error generating accessibility requirements: no available topology found
Normal   ExternalProvisioning  97s (x26 over 7m47s)   persistentvolume-controller        waiting for a volume to be created, either by external provisioner &quot;cinder.csi.openstack.org&quot; or manually created by system administrator
</code></pre>
<blockquote>
<p>https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/848
https://github.com/hetznercloud/csi-driver/issues/92</p>
</blockquote>
<h1 id="minikube---http-server-gave-http-response-to-https-client"><a class="header" href="#minikube---http-server-gave-http-response-to-https-client">minikube - http: server gave HTTP response to HTTPS client</a></h1>
<blockquote>
<p>https://minikube.sigs.k8s.io/docs/handbook/registry/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2022-04-13 20:17</p>
<blockquote>
<p>When managing multi-module management, some modules may still be under development and have not been published on github. Before Go 1.18, it was done through the replace of go mod. The go1.18 officially released in February 2022 provides another more convenient solution for multi-module management due to the new workspace feature.</p>
</blockquote>
<h1 id="past-replace-pattern"><a class="header" href="#past-replace-pattern">Past replace pattern</a></h1>
<pre><code class="language-bash">hanwei@hanweideMacBook-Air golang]$ go version
go version go1.18 darwin/arm64
hanwei@hanweideMacBook-Air golang]$ mkdir go1.18-workspace/
hanwei@hanweideMacBook-Air golang]$ cd go1.18-workspace/
hanwei@hanweideMacBook-Air go1.18-workspace]$ mkdir mypkg example
hanwei@hanweideMacBook-Air go1.18-workspace]$ tree
.
├── example
└── mypkg

2 directories, 0 files
hanwei@hanweideMacBook-Air go1.18-workspace]$ go mod init github.com/go1.18-workspace/mypkg
go: creating new go.mod: module github.com/go1.18-workspace/mypkg
go: to add module requirements and sums:
        go mod tidy
hanwei@hanweideMacBook-Air go1.18-workspace]$ cd mypkg/
hanwei@hanweideMacBook-Air mypkg]$ touch bar.go
hanwei@hanweideMacBook-Air mypkg]$ vi bar.go 
hanwei@hanweideMacBook-Air mypkg]$ cat bar.go 
package mypkg

func Bar() {
        println(&quot;This is package mypkg&quot;)
}
hanwei@hanweideMacBook-Air mypkg]$ cd ../example/
hanwei@hanweideMacBook-Air example]$ go mod init github.com/go1.18-workspace/example
go: creating new go.mod: module github.com/go1.18-workspace/example
go: to add module requirements and sums:
        go mod tidy
hanwei@hanweideMacBook-Air example]$ touch main.go
hanwei@hanweideMacBook-Air example]$ vi main.go 
hanwei@hanweideMacBook-Air example]$ cat main.go 
package main

import (
    &quot;github.com/go1.18-workspace/mypkg&quot;
)

func main() {
    mypkg.Bar()
}
</code></pre>
<p>At this time, if we run go mod tidy, an error will be reported, because our mypkg package has not been submitted to github at all, and it must not be found.</p>
<pre><code>hanwei@hanweideMacBook-Air example]$ go mod tidy
go: finding module for package github.com/go1.18-workspace/mypkg
github.com/go1.18-workspace/example imports
        github.com/go1.18-workspace/mypkg: cannot find module providing package github.com/go1.18-workspace/mypkg: module github.com/go1.18-workspace/mypkg: git ls-remote -q origin in /Users/hanwei/GoProjects/pkg/mod/cache/vcs/2c423cac5ebc1b2d018ef93a87560d369abd7dec6c155b46cddb11299415bc09: exit status 128:
        remote: Repository not found.
        fatal: repository 'https://github.com/go1.18-workspace/mypkg/' not found
hanwei@hanweideMacBook-Air example]$ go run main.go
main.go:4:5: no required module provides package github.com/go1.18-workspace/mypkg; to add it:
        go get github.com/go1.18-workspace/mypkg
</code></pre>
<p>go run main.go is also unsuccessful.</p>
<p>Of course we can submit mypkg to github, but every time we modify mypkg, we need to submit, otherwise the latest version cannot be used in the example.</p>
<p>In view of this situation, it is currently recommended to solve it by replace, that is, add the following replace to the go.mod in the example:</p>
<pre><code>hanwei@hanweideMacBook-Air example]$ go mod edit -replace=github.com/go1.18-workspace/mypkg=../mypkg
hanwei@hanweideMacBook-Air example]$ cat go.mod 
module github.com/go1.18-workspace/example

go 1.18

replace github.com/go1.18-workspace/mypkg =&gt; ../mypkg
</code></pre>
<p>(v1.0.0 is modified according to the specific situation, it has not been submitted yet, you can use v1.0.0)</p>
<pre><code>hanwei@hanweideMacBook-Air example]$ cat go.mod 
module github.com/go1.18-workspace/example

go 1.18

require github.com/go1.18-workspace/mypkg v1.0.0
replace github.com/go1.18-workspace/mypkg =&gt; ../mypkg
hanwei@hanweideMacBook-Air example]$ tree ..
..
├── example
│   ├── go.mod
│   └── main.go
└── mypkg
    ├── bar.go
    └── go.mod

2 directories, 4 files
</code></pre>
<p>Run go run main.go again, the output is as follows:</p>
<pre><code>hanwei@hanweideMacBook-Air example]$ go run main.go 
This is package mypkg
</code></pre>
<h1 id="workspace-mode"><a class="header" href="#workspace-mode">workspace mode</a></h1>
<p>Comment the replace above, and then execute go run main.go, and report an error</p>
<pre><code>hanwei@hanweideMacBook-Air example]$ cat go.mod 
module github.com/go1.18-workspace/example

go 1.18

//require github.com/go1.18-workspace/mypkg v1.0.0
//replace github.com/go1.18-workspace/mypkg =&gt; ../mypkg
hanwei@hanweideMacBook-Air example]$ go run main.go 
main.go:4:5: missing go.sum entry for module providing package github.com/go1.18-workspace/mypkg; to add:
        go mod download github.com/go1.18-workspace/mypkg
</code></pre>
<p>Initialize workspace</p>
<pre><code>hanwei@hanweideMacBook-Air go1.18-workspace]$ go version
go version go1.18 darwin/arm64
hanwei@hanweideMacBook-Air go1.18-workspace]$ go work init example mypkg
hanwei@hanweideMacBook-Air go1.18-workspace]$ tree
.
├── example
│   ├── go.mod
│   └── main.go
├── go.work
└── mypkg
    ├── bar.go
    └── go.mod

2 directories, 5 files
hanwei@hanweideMacBook-Air go1.18-workspace]$ cat go.work 
go 1.18

use (
        ./example
        ./mypkg
)
hanwei@hanweideMacBook-Air example]$ go run main.go 
This is package mypkg
</code></pre>
<p>It can be seen that using the workspace is much more convenient than replace.</p>
<blockquote>
<p>The syntax of the go.work file is similar to go.mod, so it also supports replace.</p>
</blockquote>
<blockquote>
<p>Note that go.work does not need to be committed to Git, as it is only used for local development.</p>
</blockquote>
<pre><code>cat .gitignore
# ---&gt; Go
# If you prefer the allow list template instead of the deny list, see community template:
# https://github.com/github/gitignore/blob/main/community/Golang/Go.AllowList.gitignore
#
# Binaries for programs and plugins
*.exe
*.exe~
*.dll
*.so
*.dylib
# Test binary, built with `go test -c`
*.test
# Output of the go coverage tool, specifically when used with LiteIDE
*.out
# Dependency directories (remove the comment below to include it)
# vendor/
# Go workspace file
go.work
</code></pre>
<blockquote>
<p>In the GOPATH era, multiple GOPATHs are a headache. At that time, there was no good solution, and the Module appeared, so the multi-GOPATH problem disappeared. But the multi-Module problem then appeared. The Workspace solution better solves this problem.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2022-03-25 11:24</p>
<p>2 asynchronous examples</p>
<ul>
<li>void has no return value</li>
<li>has a return value</li>
</ul>
<p>Spring boot comes with @Async annotation, just add it to the method you want to be asynchronous. There is a small pit, that is, it is only a synchronous service, and the @EnableAsync annotation needs to be added to the main method.</p>
<pre><code class="language-java">@SpringBootApplication
@EnableAsync
public class DemoApplication {
    public static void main(String[] args) {
        SpringApplication.run(DemoApplication.class, args);
    }
}
</code></pre>
<h1 id="async-with-no-return-value"><a class="header" href="#async-with-no-return-value">Async with no return value</a></h1>
<p>service</p>
<pre><code class="language-java">@Service
public class AsyncNoReturnImpl implements AsyncNoReturn {

    @Async
    @Override
    public void execAsync1(){
        try{
            Thread.sleep(2000);
            System.out.println(&quot;甲睡了2000ms&quot;);
        }catch (Exception e){
            e.printStackTrace();
        }
    }
    @Async
    @Override
    public void execAsync2(){
        try{
            Thread.sleep(4000);
            System.out.println(&quot;乙睡了4000ms&quot;);
        }catch (Exception e){
            e.printStackTrace();
        }
    }
    @Async
    @Override
    public void execAsync3(){
        try{
            Thread.sleep(3000);
            System.out.println(&quot;丙睡了3000ms&quot;);
        }catch (Exception e){
            e.printStackTrace();
        }
    }
}
</code></pre>
<p>controller</p>
<pre><code class="language-java">@GetMapping(&quot;/async-no-return&quot;)
    public ResponseEntity&lt;String&gt; getAsyncNoReturn() {
        long s = System.currentTimeMillis();
        asyncNoReturn.execAsync1();
        asyncNoReturn.execAsync2();
        asyncNoReturn.execAsync3();
        System.out.println(&quot;我执行结束了&quot;);
        long costTime = System.currentTimeMillis() - s;
        System.out.println(&quot;service async-no-return cost time: &quot; + costTime);
        return ResponseEntity.status(200).body(&quot;good&quot;);
    }
</code></pre>
<p>test</p>
<pre><code class="language-bash">## async-no-return
GET http://localhost:8080/async-no-return

我执行结束了
service async-no-return cost time: 3
甲睡了2000ms
丙睡了3000ms
乙睡了4000ms
</code></pre>
<p>Because the three services are all asynchronous, the interface returns directly, and it takes almost no time, 3ms. The three services are executed in parallel, and all of them are executed after 4 seconds.</p>
<h1 id="async-with-return-value"><a class="header" href="#async-with-return-value">async with return value</a></h1>
<p>service</p>
<pre><code class="language-java">@Service
public class AsyncHasReturnImpl implements AsyncHasReturn {

    @Async
    public Future&lt;String&gt; execAsync1(){
        try{
            Thread.sleep(2000);
            System.out.println(&quot;甲睡了2000ms&quot;);
            return new AsyncResult&lt;String&gt;(&quot;甲执行成功了&quot;);
        }catch (Exception e){
            e.printStackTrace();
        }
        return null;
    }
    @Async
    public Future&lt;String&gt; execAsync2(){
        try{
            Thread.sleep(4000);
            System.out.println(&quot;乙睡了4000ms&quot;);
            return new AsyncResult&lt;String&gt;(&quot;乙执行成功了&quot;);
        }catch (Exception e){
            e.printStackTrace();
        }
        return null;
    }
    @Async
    public Future&lt;String&gt; execAsync3(){
        try{
            Thread.sleep(3000);
            System.out.println(&quot;丙睡了3000ms&quot;);
            return new  AsyncResult&lt;String&gt;(&quot;丙执行结束了&quot;);
        }catch (Exception e){
            e.printStackTrace();
        }
        return null;
    }

}
</code></pre>
<p>controller</p>
<pre><code class="language-java">@GetMapping(&quot;/async-has-return&quot;)
    public ResponseEntity&lt;String&gt; getAsyncHasReturn() throws ExecutionException, InterruptedException {
        long s = System.currentTimeMillis();
        Future&lt;String&gt; stringFuture1 = asyncHasReturn.execAsync1();
        Future&lt;String&gt; stringFuture2 = asyncHasReturn.execAsync2();
        Future&lt;String&gt; stringFuture3 = asyncHasReturn.execAsync3();
        long costTime = System.currentTimeMillis() - s;
        System.out.println(&quot;执行到代码中间了，cost time: &quot; + costTime);
        System.out.println(&quot;我执行结束了&quot; + stringFuture1.get() + stringFuture2.get() + stringFuture3.get());
        costTime = System.currentTimeMillis() - s;
        System.out.println(&quot;service async-has-return cost time: &quot; + costTime);
        return ResponseEntity.status(200).body(&quot;good&quot;);
    }
</code></pre>
<p>test</p>
<pre><code class="language-bash">## async-has-return
GET http://localhost:8080/async-has-return

执行到代码中间了，cost time: 1
甲睡了2000ms
丙睡了3000ms
乙睡了4000ms
我执行结束了甲执行成功了乙执行成功了丙执行结束了
service async-has-return cost time: 4006
</code></pre>
<p>It can be seen that it does not take time to assign an asynchronous service to a Future, and there is no need to wait for the current asynchronous thread to finish executing. Only the get() method needs to wait for the current asynchronous thread to finish executing, and several asynchronous threads are executed in parallel. Similar to the get() method is isDone(), because get() already includes isDone(), so there is no need to use isDone() to make a judgment.</p>
<blockquote>
<p>All the code is at: https://github.com/backendcloud/example/tree/master/spring-boot/async/demo/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2021-12-01 17:48</p>
<blockquote>
<p>Read https://kubernetes.io/zh/docs/concepts/cluster-administration/logging/ before reading this article</p>
</blockquote>
<p>I wrote an article about Fluentd designing a set of log architecture to implement Kubernetes Pod log collection . Since EFK is no longer the current mainstream log architecture, I studied another more mainstream log system promtail + loki + Grafana.</p>
<h1 id="why-introduce-a-logging-system"><a class="header" href="#why-introduce-a-logging-system">Why introduce a logging system</a></h1>
<p>The Kubernetes Pod itself will generate logs, and you can view the corresponding logs. Why is the log system introduced?</p>
<p>For the sake of log reliability, because the application suddenly hangs up, we cannot find relevant logs at this time, so we need to introduce a log system to collect logs uniformly.</p>
<p>Second, after the scale is large, acquisition, centralized management, storage, indexing, query, and display are all problems, and a system solution is needed.</p>
<h1 id="promtail--loki--grafana-log-architecture"><a class="header" href="#promtail--loki--grafana-log-architecture">promtail + loki + Grafana log architecture</a></h1>
<p><img src="cloud-native/2023-01-19-12-46-03.png" alt="" /></p>
<p>The PLG log system consists of the following three parts:</p>
<ul>
<li>Promtail is a custom client for Loki, responsible for collecting logs and sending them to Loki.</li>
<li>Loki is the master server responsible for storing logs and handling queries.</li>
<li>Grafana is used for UI display (you can develop the front-end page yourself instead).</li>
</ul>
<p>The PLG log structure is customized for Kubernetes Pod logs, which is the only choice for Kubernetes Pod logs. The advantages over ELK/EFK are obvious.</p>
<p>In addition, because the alarm and monitoring of Kubernetes are based on Prometheus, if you use ELK, you need to switch between Kibana and Grafana, which affects the user experience. Therefore, the first purpose of loki is to minimize the switching cost of metrics and logs, which helps to reduce the response time of abnormal events and improve user experience.</p>
<h1 id="loki-architecture"><a class="header" href="#loki-architecture">Loki Architecture</a></h1>
<p><img src="cloud-native/2023-01-19-12-46-49.png" alt="" /></p>
<ul>
<li>The Promtail open source client is responsible for collecting and reporting logs;</li>
<li>Distributor: the log is written to the entry, and the data is forwarded to Ingester;</li>
<li>Ingester: log writing service, cache and write log content and index to the underlying storage;</li>
<li>Querier: log reading service, execute search request.</li>
</ul>
<h2 id="read-and-write"><a class="header" href="#read-and-write">read and write</a></h2>
<p>The writing of log data mainly relies on two components, Distributor and Ingester. The overall process is as follows:</p>
<p><img src="cloud-native/2023-01-19-12-47-31.png" alt="" /></p>
<h1 id="distributor"><a class="header" href="#distributor">Distributor</a></h1>
<p>Once promtail collects logs and sends them to loki, Distributor is the first component to receive logs. Since log write volumes can be large, they cannot be written to the database as they come in. This will destroy the database. We need to batch and compress data.</p>
<p>Loki achieves this by building compressed data blocks by gziping the logs when they enter. The component ingester is a stateful component responsible for building and refreshing chunks. When the chunks reach a certain amount or time, they are refreshed to storage to go. Each flow log corresponds to an ingester. When the log reaches the Distributor, it calculates which ingester it should go to based on the metadata and the hash algorithm.</p>
<p><img src="cloud-native/2023-01-19-12-47-51.png" alt="" /></p>
<p>Also, we replicate it n (3 by default) times for redundancy and resilience.</p>
<h2 id="ingester"><a class="header" href="#ingester">Ingester</a></h2>
<p>Ingester receives the logs and starts building chunks:</p>
<p><img src="cloud-native/2023-01-19-12-48-07.png" alt="" /></p>
<p>Basically, the log is compressed and appended to the chunk. Once the chunk is &quot;filled&quot; (the data reaches a certain amount or a certain period has passed), the ingester flushes it to the database. We use separate databases for blocks and indexes because they store different types of data.</p>
<p><img src="cloud-native/2023-01-19-12-48-24.png" alt="" /></p>
<p>After flushing a chunk, the ingester then creates a new empty chunk and adds new entries to that chunk.</p>
<h1 id="querier"><a class="header" href="#querier">Querier</a></h1>
<p>Reading is very simple. Given a time range and label selector, Querier looks at the index to determine which blocks match, and displays the results through greps. It also fetches the latest data from Ingester which has not been refreshed yet.</p>
<p>For each query, a queryer will show you all related logs. Query parallelization is implemented, providing distributed grep so that even large queries are adequate.</p>
<p><img src="cloud-native/2023-01-19-12-48-45.png" alt="" /></p>
<blockquote>
<p>You can refer to the official document Loki's Architecture for further in-depth understanding. Finally, in one sentence, Loki is like Prometheus, but for logs.</p>
</blockquote>
<h1 id="plg-deployment"><a class="header" href="#plg-deployment">PLG deployment</a></h1>
<pre><code>#  helm upgrade --install loki --namespace=loki grafana/loki-stack  --set grafana.enabled=true
Release &quot;loki&quot; does not exist. Installing it now.
Error: failed to download &quot;grafana/loki-stack”
</code></pre>
<p>If you are in China and it is difficult to pull images from the official DockerHub, you can use a mirror repository hosted on Alibaba Cloud:</p>
<pre><code>#  helm upgrade --install loki --namespace=loki grafana/loki-stack  --set grafana.enabled=true --set  manager.image.repository=openkruise-registry.cn-hangzhou.cr.aliyuncs.com/openkruise/kruise-manager

Release &quot;loki&quot; does not exist. Installing it now.
NAME: loki
LAST DEPLOYED: Wed Dec  1 15:34:23 2021
NAMESPACE: loki
STATUS: deployed
REVISION: 1
NOTES:
The Loki stack has been deployed to your cluster. Loki can now be added as a datasource in Grafana.

See http://docs.grafana.org/features/datasources/loki/ for more detail.
</code></pre>
<p>#Query PLG Service Status</p>
<pre><code># kubectl -n loki get all
NAME                                READY   STATUS    RESTARTS   AGE
pod/loki-0                          1/1     Running   0          27m
pod/loki-grafana-688db6c776-s8lxc   1/1     Running   0          27m
pod/loki-promtail-2g6dp             1/1     Running   0          27m
pod/loki-promtail-2th87             1/1     Running   0          27m
pod/loki-promtail-46rbc             1/1     Running   0          27m
pod/loki-promtail-8zljs             1/1     Running   0          27m
pod/loki-promtail-gn8hf             1/1     Running   0          27m
pod/loki-promtail-mc44v             1/1     Running   0          27m

NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/loki            ClusterIP   10.233.5.180    &lt;none&gt;        3100/TCP   27m
service/loki-grafana    ClusterIP   10.233.13.165   &lt;none&gt;        80/TCP     27m
service/loki-headless   ClusterIP   None            &lt;none&gt;        3100/TCP   27m

NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/loki-promtail   6         6         6       6            6           &lt;none&gt;          27m

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/loki-grafana   1/1     1            1           27m

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/loki-grafana-688db6c776   1         1         1       27m

NAME                    READY   AGE
statefulset.apps/loki   1/1     27m
</code></pre>
<p>It can be seen that the installation of the Promtail + Loki + Grafana combination is automatically completed after deployment through Helm. The Promtail deployment mode is daemonset, which is deployed on each computing node to collect log information on nodes and Pods.</p>
<p>Loki itself is deployed by statefulset by default. This is to avoid index loss when the data ingestion component crashes. Therefore, it is officially recommended to run Loki through statefulset and use persistent storage to store index files.</p>
<p>Next, visit the Grafana UI interface to view the deployment results. First, get the Grafana administrator's password with the following command:</p>
<pre><code>$ kubectl get secret --namespace loki loki-grafana -o jsonpath=&quot;{.data.admin-password}&quot; | base64 --decode ; echo
</code></pre>
<p>Then forward Grafana's interface through the following command for access through the Web UI. By default, the address of port forwarding is localhost, you can add –address <IP address> according to the situation of the instance where kubectl is located.</p>
<pre><code>$ kubectl port-forward --namespace loki service/loki-grafana 3000:80
</code></pre>
<h1 id="login-display-page"><a class="header" href="#login-display-page">Login display page</a></h1>
<p>Since PLG is deployed in Kubernetes, the service of Kubernetes is an internal ip, and a port mapping is made so that the external network can access the Grafana UI.</p>
<pre><code># nohup ssh -N -g -L 33043:10.233.13.165:80 36.133.53.67 -p22 2&gt;&amp;1 &amp;
</code></pre>
<p><img src="cloud-native/2023-01-19-12-50-37.png" alt="" /></p>
<p><img src="cloud-native/2023-01-19-12-50-43.png" alt="" /></p>
<p><img src="cloud-native/2023-01-19-12-50-51.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2021-11-30 16:36</p>
<blockquote>
<p>Read https://kubernetes.io/zh/docs/concepts/cluster-administration/logging/ before reading this article</p>
</blockquote>
<h1 id="fluentd-logging-architecture"><a class="header" href="#fluentd-logging-architecture">Fluentd logging architecture</a></h1>
<p><img src="cloud-native/2023-01-19-12-38-33.png" alt="" /></p>
<p>A typical deployment architecture of Fluentd needs to include two different roles: forwarder and aggregator.</p>
<p>Each Kubernetes working node deploys a Fluentd to forward the node's container logs to the edge cloud configuration public network working node, configure the public network working node and then forward the log to the software deployment node.</p>
<h1 id="fluent-configuration-file"><a class="header" href="#fluent-configuration-file">Fluent configuration file</a></h1>
<p>The configuration of Fluentd for each Kubernetes worker node is as follows:</p>
<pre><code>&lt;source&gt;
@type tail
path /var/log/containers/*.log
pos_file /var/log/fluentd-containers.log.pos
format json
tag kubernetes.*
time_format %Y-%m-%dT%H:%M:%S.%NZ
&lt;/source&gt;

&lt;match kubernetes.**&gt;
@type forward
send_timeout 60s
recover_wait 10s
hard_timeout 60s
&lt;server&gt;
    name myserver1
    host 192.168.200.100
    port 24224
    weight 60
&lt;/server&gt;
&lt;/match&gt;
</code></pre>
<p>The aggregation log node, that is, the working node Fluentd configured with the public network in the above figure, is configured as follows:</p>
<pre><code>&lt;source&gt;
@type forward
port 24224
bind 0.0.0.0
&lt;/source&gt;

&lt;match **&gt;
@type forward
send_timeout 60s
recover_wait 10s
hard_timeout 60s
&lt;server&gt;
    name myserver2
    host 36.134.56.149
    port 24224
    weight 60
&lt;/server&gt;
&lt;/match&gt;
</code></pre>
<p>The configuration of the software deployment node Fluentd is as follows:</p>
<pre><code>&lt;source&gt;
@type forward
port 24224
bind 0.0.0.0
&lt;/source&gt;

&lt;match **&gt;
@type stdout
&lt;/match&gt;
</code></pre>
<p>The log rollback configuration is as follows:</p>
<pre><code># cat /etc/logrotate.d/cmp-pod
/tmp/ftp_path/log_pod/*.log {
    missingok
    copytruncate
    rotate 30
    compress
    dateext
    dateformat -%Y%m%d-%H-%s
    sharedscripts
}
#  cat /etc/cron.d/cmp-pod 
# Run system activity accounting tool every 10 minutes
SHELL=/bin/bash
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
*/5 * * * * root logrotate -f /etc/logrotate.d/cmp-pod
</code></pre>
<h1 id="record-several-problems-encountered-in-the-process-of-collecting-pod-logs-with-fluentd"><a class="header" href="#record-several-problems-encountered-in-the-process-of-collecting-pod-logs-with-fluentd">Record several problems encountered in the process of collecting Pod logs with Fluentd</a></h1>
<h2 id="compared-with-bare-metal-and-container-deployment-adopt-container-deployment-solution"><a class="header" href="#compared-with-bare-metal-and-container-deployment-adopt-container-deployment-solution">Compared with bare metal and container deployment, adopt container deployment solution</a></h2>
<p>Bare metal can also be deployed, but requires a ruby ​​environment, as well as Fluent dependencies, including version dependencies, deployment is a bit cumbersome, and it is not conducive to automation. If container deployment is adopted, the above disadvantages do not exist, and the kind: DaemonSet of Kubernetes can be used to conveniently start the Fluentd service on each Kubernetes node.</p>
<p>The container image uses docker pull fluent/fluentd:latest.
You can also use fluent/fluentd-kubernetes-daemonset, which should be better. Some configurations should be configured. I did my own research and used the original image.
You can also directly use the resource list of the addon plug-in officially provided by Kubernetes at the address: https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/ and install it directly.</p>
<p>The following command starts the container</p>
<pre><code>docker run -it -d   -p 24224:24224   -v /path/to/conf:/fluentd/etc   -v /var:/var fluent/fluentd:latest
</code></pre>
<p>The initial startup failed because there is no configuration file (the host's /path/to/conf directory overwrites the fluentd configuration file directory in the container), plus the Fluentd configuration file, the container restarted successfully.</p>
<p>In order to test the Fluent log service, two small experiments were done.</p>
<p><strong>input: tail</strong></p>
<p>After the Fluent configuration file is written as follows, restart the Fluentd container.</p>
<pre><code># Directive determines the input sources
# Watches source and triggers an event with a tag attached to it
&lt;source&gt;
@type tail                                               # Uses tail plugin to read logs from
format json                                              # Assumes that the log file is in &quot;json&quot; format
read_from_head true                                      # Start to read the logs from the head of file, not bottom
tag api.user.registration                                # Tag triggered event with &quot;api.user.registration&quot;
path /home/ubuntu/logs/application/registration.log*     # Paths to the files which will be tailed
pos_file /home/ubuntu/logs/fluentd/registration.log.pos  # Path to the &quot;position&quot; database file
&lt;/source&gt;

# Directive determines the output destinations
# Catches an event with a specific tag attached to it
&lt;match api.user.registration&gt;
@type file                                               # Uses file plugin to write logs to
path /home/ubuntu/logs/fluentd/registration.log          # Path to the log file which logs will be written to
&lt;/match&gt;
</code></pre>
<p>test was successful.</p>
<pre><code>ubuntu@linux:~$ echo '{&quot;user&quot;:&quot;1&quot;}' &gt;&gt; logs/application/registration.log.1 
ubuntu@linux:~$ echo '{&quot;user&quot;:&quot;2&quot;}' &gt;&gt; logs/application/registration.log.1 
ubuntu@linux:~$ echo '{&quot;user&quot;:&quot;3&quot;}' &gt;&gt; logs/application/registration.log.1 

ubuntu@linux:~$ ls -l logs/fluentd/

-rw-r--r-- 1 td-agent td-agent 61 Apr  6 21:02 registration.log.20180406.b56933893cd87b6b8
-rw-r--r-- 1 td-agent td-agent 83 Apr  6 21:02 registration.log.pos

ubuntu@linux:~$ cat logs/fluentd/registration.log.20180406.b56933893cd87b6b8

2018-04-06T21:02:30+01:00	api.user.registration	{&quot;user&quot;:&quot;1&quot;}
2018-04-06T21:02:49+01:00	api.user.registration	{&quot;user&quot;:&quot;2&quot;}
2018-04-06T21:02:55+01:00	api.user.registration	{&quot;user&quot;:&quot;3&quot;}

ubuntu@linux:~$ touch logs/application/registration.log.2

ubuntu@linux:~$ echo '{&quot;admin&quot;:&quot;1&quot;}' &gt;&gt; logs/application/registration.log.2
ubuntu@linux:~$ echo '{&quot;admin&quot;:&quot;2&quot;}' &gt;&gt; logs/application/registration.log.2
ubuntu@linux:~$ echo '{&quot;admin&quot;:&quot;3&quot;}' &gt;&gt; logs/application/registration.log.2

ubuntu@linux:~$ cat logs/fluentd/registration.log.20180406.b56933893cd87b6b8

2018-04-06T21:02:30+01:00	api.user.registration	{&quot;user&quot;:&quot;1&quot;}
2018-04-06T21:02:49+01:00	api.user.registration	{&quot;user&quot;:&quot;2&quot;}
2018-04-06T21:02:55+01:00	api.user.registration	{&quot;user&quot;:&quot;3&quot;}
2018-04-06T21:07:37+01:00	api.user.registration	{&quot;admin&quot;:&quot;1&quot;}
2018-04-06T21:07:37+01:00	api.user.registration	{&quot;admin&quot;:&quot;2&quot;}
2018-04-06T21:07:38+01:00	api.user.registration	{&quot;admin&quot;:&quot;3&quot;}
</code></pre>
<p><strong>input: forward</strong></p>
<p>After the Fluent configuration file is written as follows, restart the Fluentd container.</p>
<pre><code>&lt;source&gt;
@type   forward
&lt;/source&gt;

&lt;match *&gt;

@type              file

path               /fluentd/log/${tag}/${tag}
append             true
&lt;format&gt;
    @type            single_value
    message_key      log
&lt;/format&gt;
&lt;buffer tag,time&gt;
    @type             file
    timekey           1d
    timekey_wait      10m
    flush_mode        interval
    flush_interval    30s
&lt;/buffer&gt;
&lt;/match&gt;
</code></pre>
<p>Start a new container, specify the logging driver of the container</p>
<pre><code>docker run -d \
...
--log-driver=fluentd \
--log-opt fluentd-address=&lt;fluentdhost&gt;:24224 \
--log-opt mode=non-blocking \
--log-opt tag={{.Name}} \
&lt;image&gt;
</code></pre>
<p>Observe the log, and you can see a directory structure similar to this in the /home/ubuntu/container-logs directory:</p>
<pre><code>.
└── &lt;container-name&gt;
    └── &lt;container-name&gt;.20190123.log
</code></pre>
<h2 id="the-log-file-of-the-pod-can-be-cated-in-the-fluentd-container-but-the-log-report-of-the-fluentd-service-cannot-read-the-log-file-of-the-pod"><a class="header" href="#the-log-file-of-the-pod-can-be-cated-in-the-fluentd-container-but-the-log-report-of-the-fluentd-service-cannot-read-the-log-file-of-the-pod">The log file of the pod can be cated in the Fluentd container, but the log report of the Fluentd service cannot read the log file of the pod</a></h2>
<pre><code>/var/log/containers/samplelog-79bd66868b-t7xn9_logging1_fluentd-70e85c5d6328e7d.log unreadable. It is excluded and would be examined next time.
</code></pre>
<p>Log in to the Fluentd container to cat log files. After reading the read and write attributes of the log files, the root user can read them, but other users cannot read them. ps the Fluentd process and found that they are all running as the fluent user.</p>
<pre><code>/ # ps -ef
PID   USER     TIME  COMMAND
    1 root      0:00 {entrypoint.sh} /usr/bin/dumb-init /bin/sh /bin/entrypoint.sh /bin/sh -c exec fluentd -c /fluentd/etc/${FLUENTD_CONF} -p /fluentd/plugins $FLUENTD_
    6 fluent    0:02 {fluentd} /usr/bin/ruby /usr/bin/fluentd -c /fluentd/etc/fluent.conf -p /fluentd/plugins
17 fluent    0:28 /usr/bin/ruby -Eascii-8bit:ascii-8bit /usr/bin/fluentd -c /fluentd/etc/fluent.conf -p /fluentd/plugins --under-supervisor
21 root      0:00 sh
26 root      0:00 ps -ef
/ # 
</code></pre>
<p>Because the fluentd image is built with fluent user privileges, insufficient privileges may occur.</p>
<p>The solution is to pull the fluent code to build it yourself and specify the user in the Dockfile; or use the simplest method to specify the UID of the fluent user as 0 in the env. I have an answer to this question on stackoverflow for the specific operation method.</p>
<p>https://stackoverflow.com/questions/51671212/fluentd-log-unreadable-it-is-excluded-and-would-be-examined-next-time/70165516#70165516</p>
<p>The most direct way is to change mode:</p>
<pre><code>chmod 777 /var/log/containers/*.log
</code></pre>
<p>but the best way is: change fluent user to root (set FLUENT_UID environment variable to 0 in your docker/kubernetes configuration);</p>
<p>add –env FLUENT_UID=0 to docker command, for example:</p>
<pre><code>docker run -it -d -p 24224:24224 -v /path/to/conf:/fluentd/etc -v /var:/var --env FLUENT_UID=0 fluent/fluentd:latest
</code></pre>
<p>or add to Kubernetes yaml file:</p>
<pre><code class="language-yaml">apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
  # namespace: default
  labels:
    k8s-app: fluentd-logging
    version: v1
    kubernetes.io/cluster-service: &quot;true&quot;
spec:
  template:
    metadata:
      labels:
        k8s-app: fluentd-logging
        version: v1
        kubernetes.io/cluster-service: &quot;true&quot;
    spec:
      serviceAccount: fluentd
      serviceAccountName: fluentd
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1.4-debian-elasticsearch
        env:
          - name: FLUENT_UID  # change this place
            value: &quot;0&quot;
</code></pre>
<p>After modifying as per my answer on stackoverflow, everything works fine.</p>
<pre><code>/ # ps -ef
PID   USER     TIME  COMMAND
    1 root      0:00 {entrypoint.sh} /usr/bin/dumb-init /bin/sh /bin/entrypoint.sh /bin/sh -c exec fluentd -c /fluentd/etc/${FLUENTD_CONF} -p /fluentd/plugins $FLUENTD_
    7 root      0:05 {fluentd} /usr/bin/ruby /usr/bin/fluentd -c /fluentd/etc/fluent.conf -p /fluentd/plugins
16 root      1:58 /usr/bin/ruby -Eascii-8bit:ascii-8bit /usr/bin/fluentd -c /fluentd/etc/fluent.conf -p /fluentd/plugins --under-supervisor
22 root      0:00 sh
28 root      0:00 ps -ef
/ # 
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2021-11-23 15:04</p>
<p>Record some problems encountered in developing projects in Java:</p>
<h1 id="pattern-annotation"><a class="header" href="#pattern-annotation">@Pattern annotation</a></h1>
<p>For the fields in the http request body, regular checks need to be done. For general regular checks that do not require conditional judgments, you can use @Pattern instead of writing regular checks in the method, which simplifies development.</p>
<p>In order to verify, it must start with &quot;sftp://&quot;, written in the following form
@Pattern(regexp = &quot;^sftp://&quot;)</p>
<p>But the content of the field is an sftp://10.10.10.10:33022/ftp_testerror
, and the @Pattern annotation will automatically add ^ and &amp;. The correct spelling is
@Pattern(regexp = &quot;^sftp://.<em>$&quot;)or@Pattern(regexp = &quot;sftp://.</em>&quot;)</p>
<p>So the regular expression is improved and written in the following form:</p>
<pre><code class="language-java">/**
 * ftpPath
 */
@JsonProperty(&quot;ftpServer&quot;)
@Pattern(regexp = &quot;sftp://.+[:\\d+]?/.*&quot;, message = &quot;ftpPath&quot;)
private String ftpPath;
/**
 * ftpUser
 */
@JsonProperty(&quot;ftpUser&quot;)
private String ftpUser;
/**
 * ftpPasswd
 */
@JsonProperty(&quot;ftpPasswd&quot;)
private String ftpPasswd;
</code></pre>
<h1 id="to-enable-https-and-https-services-at-the-same-time"><a class="header" href="#to-enable-https-and-https-services-at-the-same-time">To enable https and https services at the same time</a></h1>
<p>Open the http service separately, and write the configuration file of springboot</p>
<pre><code>server.port: 33021
</code></pre>
<p>Open the https service separately, use keytool to make a certificate, and the configuration file of springboot is written as follows:</p>
<pre><code>#https port
server.port: 33021

#https config
server.ssl.key-store: classpath:keystore.p12
#server.ssl.key-store: file:/Users/hanwei/xxx/yyy/api/src/main/resources/keystore.p12
server.ssl.key-store-password: 123456
server.ssl.keyStoreType: PKCS12
server.ssl.keyAlias: tomcat
</code></pre>
<p>To enable http service and https service at the same time, the configuration file of springboot is written as follows:</p>
<pre><code>#https port
server.port: 33021

# http port
http.port: 33011

#https config
server.ssl.key-store: classpath:keystore.p12
#server.ssl.key-store: file:/Users/hanwei/xxx/yyy/api/src/main/resources/keystore.p12
server.ssl.key-store-password: 123456
server.ssl.keyStoreType: PKCS12
</code></pre>
<p>and create the following java file:</p>
<pre><code class="language-java">import org.apache.catalina.connector.Connector;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory;
import org.springframework.boot.web.servlet.server.ServletWebServerFactory;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;


@Configuration
public class SSLConfig {
    @Value(&quot;${http.port}&quot;)
    private Integer port;

    @Bean
    public ServletWebServerFactory servletContainer() {

        TomcatServletWebServerFactory tomcat = new TomcatServletWebServerFactory();
        Connector connector = new Connector(&quot;org.apache.coyote.http11.Http11NioProtocol&quot;);
        connector.setPort(port);
        tomcat.addAdditionalTomcatConnectors(connector);
        return tomcat;
    }

}
</code></pre>
<h1 id="i-want-the-null-field-of-the-http-response-body-json-to-be-displayed-as-an-empty-string"><a class="header" href="#i-want-the-null-field-of-the-http-response-body-json-to-be-displayed-as-an-empty-string">I want the null field of the http response body json to be displayed as an empty string&quot;&quot;</a></h1>
<pre><code class="language-java">//demo
    @RequestMapping(&quot;/map&quot;)
    public Map&lt;String, Object&gt; getMap() {
        Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(3);
        User user = new User(1, &quot;xxx&quot;, null);
        map.put(&quot;auth&quot;, user);
        map.put(&quot;blog url&quot;, &quot;http://www.yyy.com&quot;);
        map.put(&quot;phone number&quot;, null);
        return map;
    }
</code></pre>
<p>Call interface display: {&quot;author information&quot;:{&quot;id&quot;:1,&quot;username&quot;:&quot;xxx&quot;,&quot;password&quot;:null},&quot;CSDN address&quot;:null,&quot;blog address&quot;:&quot; http://www .yyy.com&quot;}</p>
<p>Create the following java file:</p>
<pre><code class="language-java">import com.fasterxml.jackson.core.JsonGenerator;
import com.fasterxml.jackson.databind.*;
import com.fasterxml.jackson.databind.Module;
import com.fasterxml.jackson.databind.module.SimpleModule;
import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import org.springframework.http.converter.json.Jackson2ObjectMapperBuilder;

import java.io.IOException;

@Configuration
public class JacksonConfig {
    @Bean
    @Primary
    @ConditionalOnMissingBean(ObjectMapper.class)
    public ObjectMapper jacksonObjectMapper(Jackson2ObjectMapperBuilder builder) {
        ObjectMapper objectMapper = builder.createXmlMapper(false).build();
        objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, true);
        objectMapper.configure(MapperFeature.ALLOW_COERCION_OF_SCALARS, false);
        objectMapper.getSerializerProvider().setNullValueSerializer(new JsonSerializer&lt;Object&gt;() {
            @Override
            public void serialize(Object o, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException {
                jsonGenerator.writeString(&quot;&quot;);
            }
        });
        return objectMapper;
    }

    @Bean
    public Module customStringDeserializer() {
        SimpleModule module = new SimpleModule();
        module.addDeserializer(String.class, CustomStringDeserializer.instance);
        return module;
    }
}
</code></pre>
<p>Call interface display: {&quot;author information&quot;:{&quot;id&quot;:1,&quot;username&quot;:&quot;xxx&quot;,&quot;password&quot;:&quot;&quot;},&quot;CSDN address&quot;:&quot;&quot;,&quot;blog address&quot;:&quot; http:/ /www.yyy.com&quot;}</p>
<h1 id="i-want-the-null-field-of-http-response-body-json-not-to-be-displayed"><a class="header" href="#i-want-the-null-field-of-http-response-body-json-not-to-be-displayed">I want the null field of http response body json not to be displayed</a></h1>
<p>@JsonInclude(JsonInclude.Include.NON_NULL)Annotations can be used , for example:</p>
<pre><code class="language-java">import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;

@NoArgsConstructor
@Data
@JsonInclude(JsonInclude.Include.NON_NULL)
public class ListPoolTenantsResponse {

    /**
     * tenantId
     */
    @JsonProperty(&quot;tenantId&quot;)
    private String tenantId;
    /**
     * poolTenants
     */
    @JsonProperty(&quot;poolTenants&quot;)
    private List&lt;PoolTenantsDTO&gt; poolTenants;

    /**
     * PoolTenantsDTO
     */
    @NoArgsConstructor
    @Data
    @JsonInclude(JsonInclude.Include.NON_NULL)
    public static class PoolTenantsDTO {
        /**
         * id
         */
        @JsonProperty(&quot;id&quot;)
        private String id;
        /**
         * name
         */
        @JsonProperty(&quot;name&quot;)
        private String name;
        /**
         * poolId
         */
        @JsonProperty(&quot;poolId&quot;)
        private String poolId;
        /**
         * poolName
         */
        @JsonProperty(&quot;poolName&quot;)
        private String poolName;
        /**
         * poolType
         */
        @JsonProperty(&quot;poolType&quot;)
        private Integer poolType;
        /**
         * description
         */
        @JsonProperty(&quot;description&quot;)
        private String description;
        /**
         * status
         */
        @JsonProperty(&quot;status&quot;)
        private String status;
        /**
         * quotaSpec
         */
        @JsonProperty(&quot;quotaSpec&quot;)
        private QuotaSpecDTO quotaSpec;

        /**
         * QuotaSpecDTO
         */
        @NoArgsConstructor
        @Data
        @JsonInclude(JsonInclude.Include.NON_NULL)
        public static class QuotaSpecDTO {
            /**
             * cpu
             */
            @JsonProperty(&quot;cpu&quot;)
            private Integer cpu;
            /**
             * memory
             */
            @JsonProperty(&quot;memory&quot;)
            private Integer memory;
        }
    }
}
</code></pre>
<h1 id="when-mybatis-plus-is-used-as-orm-i-want-to-update-a-field-in-the-database-to-null"><a class="header" href="#when-mybatis-plus-is-used-as-orm-i-want-to-update-a-field-in-the-database-to-null">When Mybatis-Plus is used as ORM, I want to update a field in the database to null</a></h1>
<p>By default, the field cannot be updated to null. Even if it is updated to null, the query database finds that the field is still the original field and has not been updated, because mybatis-plus FieldStrategy has three strategies:</p>
<ul>
<li>IGNORED: 0 Ignore</li>
<li>NOT_NULL: 1 is not NULL, the default policy</li>
<li>NOT_EMPTY: 2 is not empty</li>
</ul>
<p>and the default update policy is NOT_NULL: not NULL; that is, when the data is NULL when updating data through the interface, it will not be updated into the database.</p>
<p>The default strategy of Mybatis-Plus provides great convenience for the update operation. For example, only the fields in the json body are updated when the http request is updated, and the fields that are not in the body will not be updated, which also meets the general requirements. It is a bit strange to update the fields that are not in the body to null. The general requirement is to keep fields that are null as-is.</p>
<p>If a field needs to be updated to null, it can be @TableField(updateStrategy = FieldStrategy.IGNORED)realized through annotations.</p>
<p>For example:</p>
<pre><code class="language-java">import com.alibaba.fastjson.JSONObject;
import com.baomidou.mybatisplus.annotation.FieldStrategy;
import com.baomidou.mybatisplus.annotation.TableField;
import com.baomidou.mybatisplus.annotation.TableId;
import com.baomidou.mybatisplus.annotation.TableName;
import com.baomidou.mybatisplus.extension.handlers.JacksonTypeHandler;
import com.fasterxml.jackson.databind.node.ObjectNode;
import lombok.Data;

@Data
@TableName(value = &quot;image&quot;, autoResultMap = true)
public class Image {
//    private String id;
    @TableId(&quot;name&quot;)
    private String name;
    private String description;
    private boolean publicView; //和Harbor一致
    private String type;
    private String registry; //镜像仓库地址，支持多镜像仓库
    private String repository; //&quot;project/repository&quot;的形式，和Harbor一致
    @TableField(typeHandler = JacksonTypeHandler.class, updateStrategy = FieldStrategy.IGNORED)
    private JSONObject lables; //&quot;lables&quot;: {&quot;gpu&quot;: &quot;yes&quot;}
}
</code></pre>
<h1 id="how-does-the-json-data-type-map-between-java-entity-fields-and-database-fields"><a class="header" href="#how-does-the-json-data-type-map-between-java-entity-fields-and-database-fields">How does the json data type map between Java entity fields and database fields</a></h1>
<p>The mapping between the json data type and the Java entity class is very common, and json nested json can also write a nested inner class in the Java entity class. The internal json data type corresponds to the internal class of the entity, but the current requirement is to associate it with a certain json type field in the database. It can be done in the following ways.</p>
<p>For example:</p>
<pre><code class="language-java">CREATE TABLE image
(
    #id     int AUTO_INCREMENT comment 'id',
--     id   varchar(80),
    name varchar(80),
    description varchar(500),
    public_view tinyint(1) comment '0:非公开；1:公开',
    type varchar(80),
    registry varchar(80),
    repository varchar(80),
    lables JSON,
    primary key (name)
);
</code></pre>
<pre><code class="language-java">import com.alibaba.fastjson.JSONObject;
import com.baomidou.mybatisplus.annotation.FieldStrategy;
import com.baomidou.mybatisplus.annotation.TableField;
import com.baomidou.mybatisplus.annotation.TableId;
import com.baomidou.mybatisplus.annotation.TableName;
import com.baomidou.mybatisplus.extension.handlers.JacksonTypeHandler;
import com.fasterxml.jackson.databind.node.ObjectNode;
import lombok.Data;

@Data
@TableName(value = &quot;image&quot;, autoResultMap = true)
public class Image {
//    private String id;
    @TableId(&quot;name&quot;)
    private String name;
    private String description;
    private boolean publicView; //和Harbor一致
    private String type;
    private String registry; //记录仓库地址，支持多镜像仓库
    private String repository; //&quot;project/repository&quot;的形式，和Harbor一致
    @TableField(typeHandler = JacksonTypeHandler.class, updateStrategy = FieldStrategy.IGNORED)
    private JSONObject lables; //&quot;lables&quot;: {&quot;gpu&quot;: &quot;yes&quot;}
}
</code></pre>
<p>typeHandler: Field type handler for conversion between JavaType and JdbcType.</p>
<p>If you do not follow the above processing, the following error will be reported:</p>
<pre><code>com.mysql.cj.jdbc.exceptions.MysqlDataTruncation: Data truncation: Cannot create a JSON value from a string with CHARACTER SET 'binary'.&quot;
</code></pre>
<h1 id="the-http-response-body-field-displays-the-time-format-on-demand"><a class="header" href="#the-http-response-body-field-displays-the-time-format-on-demand">The http response body field displays the time format on demand</a></h1>
<p>Add annotations before the response body field:</p>
<pre><code>@JsonFormat(pattern=&quot;yyyy-MM-dd HH:mm:ss&quot;, timezone=&quot;GMT+8&quot;) Display the time of East Eight District
@JsonFormat(pattern=&quot;yyyy-MM-dd'T'HH:mm: ss.SSS'Z'&quot;) displays UTC time, accurate to milliseconds, for example: 2017-06-01T00:00:00.000Z
</code></pre>
<h1 id="synchronize-data-between-servers"><a class="header" href="#synchronize-data-between-servers">Synchronize data between servers</a></h1>
<p>You can use the put method of com.jcraft.jsch.ChannelSftp to write a scheduled update task</p>
<pre><code class="language-java">@Async(&quot;taskScheduler&quot;)
@Scheduled(cron = &quot;0/15 * * * * ?&quot;)
public void update() {
</code></pre>
<p>Org.apache.camel can also be used to synchronize</p>
<pre><code class="language-java">import org.apache.camel.builder.RouteBuilder;
import org.springframework.stereotype.Component;

import java.util.List;

@Component
public class FtpRouteBuilder extends RouteBuilder {

    @Override
    public void configure() throws Exception {
        from(&quot;file:/tmp/ftp_test?recursive=true&amp;noop=true&quot;).to(&quot;sftp://10.10.10.10:33022/ftp_test?username=root&amp;password=123456@&amp;binary=true&quot;);
    }
}
</code></pre>
<h1 id="modification-log-packaging-cycle"><a class="header" href="#modification-log-packaging-cycle">Modification log packaging cycle</a></h1>
<h2 id="at-the-same-time-modify-the-packaging-by-unit-time-and-size-take-the-modification-of-systemlog-as-an-example"><a class="header" href="#at-the-same-time-modify-the-packaging-by-unit-time-and-size-take-the-modification-of-systemlog-as-an-example">At the same time, modify the packaging by unit time and size (take the modification of SystemLog as an example)</a></h2>
<pre><code>&lt;appender name=&quot;FILE-SFTP-SystemLog&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;
    &lt;File&gt;${FILE_PATH_SFTP_SystemLog}&lt;/File&gt;
    &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy&quot;&gt;
        &lt;FileNamePattern&gt;${FILE_PATH_SFTP_SystemLog}_%d{&quot;yyyy-MM-dd_HH&quot;, UTC}_0.zip&lt;/FileNamePattern&gt;
        &lt;MaxFileSize&gt;100MB&lt;/MaxFileSize&gt;
        &lt;totalSizeCap&gt;1GB&lt;/totalSizeCap&gt;
        &lt;MaxHistory&gt;2&lt;/MaxHistory&gt;
    &lt;/rollingPolicy&gt;

    &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt;
        &lt;level&gt;INFO&lt;/level&gt;
    &lt;/filter&gt;
    &lt;encoder&gt;
        &lt;pattern&gt;
            {
            &quot;time&quot;: &quot;%d{yyyy-MM-dd HH:mm:ss.SSS}&quot;,
            &quot;level&quot;: &quot;%level&quot;,
            &quot;thread&quot;: &quot;%thread&quot;,
            &quot;class&quot;: &quot;%logger{40}&quot;,
            &quot;message&quot;: &quot;%message&quot;
            }%n
        &lt;/pattern&gt;
        &lt;charset&gt;UTF-8&lt;/charset&gt;
    &lt;/encoder&gt;
&lt;/appender&gt;
</code></pre>
<pre><code class="language-bash">    &lt;rollingPolicy ... /rollingPolicy&gt;Several parameters are used to configure the log file splitting method:

    &lt;FileNamePattern&gt;The in yyyy-MM-dd_HH-mmmeans to split the file by hour, yyyy-MM-dd_HHmeans to split the file by hour, means to split the file yyyy-MM-ddby day, means to split the file by yyyy-MMmonth, and yyyymeans to split the file by year.
    &lt;MaxFileSize&gt;Indicates that the size of a packaged single file does not exceed this value.
    &lt;totalSizeCap&gt;Limit the size of log packaging.
    &lt;MaxHistory&gt;Limit the number of historical pack files.
</code></pre>
<h2 id="modify-logback-to-be-packaged-in-minutes-within-hours-take-modifying-systemlog-as-an-example-the-following-time-intervals-are-supported-1-2-5-10-15-20-30-minutes"><a class="header" href="#modify-logback-to-be-packaged-in-minutes-within-hours-take-modifying-systemlog-as-an-example-the-following-time-intervals-are-supported-1-2-5-10-15-20-30-minutes">Modify Logback to be packaged in minutes within hours (take modifying SystemLog as an example) (the following time intervals are supported: 1, 2, 5, 10, 15, 20, 30 minutes)</a></h2>
<pre><code>&lt;appender name=&quot;FILE-SFTP-SystemLog&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;
    &lt;File&gt;${FILE_PATH_SFTP_SystemLog}&lt;/File&gt;
    &lt;!--        &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy&quot;&gt;--&gt;
    &lt;rollingPolicy
            class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;
        &lt;!--            &lt;FileNamePattern&gt;${FILE_PATH_SFTP_OperationLog}_%d{yyyy-MM-dd-HH-mm}_%i.zip&lt;/FileNamePattern&gt;--&gt;
        &lt;FileNamePattern&gt;${FILE_PATH_SFTP_SystemLog}_%d{&quot;yyyy-MM-dd_HH-mm&quot;, UTC}_0.zip&lt;/FileNamePattern&gt;
        &lt;timeBasedFileNamingAndTriggeringPolicy
                class=&quot;xxx.yyy.api.config.MyTimeBasedFileNamingAndTriggeringPolicy&quot;&gt;
            &lt;multiple&gt;5&lt;/multiple&gt;
        &lt;/timeBasedFileNamingAndTriggeringPolicy&gt;
        &lt;!--            &lt;MaxFileSize&gt;100MB&lt;/MaxFileSize&gt;--&gt;
        &lt;!--            &lt;totalSizeCap&gt;1GB&lt;/totalSizeCap&gt;--&gt;
        &lt;MaxHistory&gt;90&lt;/MaxHistory&gt;
    &lt;/rollingPolicy&gt;
    &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt;
        &lt;level&gt;INFO&lt;/level&gt;
    &lt;/filter&gt;
    &lt;encoder&gt;
        &lt;pattern&gt;
            {
            &quot;time&quot;: &quot;%d{yyyy-MM-dd HH:mm:ss.SSS}&quot;,
            &quot;level&quot;: &quot;%level&quot;,
            &quot;thread&quot;: &quot;%thread&quot;,
            &quot;class&quot;: &quot;%logger{40}&quot;,
            &quot;message&quot;: &quot;%message&quot;
            }%n
        &lt;/pattern&gt;
        &lt;charset&gt;UTF-8&lt;/charset&gt;
    &lt;/encoder&gt;
&lt;/appender&gt;
</code></pre>
<pre><code class="language-bash">&lt;rollingPolicy ... /rollingPolicy&gt;Several parameters are used to configure the log file splitting method:

&lt;FileNamePattern&gt;The time format part in can only be fixed to yyyy-MM-dd_HH-mm.
&lt;multiple&gt;Indicates the packaging interval in minutes. The following intervals are supported: 1, 2, 5, 10, 12, 15, 20, 30 minutes.
&lt;MaxHistory&gt;Limit the time of historical pack files. That is to say, the number of historical pack files is &lt;MaxHistory&gt;divided by &lt;multiple&gt;.
</code></pre>
<p>The MyTimeBasedFileNamingAndTriggeringPolicy.java file is as follows:</p>
<pre><code class="language-java">package xxx.yyy.api.config;

import ch.qos.logback.core.joran.spi.NoAutoStart;
import ch.qos.logback.core.rolling.DefaultTimeBasedFileNamingAndTriggeringPolicy;

@NoAutoStart
public class MyTimeBasedFileNamingAndTriggeringPolicy&lt;E&gt; extends DefaultTimeBasedFileNamingAndTriggeringPolicy&lt;E&gt; {
    //这个用来指定时间间隔
    private Integer multiple = 1;

    @Override
    protected void computeNextCheck() {
        nextCheck = rc.getEndOfNextNthPeriod(dateInCurrentPeriod, multiple).getTime();
    }

    public Integer getMultiple() {
        return multiple;
    }

    public void setMultiple(Integer multiple) {
        if (multiple &gt; 1) {
            this.multiple = multiple;
        }
    }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2021-11-23 14:22</p>
<p>The same code is pulled from the git repository, some machines report errors, and some machines run completely normally. Check the place where the error is reported, the error of the private certificate made in order to open the https service:</p>
<pre><code>Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2021-11-10 09:55:47.873 [ERROR] [restartedMain] org.springframework.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Failed to start bean 'webServerStartStop'; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat server
    at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:181)
    at org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:54)
    at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:356)
    at java.lang.Iterable.forEach(Iterable.java:75)
    at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:155)
    at org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:123)
    at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:935)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:586)
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:145)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758)
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:438)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:337)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1336)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1325)
    at com.chinamobile.edge.cmp.api.WebApplication.main(WebApplication.java:15)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat server
    at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.start(TomcatWebServer.java:229)
    at org.springframework.boot.web.servlet.context.WebServerStartStopLifecycle.start(WebServerStartStopLifecycle.java:43)
    at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:178)
    ... 19 common frames omitted
Caused by: java.lang.IllegalArgumentException: standardService.connector.startFailed
    at org.apache.catalina.core.StandardService.addConnector(StandardService.java:243)
    at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.addPreviouslyRemovedConnectors(TomcatWebServer.java:282)
    at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.start(TomcatWebServer.java:213)
    ... 21 common frames omitted
Caused by: org.apache.catalina.LifecycleException: Protocol handler start failed
    at org.apache.catalina.connector.Connector.startInternal(Connector.java:1075)
    at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183)
    at org.apache.catalina.core.StandardService.addConnector(StandardService.java:239)
    ... 23 common frames omitted
Caused by: java.lang.IllegalArgumentException: Key protection  algorithm not found: java.security.UnrecoverableKeyException: Encrypt Private Key failed: unrecognized algorithm name: PBEWithSHA1AndDESede
    at org.apache.tomcat.util.net.AbstractJsseEndpoint.createSSLContext(AbstractJsseEndpoint.java:99)
    at org.apache.tomcat.util.net.AbstractJsseEndpoint.initialiseSsl(AbstractJsseEndpoint.java:71)
    at org.apache.tomcat.util.net.NioEndpoint.bind(NioEndpoint.java:258)
    at org.apache.tomcat.util.net.AbstractEndpoint.bindWithCleanup(AbstractEndpoint.java:1204)
    at org.apache.tomcat.util.net.AbstractEndpoint.start(AbstractEndpoint.java:1290)
    at org.apache.coyote.AbstractProtocol.start(AbstractProtocol.java:614)
    at org.apache.catalina.connector.Connector.startInternal(Connector.java:1072)
    ... 25 common frames omitted
Caused by: java.security.KeyStoreException: Key protection  algorithm not found: java.security.UnrecoverableKeyException: Encrypt Private Key failed: unrecognized algorithm name: PBEWithSHA1AndDESede
    at sun.security.pkcs12.PKCS12KeyStore.setKeyEntry(PKCS12KeyStore.java:694)
    at sun.security.pkcs12.PKCS12KeyStore.engineSetKeyEntry(PKCS12KeyStore.java:594)
    at java.security.KeyStore.setKeyEntry(KeyStore.java:1140)
    at org.apache.tomcat.util.net.SSLUtilBase.getKeyManagers(SSLUtilBase.java:365)
    at org.apache.tomcat.util.net.SSLUtilBase.createSSLContext(SSLUtilBase.java:246)
    at org.apache.tomcat.util.net.AbstractJsseEndpoint.createSSLContext(AbstractJsseEndpoint.java:97)
    ... 31 common frames omitted
Caused by: java.security.UnrecoverableKeyException: Encrypt Private Key failed: unrecognized algorithm name: PBEWithSHA1AndDESede
    at sun.security.pkcs12.PKCS12KeyStore.encryptPrivateKey(PKCS12KeyStore.java:938)
    at sun.security.pkcs12.PKCS12KeyStore.setKeyEntry(PKCS12KeyStore.java:631)
    ... 36 common frames omitted
Caused by: java.security.NoSuchAlgorithmException: unrecognized algorithm name: PBEWithSHA1AndDESede
    at sun.security.x509.AlgorithmId.get(AlgorithmId.java:448)
    at sun.security.pkcs12.PKCS12KeyStore.mapPBEAlgorithmToOID(PKCS12KeyStore.java:955)
    at sun.security.pkcs12.PKCS12KeyStore.encryptPrivateKey(PKCS12KeyStore.java:912)
    ... 37 common frames omitted
</code></pre>
<p>After checking, it is a bug of jdk 292 version. Roll back to an old version or upgrade to a newer version will be fine.</p>
<ul>
<li>Oracle JDK 8u291: Passed.</li>
<li>Open JDK 8u282: Passed.</li>
<li>Open JDK 8u292: Failed, KeyStoreException thrown.</li>
<li>OpenJDK 8u302-b04 Passed.</li>
</ul>
<blockquote>
<p>https://bugs.openjdk.java.net/browse/JDK-8266261
https://bugs.openjdk.java.net/browse/JDK-8242565</p>
</blockquote>
<p>After upgrading the version of JDK from 292 to 312, everything works fine.</p>
<pre><code>hanwei@hanweideMacBook-Air tt]$ java -version
openjdk version &quot;1.8.0_292&quot;
OpenJDK Runtime Environment (Zulu 8.54.0.21-CA-macos-aarch64) (build 1.8.0_292-b10)
OpenJDK 64-Bit Server VM (Zulu 8.54.0.21-CA-macos-aarch64) (build 25.292-b10, mixed mode)
hanwei@hanweideMacBook-Air tt]$ java -version
openjdk version &quot;1.8.0_312&quot;
OpenJDK Runtime Environment (Zulu 8.58.0.13-CA-macos-aarch64) (build 1.8.0_312-b07)
OpenJDK 64-Bit Server VM (Zulu 8.58.0.13-CA-macos-aarch64) (build 25.312-b07, mixed mode)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2021-09-13 14:22</p>
<h1 id="implementation-plan"><a class="header" href="#implementation-plan">Implementation plan</a></h1>
<p>To create a resource pool tenant in the Java version of the cloud management platform project, the southbound interface needs to create a namespace for the underlying Kubernetes and create a Kubernetes user with the same name.</p>
<p>Requests to the k8s apiserver (kubectl client, client library, or constructing REST requests to access the kubernetes API).</p>
<p>The Java version of the Kubernetes client library has the official version io.kubernetes.client and the unofficial io.fabric8.kubernetes. The latter's unofficial ones are stronger than the official ones, so the plan is to choose unofficial ones. However, after investigation, it is found that even unofficial interfaces have not been implemented. For example, the &quot;kubectl config set-context&quot; that needs to be used in the requirements solution does not correspond to the library, but the interface is implemented. Refer to the discussion in the github community https://github.com/fabric8io/kubernetes-client/issues/1512 These interface client libraries have not been implemented. </p>
<pre><code class="language-java">    public static void nsCreateOrReplace(String yamlString, String name, String quota) throws IOException {
        KubernetesClient k8sClient = KubernetesClientGenerator.fromYamlStringGetKubernetesClient(yamlString);
        Namespace ns = new NamespaceBuilder().withNewMetadata().withName(name).endMetadata().build();
        k8sClient.namespaces().createOrReplace(ns);
        ResourceQuota resourceQuota = new ResourceQuotaBuilder()
                .withNewMetadata().withName(&quot;ns-quota&quot;).endMetadata()
                .withNewSpec()
                .addToHard(&quot;cpu&quot;, new Quantity(&quot;0&quot;))
                .addToHard(&quot;memory&quot;, new Quantity(&quot;0&quot;))
                .addToHard(&quot;pods&quot;, new Quantity(&quot;0&quot;))
                .endSpec().build();
        k8sClient.resourceQuotas().inNamespace(name).createOrReplace(resourceQuota);
        if (quota == null) {return;}
        Gson gson = new Gson();
        ListVDCDetailResponse.PoolTenantsDTO.QuotaSpecDTO quotaSpec = gson.fromJson(quota, ListVDCDetailResponse.PoolTenantsDTO.QuotaSpecDTO.class);
//        if (cpu == null) {cpu = 0;}
//        if (memory == null) {memory = 0;}
//        if (pods ==  null) {pods = 0;}
        if (quotaSpec == null) {return;}
        resourceQuota = new ResourceQuotaBuilder()
                .withNewMetadata().withName(&quot;ns-quota&quot;).endMetadata()
                .withNewSpec()
                .addToHard(&quot;cpu&quot;, new Quantity(quotaSpec.getCpu().toString()))
                .addToHard(&quot;memory&quot;, new Quantity(quotaSpec.getMemory().toString()))
                .addToHard(&quot;pods&quot;, new Quantity(quotaSpec.getPods().toString()))
                .endSpec().build();
        k8sClient.resourceQuotas().inNamespace(name).createOrReplace(resourceQuota);
    }
</code></pre>
<pre><code class="language-java">public static void nsDelete(String yamlString, String name) throws IOException {
    KubernetesClient k8sClient = KubernetesClientGenerator.fromYamlStringGetKubernetesClient(yamlString);
    Namespace ns = new NamespaceBuilder().withNewMetadata().withName(name).endMetadata().build();
    k8sClient.namespaces().delete(ns);
}
</code></pre>
<pre><code class="language-java">public static String createKubeConfig(String cluster, String namespace) throws IOException {
    File file = new File(&quot;/etc/cmp/kubeconfig/create-kubeconfig.sh&quot;);
    if(file.exists()) {
        String tt = &quot;/etc/cmp/kubeconfig/create-kubeconfig.sh &quot; + cluster + &quot; &quot; + namespace + &quot; 2&gt;&amp;1&quot;;
        String[] cmd = new String[] { &quot;/bin/bash&quot;, &quot;-c&quot;, tt};
        Process ps = Runtime.getRuntime().exec(cmd);
        BufferedReader br = new BufferedReader(new InputStreamReader(ps.getInputStream()));
        StringBuffer sb = new StringBuffer();
        String line;
        while ((line = br.readLine()) != null) {
            sb.append(line).append(&quot;\n&quot;);
        }
        String result = sb.toString();
        log.debug(tt);
        log.debug(result);
        String configFilePath = &quot;/etc/cmp/kubeconfig/&quot; + cluster + &quot;/&quot; + namespace + &quot;.config&quot;;
        File configFile = new File(configFilePath);
        if(configFile.exists()) {
            BufferedReader in = new BufferedReader(new FileReader(configFilePath));
            String str;
            StringBuffer sb2 = new StringBuffer();
            while ((str = in.readLine()) != null) {
                sb2.append(str).append(&quot;\n&quot;);
            }
            String result2 = sb2.toString();
            log.debug(&quot;kube config:&quot;);
            log.debug(result2);
            return result2;
        }
    }
    return null;
}
</code></pre>
<pre><code class="language-java">public static void deleteKubeConfig(String cluster, String namespace) throws IOException {
    File file = new File(&quot;/etc/cmp/kubeconfig/delete-kubeconfig.sh&quot;);
    if(file.exists()) {
        String[] cmd = new String[] { &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;/etc/cmp/kubeconfig/delete-kubeconfig.sh &quot; + cluster + &quot; &quot; + namespace};
        Process ps = Runtime.getRuntime().exec(cmd);
        BufferedReader br = new BufferedReader(new InputStreamReader(ps.getInputStream()));
        StringBuffer sb = new StringBuffer();
        String line;
        while ((line = br.readLine()) != null) {
            sb.append(line).append(&quot;\n&quot;);
        }
        String result = sb.toString();
        log.info(result);
    }
}
</code></pre>
<p>The functions realized by the above four functions are: create ns, delete ns, create kubeconfig, delete kubeconfig.</p>
<p>Two shell scripts are called in the java program that creates and deletes kubeconfig create-kubeconfig.shand delete-kubeconfig.shis used to create and delete kubeconfig respectively.</p>
<pre><code># cat create-kubeconfig.sh
set -v
[[ $# -ne 2 ]] &amp;&amp; usage

mkdir -p /etc/cmp/kubeconfig/$1/
cd /etc/cmp/kubeconfig/$1/
openssl genrsa -out $2.key 2048
openssl req -new -key $2.key -out $2.csr -subj &quot;/CN=$2/O=$2&quot;
openssl x509 -req -in $2.csr -CA kubernetes/ca.crt -CAkey kubernetes/ca.key -CAcreateserial -out $2.crt -days 36500
kubectl config set-credentials $2 --client-certificate=$2.crt  --client-key=$2.key
kubectl config set-context $2 --cluster=$1 --namespace=$2 --user=$2
kubectl config view --flatten --merge --minify --context $2 &gt; $2.config
#kubectl create ns $2

cat&gt;$2-rolebinding.yaml&lt;&lt;EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
name: $2-rolebinding
namespace: $2
subjects:
- kind: User
name: $2
apiGroup: &quot;&quot;
roleRef:
kind: ClusterRole
name: admin
apiGroup: &quot;&quot;
EOF

kubectl apply -f $2-rolebinding.yaml

usage(){
    echo &quot;Usage: $0 &lt;cluster-name&gt; &lt;namespace-name&gt;&quot;
    exit 1
}
</code></pre>
<pre><code class="language-bash"># cat delete-kubeconfig.sh
set -v
if [ $# -ne 2 ];
then
    exit
fi

cd /etc/cmp/kubeconfig/$1/
kubectl delete -f $2-rolebinding.yaml
kubectl config delete-context $2
kubectl config delete-user $2
rm -rf $2-rolebinding.yaml $2.config $2.key $2.crt $2.csr
#kubectl delete ns $2
</code></pre>
<h2 id="create-user-credentials"><a class="header" href="#create-user-credentials">Create user credentials</a></h2>
<p>Kubernetes does not have a User Account API object. To create a user account, use a private key assigned by the administrator to create it. Refer to the method in the official document to create a User with an OpenSSL certificate, or use the simpler cfssl tool to create:</p>
<p>Create a private key for user xxx and name it: xxx.key:</p>
<pre><code>$ openssl genrsa -out xxx.key 2048
</code></pre>
<p>Use the private key you just created to create a certificate signing request file: xxx.csr, be sure to specify the user name and group in the -subj parameter (CN means user name, O means group):</p>
<pre><code>$ openssl req -new -key xxx.key -out xxx.csr -subj &quot;/CN=xxx/O=xxx&quot;
</code></pre>
<p>Then find the CA of the Kubernetes cluster. If the cluster installed by kubeadm is used, the CA-related certificate is located under the /etc/kubernetes/ssl/ directory. If it is built in binary mode, the CA directory has been specified when the cluster was first built. Use the two files ca.crt and ca.key under this directory to approve the above certificate request</p>
<p>Generate the final certificate file, here we set the validity period of the certificate to 500 days:</p>
<pre><code>$ openssl x509 -req -in xxx.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out xxx.crt -days 500

Now check whether a certificate file is generated under the current folder:

$ ls
xxx.csr xxx.key xxx.crt
</code></pre>
<p>Now you can create a new credential and context in the cluster using the certificate file and private key file you just created:</p>
<pre><code>$ kubectl config set-credentials xxx –client-certificate=xxx.crt –client-key=xxx.key
</code></pre>
<p>You can see that a user xxx has been created, and then set a new Context for this user:</p>
<pre><code>$ kubectl config set-context xxx –cluster=kubernetes –namespace=kube-system –user=xxx
</code></pre>
<p>At this point, the user haimaxy has been successfully created. Now when using the current configuration file to operate the kubectl command, an error should occur, because no operation permissions have been defined for the user:</p>
<pre><code>$ kubectl get pods –context=xxx
</code></pre>
<p>Error from server (Forbidden): pods is forbidden: User “xxx” cannot list pods in the namespace “default”</p>
<h2 id="create-roles-and-role-permission-bindings"><a class="header" href="#create-roles-and-role-permission-bindings">Create roles and role permission bindings</a></h2>
<p>Role and ClusterRole: role and cluster role, both objects contain the above Rules element, the difference between the two is that in Role, the defined rules are only applicable to a single namespace, which is associated with namespace, while ClusterRole is Cluster-wide, so defined rules are not bound by namespaces.</p>
<p>After the user is created, it is necessary to add operation permissions to the user. Let's define a YAML file and create a role that allows the user to operate Deployment, Pod, and ReplicaSets, as defined below: (xxx-role.yaml)</p>
<pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
name: xxx-role
namespace: kube-system
rules:
- apiGroups: [&quot;&quot;, &quot;extensions&quot;, &quot;apps&quot;]
resources: [&quot;deployments&quot;, &quot;replicasets&quot;, &quot;pods&quot;]
verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;] # 也可以使用['*']
</code></pre>
<p>Among them, Pod belongs to the core API Group, and you can use empty characters in YAML, while Deployment belongs to the apps API Group, and ReplicaSets belongs to the extensions API Group, so apiGroups under rules integrates the API Groups of these resources: [&quot; &quot;, &quot;extensions&quot;, &quot;apps&quot;], where verbs can perform operations on these resource objects, all operation methods are required, and ['*'] can also be used instead.</p>
<p>Then create this Role:</p>
<p>$ kubectl create -f xxx-role.yaml</p>
<p>Note that the context of xxx above is not used here, because there is no permission</p>
<p>The Role is created. This Role has nothing to do with the user xxx. You need to create a RoleBinding object, and bind the above xxx-role role to the user xxx under the kube-system namespace: (xxx-rolebinding.yaml)</p>
<pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
name: xxx-rolebinding
namespace: kube-system
subjects:
- kind: User
name: xxx
apiGroup: &quot;&quot;
roleRef:
kind: Role
name: xxx-role
apiGroup: “&quot;
</code></pre>
<p>The subjects keyword in the above YAML file is the object mentioned above to try to operate the cluster, which corresponds to the above User account xxx, and use kubectl to create the above resource object:</p>
<pre><code>$ kubectl create -f xxx-rolebinding.yaml
</code></pre>
<p>test</p>
<p>Now you should be able to operate the cluster with the xxx context above:</p>
<pre><code>$ kubectl get pods --context=xxx
....
</code></pre>
<p>It can be seen that the use of kubectl does not specify a namespace. This is because we have assigned permissions to the user, and we can view all pods in the assigned namespace. If we add a -n default at the end</p>
<pre><code>$ kubectl –context=xxx-context get pods –namespace=default
</code></pre>
<p>Error from server (Forbidden): pods is forbidden: User “xxx” cannot list pods in the namespace “default”
is as expected. Because the user does not have the operation permission of the default namespace</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2021-08-10 07:06</p>
<p>Continuing from the previous article, iperf3 tests the four-layer performance of Kubernetes Service (Part 1)</p>
<p>Start 16 iperf3 clients, simulating 2000 concurrency (a single iperf3 client can only be up to 128 concurrency)</p>
<pre><code>#  cat iperf3.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
name: iperf3-server-deployment
labels:
    app: iperf3-server
spec:
replicas: 1
selector:
    matchLabels:
    app: iperf3-server
template:
    metadata:
    labels:
        app: iperf3-server
    spec:
    affinity:
        nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
            preference:
            matchExpressions:
            - key: kubernetes.io/role
                operator: In
                values:
                - master
    #tolerations:
    #  - key: node-role.kubernetes.io/master
    #    operator: Exists
    #    effect: NoSchedule
    containers:
    - name: iperf3-server
        image: registry.paas/cmss/iperf3
        command: [&quot;/bin/sh&quot;,&quot;-c&quot;]
        args:
        - |
            iperf3 -s -p 5201&amp;
            iperf3 -s -p 5202&amp;
            iperf3 -s -p 5203&amp;
            iperf3 -s -p 5204&amp;
            iperf3 -s -p 5205&amp;
            iperf3 -s -p 5206&amp;
            iperf3 -s -p 5207&amp;
            iperf3 -s -p 5208&amp;
            iperf3 -s -p 5209&amp;
            iperf3 -s -p 5210&amp;
            iperf3 -s -p 5211&amp;
            iperf3 -s -p 5212&amp;
            iperf3 -s -p 5213&amp;
            iperf3 -s -p 5214&amp;
            iperf3 -s -p 5215&amp;
            iperf3 -s -p 5216&amp;
            sleep infinity
        ports:
        - containerPort: 5201
        - containerPort: 5202
        - containerPort: 5203
        - containerPort: 5204
        - containerPort: 5205
        - containerPort: 5206
        - containerPort: 5207
        - containerPort: 5208
        - containerPort: 5209
        - containerPort: 5210
        - containerPort: 5211
        - containerPort: 5212
        - containerPort: 5213
        - containerPort: 5214
        - containerPort: 5215
        - containerPort: 5216
    terminationGracePeriodSeconds: 0


---


apiVersion: v1
kind: Service
metadata:
name: iperf3-server
spec:
selector:
    app: iperf3-server
ports:
- port: 5201
    targetPort: 5201
    name: server5201
- port: 5202
    targetPort: 5202
    name: server5202
- port: 5203
    targetPort: 5203
    name: server5203
- port: 5204
    targetPort: 5204
    name: server5204
- port: 5205
    targetPort: 5205
    name: server5205
- port: 5206
    targetPort: 5206
    name: server5206
- port: 5207
    targetPort: 5207
    name: server5207
- port: 5208
    targetPort: 5208
    name: server5208
- port: 5209
    targetPort: 5209
    name: server5209
- port: 5210
    targetPort: 5210
    name: server5210
- port: 5211
    targetPort: 5211
    name: server5211
- port: 5212
    targetPort: 5212
    name: server5212
- port: 5213
    targetPort: 5213
    name: server5213
- port: 5214
    targetPort: 5214
    name: server5214
- port: 5215
    targetPort: 5215
    name: server5215
- port: 5216
    targetPort: 5216
    name: server5216
    
---
    
apiVersion: apps/v1
kind: DaemonSet
metadata:
name: iperf3-clients
labels:
    app: iperf3-client
spec:
selector:
    matchLabels:
    app: iperf3-client
template:
    metadata:
    labels:
        app: iperf3-client
    spec:
    affinity:
        nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
            - key: kubernetes.io/hostname
                operator: NotIn
                values:
                - vcdn-bm6
                - vcdn-bm7
    #tolerations:
    #  - key: node-role.kubernetes.io/master
    #    operator: Exists
    #    effect: NoSchedule
    containers:
    - name: iperf3-client
        image: registry.paas/cmss/iperf3
        command: ['/bin/sh', '-c', 'sleep infinity']
        # To benchmark manually: kubectl exec iperf3-clients-jlfxq -- /bin/sh -c 'iperf3 -c iperf3-server'
    terminationGracePeriodSeconds: 0
</code></pre>
<pre><code class="language-bash">iperf3 -c iperf3-server -t 10 -p 5201 -M 2000 -P 125 -b 0 &gt;tt1.log&amp;
iperf3 -c iperf3-server -t 10 -p 5202 -M 2000 -P 125 -b 0 &gt;tt2.log&amp;
iperf3 -c iperf3-server -t 10 -p 5203 -M 2000 -P 125 -b 0 &gt;tt3.log&amp;
iperf3 -c iperf3-server -t 10 -p 5204 -M 2000 -P 125 -b 0 &gt;tt4.log&amp;
iperf3 -c iperf3-server -t 10 -p 5205 -M 2000 -P 125 -b 0 &gt;tt5.log&amp;
iperf3 -c iperf3-server -t 10 -p 5206 -M 2000 -P 125 -b 0 &gt;tt6.log&amp;
iperf3 -c iperf3-server -t 10 -p 5207 -M 2000 -P 125 -b 0 &gt;tt7.log&amp;
iperf3 -c iperf3-server -t 10 -p 5208 -M 2000 -P 125 -b 0 &gt;tt8.log&amp;
iperf3 -c iperf3-server -t 10 -p 5209 -M 2000 -P 125 -b 0 &gt;tt9.log&amp;
iperf3 -c iperf3-server -t 10 -p 5210 -M 2000 -P 125 -b 0 &gt;tt10.log&amp;
iperf3 -c iperf3-server -t 10 -p 5211 -M 2000 -P 125 -b 0 &gt;tt11.log&amp;
iperf3 -c iperf3-server -t 10 -p 5212 -M 2000 -P 125 -b 0 &gt;tt12.log&amp;
iperf3 -c iperf3-server -t 10 -p 5213 -M 2000 -P 125 -b 0 &gt;tt13.log&amp;
iperf3 -c iperf3-server -t 10 -p 5214 -M 2000 -P 125 -b 0 &gt;tt14.log&amp;
iperf3 -c iperf3-server -t 10 -p 5215 -M 2000 -P 125 -b 0 &gt;tt15.log&amp;
iperf3 -c iperf3-server -t 10 -p 5216 -M 2000 -P 125 -b 0 &gt;tt16.log&amp;
</code></pre>
<pre><code>root@iperf3-clients-4cqqc:/# grep SUM *.log
tt1.log:[SUM]   0.00-1.00   sec  81.5 MBytes   683 Mbits/sec  1078             
tt1.log:[SUM]   1.00-2.00   sec  70.8 MBytes   594 Mbits/sec  1036             
tt1.log:[SUM]   2.00-3.00   sec  69.1 MBytes   580 Mbits/sec  923             
tt1.log:[SUM]   3.00-4.00   sec  68.0 MBytes   570 Mbits/sec  978             
tt1.log:[SUM]   4.00-5.00   sec  68.3 MBytes   572 Mbits/sec  1120             
tt1.log:[SUM]   5.00-6.00   sec  69.1 MBytes   581 Mbits/sec  1368             
tt1.log:[SUM]   6.00-7.00   sec  68.1 MBytes   571 Mbits/sec  891             
tt1.log:[SUM]   7.00-8.00   sec  69.7 MBytes   585 Mbits/sec  1170             
tt1.log:[SUM]   8.00-9.00   sec  67.3 MBytes   564 Mbits/sec  1068             
tt1.log:[SUM]   9.00-10.00  sec  67.9 MBytes   570 Mbits/sec  866             
tt1.log:[SUM]   0.00-10.00  sec   700 MBytes   587 Mbits/sec  10498             sender
tt1.log:[SUM]   0.00-10.09  sec   685 MBytes   570 Mbits/sec                  receiver
tt10.log:[SUM]   0.00-1.00   sec  74.2 MBytes   623 Mbits/sec  902             
tt10.log:[SUM]   1.00-2.00   sec  67.0 MBytes   562 Mbits/sec  900             
tt10.log:[SUM]   2.00-3.00   sec  66.4 MBytes   557 Mbits/sec  760             
tt10.log:[SUM]   3.00-4.00   sec  64.9 MBytes   545 Mbits/sec  864             
tt10.log:[SUM]   4.00-5.00   sec  65.2 MBytes   547 Mbits/sec  1181             
tt10.log:[SUM]   5.00-6.00   sec  64.6 MBytes   542 Mbits/sec  1348             
tt10.log:[SUM]   6.00-7.00   sec  64.3 MBytes   539 Mbits/sec  1256             
tt10.log:[SUM]   7.00-8.00   sec  63.7 MBytes   535 Mbits/sec  1342             
tt10.log:[SUM]   8.00-9.00   sec  61.7 MBytes   518 Mbits/sec  1147             
tt10.log:[SUM]   9.00-10.00  sec  63.3 MBytes   531 Mbits/sec  1031             
tt10.log:[SUM]   0.00-10.00  sec   656 MBytes   550 Mbits/sec  10731             sender
tt10.log:[SUM]   0.00-10.11  sec   643 MBytes   533 Mbits/sec                  receiver
tt11.log:[SUM]   0.00-1.00   sec  75.6 MBytes   634 Mbits/sec  1034             
tt11.log:[SUM]   1.00-2.00   sec  69.9 MBytes   586 Mbits/sec  984             
tt11.log:[SUM]   2.00-3.00   sec  69.2 MBytes   581 Mbits/sec  808             
tt11.log:[SUM]   3.00-4.00   sec  69.5 MBytes   582 Mbits/sec  723             
tt11.log:[SUM]   4.00-5.00   sec  67.5 MBytes   567 Mbits/sec  1133             
tt11.log:[SUM]   5.00-6.00   sec  68.7 MBytes   576 Mbits/sec  1241             
tt11.log:[SUM]   6.00-7.00   sec  68.8 MBytes   577 Mbits/sec  902             
tt11.log:[SUM]   7.00-8.00   sec  67.3 MBytes   565 Mbits/sec  1115             
tt11.log:[SUM]   8.00-9.00   sec  67.1 MBytes   563 Mbits/sec  1177             
tt11.log:[SUM]   9.00-10.00  sec  67.5 MBytes   566 Mbits/sec  725             
tt11.log:[SUM]   0.00-10.00  sec   691 MBytes   580 Mbits/sec  9842             sender
tt11.log:[SUM]   0.00-10.12  sec   678 MBytes   562 Mbits/sec                  receiver
tt12.log:[SUM]   0.00-1.00   sec  79.6 MBytes   668 Mbits/sec  1037             
tt12.log:[SUM]   1.00-2.00   sec  73.6 MBytes   617 Mbits/sec  806             
tt12.log:[SUM]   2.00-3.00   sec  70.1 MBytes   588 Mbits/sec  915             
tt12.log:[SUM]   3.00-4.00   sec  69.1 MBytes   579 Mbits/sec  853             
tt12.log:[SUM]   4.00-5.00   sec  71.4 MBytes   599 Mbits/sec  969             
tt12.log:[SUM]   5.00-6.00   sec  71.9 MBytes   603 Mbits/sec  963             
tt12.log:[SUM]   6.00-7.00   sec  70.7 MBytes   593 Mbits/sec  973             
tt12.log:[SUM]   7.00-8.00   sec  68.3 MBytes   573 Mbits/sec  1223             
tt12.log:[SUM]   8.00-9.00   sec  67.3 MBytes   564 Mbits/sec  1078             
tt12.log:[SUM]   9.00-10.00  sec  69.0 MBytes   579 Mbits/sec  628             
tt12.log:[SUM]   0.00-10.00  sec   711 MBytes   596 Mbits/sec  9445             sender
tt12.log:[SUM]   0.00-10.09  sec   694 MBytes   577 Mbits/sec                  receiver
tt13.log:[SUM]   0.00-1.00   sec  77.7 MBytes   651 Mbits/sec  935             
tt13.log:[SUM]   1.00-2.00   sec  70.6 MBytes   593 Mbits/sec  1005             
tt13.log:[SUM]   2.00-3.00   sec  69.4 MBytes   582 Mbits/sec  902             
tt13.log:[SUM]   3.00-4.00   sec  68.7 MBytes   576 Mbits/sec  998             
tt13.log:[SUM]   4.00-5.00   sec  68.2 MBytes   572 Mbits/sec  1079             
tt13.log:[SUM]   5.00-6.00   sec  68.4 MBytes   574 Mbits/sec  1293             
tt13.log:[SUM]   6.00-7.00   sec  66.7 MBytes   560 Mbits/sec  933             
tt13.log:[SUM]   7.00-8.00   sec  66.7 MBytes   559 Mbits/sec  1036             
tt13.log:[SUM]   8.00-9.00   sec  66.6 MBytes   559 Mbits/sec  1088             
tt13.log:[SUM]   9.00-10.00  sec  67.5 MBytes   566 Mbits/sec  1010             
tt13.log:[SUM]   0.00-10.00  sec   691 MBytes   579 Mbits/sec  10279             sender
tt13.log:[SUM]   0.00-10.00  sec   670 MBytes   562 Mbits/sec                  receiver
tt14.log:[SUM]   0.00-1.00   sec  76.5 MBytes   642 Mbits/sec  1015             
tt14.log:[SUM]   1.00-2.00   sec  67.0 MBytes   562 Mbits/sec  921             
tt14.log:[SUM]   2.00-3.00   sec  65.7 MBytes   551 Mbits/sec  982             
tt14.log:[SUM]   3.00-4.00   sec  66.1 MBytes   555 Mbits/sec  923             
tt14.log:[SUM]   4.00-5.00   sec  62.6 MBytes   525 Mbits/sec  1208             
tt14.log:[SUM]   5.00-6.00   sec  63.4 MBytes   532 Mbits/sec  1095             
tt14.log:[SUM]   6.00-7.00   sec  64.1 MBytes   538 Mbits/sec  1191             
tt14.log:[SUM]   7.00-8.00   sec  64.0 MBytes   537 Mbits/sec  893             
tt14.log:[SUM]   8.00-9.00   sec  65.1 MBytes   546 Mbits/sec  1084             
tt14.log:[SUM]   9.00-10.00  sec  66.1 MBytes   555 Mbits/sec  846             
tt14.log:[SUM]   0.00-10.00  sec   661 MBytes   554 Mbits/sec  10158             sender
tt14.log:[SUM]   0.00-10.00  sec   640 MBytes   537 Mbits/sec                  receiver
tt15.log:[SUM]   0.00-1.00   sec  77.4 MBytes   649 Mbits/sec  877             
tt15.log:[SUM]   1.00-2.00   sec  73.3 MBytes   615 Mbits/sec  902             
tt15.log:[SUM]   2.00-3.00   sec  70.8 MBytes   594 Mbits/sec  823             
tt15.log:[SUM]   3.00-4.00   sec  70.7 MBytes   593 Mbits/sec  894             
tt15.log:[SUM]   4.00-5.00   sec  70.4 MBytes   590 Mbits/sec  902             
tt15.log:[SUM]   5.00-6.00   sec  70.9 MBytes   594 Mbits/sec  1035             
tt15.log:[SUM]   6.00-7.00   sec  69.6 MBytes   584 Mbits/sec  1201             
tt15.log:[SUM]   7.00-8.00   sec  72.6 MBytes   609 Mbits/sec  949             
tt15.log:[SUM]   8.00-9.00   sec  70.2 MBytes   589 Mbits/sec  1055             
tt15.log:[SUM]   9.00-10.00  sec  68.4 MBytes   574 Mbits/sec  1056             
tt15.log:[SUM]   0.00-10.00  sec   714 MBytes   599 Mbits/sec  9694             sender
tt15.log:[SUM]   0.00-10.01  sec   692 MBytes   580 Mbits/sec                  receiver
tt16.log:[SUM]   0.00-1.00   sec  80.4 MBytes   675 Mbits/sec  1039             
tt16.log:[SUM]   1.00-2.00   sec  71.9 MBytes   603 Mbits/sec  1155             
tt16.log:[SUM]   2.00-3.00   sec  69.3 MBytes   581 Mbits/sec  968             
tt16.log:[SUM]   3.00-4.00   sec  68.6 MBytes   575 Mbits/sec  1073             
tt16.log:[SUM]   4.00-5.00   sec  67.3 MBytes   564 Mbits/sec  1047             
tt16.log:[SUM]   5.00-6.00   sec  66.6 MBytes   558 Mbits/sec  1087             
tt16.log:[SUM]   6.00-7.00   sec  69.1 MBytes   580 Mbits/sec  1026             
tt16.log:[SUM]   7.00-8.00   sec  69.1 MBytes   580 Mbits/sec  1031             
tt16.log:[SUM]   8.00-9.00   sec  66.9 MBytes   561 Mbits/sec  902             
tt16.log:[SUM]   9.00-10.00  sec  65.4 MBytes   549 Mbits/sec  1093             
tt16.log:[SUM]   0.00-10.00  sec   695 MBytes   583 Mbits/sec  10421             sender
tt16.log:[SUM]   0.00-10.00  sec   674 MBytes   565 Mbits/sec                  receiver
tt2.log:[SUM]   0.00-1.00   sec  89.6 MBytes   752 Mbits/sec  1067             
tt2.log:[SUM]   1.00-2.00   sec  67.2 MBytes   564 Mbits/sec  974             
tt2.log:[SUM]   2.00-3.00   sec  66.5 MBytes   558 Mbits/sec  854             
tt2.log:[SUM]   3.00-4.00   sec  66.4 MBytes   557 Mbits/sec  946             
tt2.log:[SUM]   4.00-5.00   sec  64.8 MBytes   543 Mbits/sec  1220             
tt2.log:[SUM]   5.00-6.00   sec  65.4 MBytes   548 Mbits/sec  1158             
tt2.log:[SUM]   6.00-7.00   sec  63.6 MBytes   534 Mbits/sec  962             
tt2.log:[SUM]   7.00-8.00   sec  65.2 MBytes   547 Mbits/sec  1120             
tt2.log:[SUM]   8.00-9.00   sec  66.0 MBytes   554 Mbits/sec  866             
tt2.log:[SUM]   9.00-10.00  sec  62.9 MBytes   527 Mbits/sec  934             
tt2.log:[SUM]   0.00-10.00  sec   678 MBytes   568 Mbits/sec  10101             sender
tt2.log:[SUM]   0.00-10.06  sec   659 MBytes   550 Mbits/sec                  receiver
tt3.log:[SUM]   0.00-1.00   sec  82.2 MBytes   689 Mbits/sec  1026             
tt3.log:[SUM]   1.00-2.00   sec  69.7 MBytes   584 Mbits/sec  1025             
tt3.log:[SUM]   2.00-3.00   sec  70.0 MBytes   587 Mbits/sec  935             
tt3.log:[SUM]   3.00-4.00   sec  70.7 MBytes   593 Mbits/sec  980             
tt3.log:[SUM]   4.00-5.00   sec  69.4 MBytes   583 Mbits/sec  1261             
tt3.log:[SUM]   5.00-6.00   sec  70.8 MBytes   594 Mbits/sec  1112             
tt3.log:[SUM]   6.00-7.00   sec  71.5 MBytes   600 Mbits/sec  858             
tt3.log:[SUM]   7.00-8.00   sec  73.0 MBytes   613 Mbits/sec  1290             
tt3.log:[SUM]   8.00-9.00   sec  73.8 MBytes   619 Mbits/sec  931             
tt3.log:[SUM]   9.00-10.00  sec  74.7 MBytes   626 Mbits/sec  975             
tt3.log:[SUM]   0.00-10.00  sec   726 MBytes   609 Mbits/sec  10393             sender
tt3.log:[SUM]   0.00-10.10  sec   711 MBytes   591 Mbits/sec                  receiver
tt4.log:[SUM]   0.00-1.00   sec  79.6 MBytes   668 Mbits/sec  959             
tt4.log:[SUM]   1.00-2.00   sec  69.9 MBytes   587 Mbits/sec  962             
tt4.log:[SUM]   2.00-3.00   sec  69.0 MBytes   579 Mbits/sec  926             
tt4.log:[SUM]   3.00-4.00   sec  67.7 MBytes   568 Mbits/sec  876             
tt4.log:[SUM]   4.00-5.00   sec  69.4 MBytes   583 Mbits/sec  1233             
tt4.log:[SUM]   5.00-6.00   sec  68.6 MBytes   576 Mbits/sec  1292             
tt4.log:[SUM]   6.00-7.00   sec  67.7 MBytes   568 Mbits/sec  910             
tt4.log:[SUM]   7.00-8.00   sec  69.9 MBytes   586 Mbits/sec  1306             
tt4.log:[SUM]   8.00-9.00   sec  69.7 MBytes   585 Mbits/sec  1027             
tt4.log:[SUM]   9.00-10.00  sec  69.0 MBytes   579 Mbits/sec  1086             
tt4.log:[SUM]   0.00-10.00  sec   701 MBytes   588 Mbits/sec  10577             sender
tt4.log:[SUM]   0.00-10.00  sec   679 MBytes   570 Mbits/sec                  receiver
tt5.log:[SUM]   0.00-1.00   sec  79.0 MBytes   662 Mbits/sec  951             
tt5.log:[SUM]   1.00-2.00   sec  71.0 MBytes   596 Mbits/sec  1000             
tt5.log:[SUM]   2.00-3.00   sec  68.6 MBytes   575 Mbits/sec  871             
tt5.log:[SUM]   3.00-4.00   sec  66.7 MBytes   559 Mbits/sec  1038             
tt5.log:[SUM]   4.00-5.00   sec  66.5 MBytes   558 Mbits/sec  1093             
tt5.log:[SUM]   5.00-6.00   sec  67.6 MBytes   567 Mbits/sec  1336             
tt5.log:[SUM]   6.00-7.00   sec  66.7 MBytes   559 Mbits/sec  1135             
tt5.log:[SUM]   7.00-8.00   sec  66.1 MBytes   555 Mbits/sec  1214             
tt5.log:[SUM]   8.00-9.00   sec  66.2 MBytes   555 Mbits/sec  915             
tt5.log:[SUM]   9.00-10.00  sec  68.4 MBytes   574 Mbits/sec  733             
tt5.log:[SUM]   0.00-10.00  sec   687 MBytes   576 Mbits/sec  10286             sender
tt5.log:[SUM]   0.00-10.10  sec   672 MBytes   558 Mbits/sec                  receiver
tt6.log:[SUM]   0.00-1.00   sec  77.9 MBytes   653 Mbits/sec  1030             
tt6.log:[SUM]   1.00-2.00   sec  72.5 MBytes   608 Mbits/sec  908             
tt6.log:[SUM]   2.00-3.00   sec  70.1 MBytes   588 Mbits/sec  861             
tt6.log:[SUM]   3.00-4.00   sec  68.6 MBytes   576 Mbits/sec  919             
tt6.log:[SUM]   4.00-5.00   sec  69.9 MBytes   586 Mbits/sec  1124             
tt6.log:[SUM]   5.00-6.00   sec  69.2 MBytes   581 Mbits/sec  1144             
tt6.log:[SUM]   6.00-7.00   sec  68.8 MBytes   577 Mbits/sec  1156             
tt6.log:[SUM]   7.00-8.00   sec  69.6 MBytes   584 Mbits/sec  1046             
tt6.log:[SUM]   8.00-9.00   sec  69.7 MBytes   585 Mbits/sec  914             
tt6.log:[SUM]   9.00-10.00  sec  68.7 MBytes   577 Mbits/sec  659             
tt6.log:[SUM]   0.00-10.00  sec   705 MBytes   591 Mbits/sec  9761             sender
tt6.log:[SUM]   0.00-10.12  sec   692 MBytes   574 Mbits/sec                  receiver
tt7.log:[SUM]   0.00-1.00   sec  74.5 MBytes   625 Mbits/sec  933             
tt7.log:[SUM]   1.00-2.00   sec  76.2 MBytes   639 Mbits/sec  916             
tt7.log:[SUM]   2.00-3.00   sec  74.6 MBytes   626 Mbits/sec  880             
tt7.log:[SUM]   3.00-4.00   sec  72.7 MBytes   610 Mbits/sec  920             
tt7.log:[SUM]   4.00-5.00   sec  71.1 MBytes   596 Mbits/sec  1160             
tt7.log:[SUM]   5.00-6.00   sec  73.3 MBytes   614 Mbits/sec  1200             
tt7.log:[SUM]   6.00-7.00   sec  73.1 MBytes   614 Mbits/sec  1065             
tt7.log:[SUM]   7.00-8.00   sec  72.0 MBytes   604 Mbits/sec  1219             
tt7.log:[SUM]   8.00-9.00   sec  72.1 MBytes   605 Mbits/sec  999             
tt7.log:[SUM]   9.00-10.00  sec  73.2 MBytes   614 Mbits/sec  1071             
tt7.log:[SUM]   0.00-10.00  sec   733 MBytes   615 Mbits/sec  10363             sender
tt7.log:[SUM]   0.00-10.12  sec   719 MBytes   596 Mbits/sec                  receiver
tt8.log:[SUM]   0.00-1.00   sec  79.4 MBytes   666 Mbits/sec  876             
tt8.log:[SUM]   1.00-2.00   sec  70.1 MBytes   588 Mbits/sec  1051             
tt8.log:[SUM]   2.00-3.00   sec  70.3 MBytes   590 Mbits/sec  800             
tt8.log:[SUM]   3.00-4.00   sec  69.4 MBytes   582 Mbits/sec  1062             
tt8.log:[SUM]   4.00-5.00   sec  70.1 MBytes   588 Mbits/sec  985             
tt8.log:[SUM]   5.00-6.00   sec  68.7 MBytes   576 Mbits/sec  1317             
tt8.log:[SUM]   6.00-7.00   sec  69.5 MBytes   583 Mbits/sec  1107             
tt8.log:[SUM]   7.00-8.00   sec  69.5 MBytes   583 Mbits/sec  967             
tt8.log:[SUM]   8.00-9.00   sec  68.8 MBytes   577 Mbits/sec  1041             
tt8.log:[SUM]   9.00-10.00  sec  68.2 MBytes   572 Mbits/sec  838             
tt8.log:[SUM]   0.00-10.00  sec   704 MBytes   591 Mbits/sec  10044             sender
tt8.log:[SUM]   0.00-10.10  sec   689 MBytes   573 Mbits/sec                  receiver
tt9.log:[SUM]   0.00-1.00   sec  80.2 MBytes   673 Mbits/sec  1032             
tt9.log:[SUM]   1.00-2.00   sec  73.9 MBytes   620 Mbits/sec  1006             
tt9.log:[SUM]   2.00-3.00   sec  71.7 MBytes   601 Mbits/sec  936             
tt9.log:[SUM]   3.00-4.00   sec  72.6 MBytes   609 Mbits/sec  939             
tt9.log:[SUM]   4.00-5.00   sec  72.5 MBytes   608 Mbits/sec  1059             
tt9.log:[SUM]   5.00-6.00   sec  73.2 MBytes   614 Mbits/sec  1457             
tt9.log:[SUM]   6.00-7.00   sec  73.1 MBytes   613 Mbits/sec  869             
tt9.log:[SUM]   7.00-8.00   sec  74.2 MBytes   623 Mbits/sec  1090             
tt9.log:[SUM]   8.00-9.00   sec  74.7 MBytes   627 Mbits/sec  819             
tt9.log:[SUM]   9.00-10.00  sec  74.9 MBytes   628 Mbits/sec  940             
tt9.log:[SUM]   0.00-10.00  sec   741 MBytes   622 Mbits/sec  10147             sender
tt9.log:[SUM]   0.00-10.05  sec   722 MBytes   602 Mbits/sec                  receiver
root@iperf3-clients-4cqqc:/# 
</code></pre>
<p><strong>Result:</strong></p>
<p>The concurrent number of iperf3 tool 1~2000 has no effect on the result, and they are all close to the bandwidth limit of a single 10G network card.</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2021-08-09 17:06</p>
<p>Download an iperf3 image</p>
<p>networkstatic/iperf3 from docker hub - Docker Image | Docker Hub</p>
<pre><code>docker run -it --rm -p 5201:5201 networkstatic/iperf3 --help
</code></pre>
<p>Start the server</p>
<pre><code>docker run  -it --rm --name=iperf3-server -p 5201:5201 networkstatic/iperf3 -s
-----------------------------------------------------------
Server listening on 5201
-----------------------------------------------------------
</code></pre>
<p>Get the ip of the server</p>
<pre><code>docker inspect --format &quot;{{ .NetworkSettings.IPAddress }}&quot; iperf3-server
(Returned) 172.17.0.163
</code></pre>
<p>Start the client</p>
<pre><code class="language-bash">docker run  -it --rm networkstatic/iperf3 -c 172.17.0.163
</code></pre>
<pre><code>Connecting to host 172.17.0.163, port 5201
[  4] local 172.17.0.191 port 51148 connected to 172.17.0.163 port 5201
[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd
[  4]   0.00-1.00   sec  4.16 GBytes  35.7 Gbits/sec    0    468 KBytes
[  4]   1.00-2.00   sec  4.10 GBytes  35.2 Gbits/sec    0    632 KBytes
[  4]   2.00-3.00   sec  4.28 GBytes  36.8 Gbits/sec    0   1.02 MBytes
[  4]   3.00-4.00   sec  4.25 GBytes  36.5 Gbits/sec    0   1.28 MBytes
[  4]   4.00-5.00   sec  4.20 GBytes  36.0 Gbits/sec    0   1.37 MBytes
[  4]   5.00-6.00   sec  4.23 GBytes  36.3 Gbits/sec    0   1.40 MBytes
[  4]   6.00-7.00   sec  4.17 GBytes  35.8 Gbits/sec    0   1.40 MBytes
[  4]   7.00-8.00   sec  4.14 GBytes  35.6 Gbits/sec    0   1.40 MBytes
[  4]   8.00-9.00   sec  4.29 GBytes  36.8 Gbits/sec    0   1.64 MBytes
[  4]   9.00-10.00  sec  4.15 GBytes  35.7 Gbits/sec    0   1.68 MBytes
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bandwidth       Retr
[  4]   0.00-10.00  sec  42.0 GBytes  36.1 Gbits/sec    0             sender
[  4]   0.00-10.00  sec  42.0 GBytes  36.0 Gbits/sec                  receiver

iperf Done.
</code></pre>
<p>You can also perform the above two steps in one step</p>
<pre><code>docker run  -it --rm networkstatic/iperf3 -c $(docker inspect --format &quot;{{ .NetworkSettings.IPAddress }}&quot; $(docker ps -ql))
</code></pre>
<p>There is an open source project on github to test the performance of Kubernetes Service: https://github.com/Pharb/kubernetes-iperf3</p>
<pre><code># cd kubernetes-iperf3/
# tree
.
├── LICENSE
├── README.md
├── iperf3.sh
├── iperf3.yaml
├── network-policy.yaml
└── steps
    ├── cleanup.sh
    ├── run.sh
    └── setup.sh
</code></pre>
<p>iper3.yaml defines a deployment, a server pod, a service corresponding to a deployment, and a set of DaemonSets (client pods distributed on each node)</p>
<pre><code># cat iper3.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
name: iperf3-server-deployment
labels:
    app: iperf3-server
spec:
replicas: 1
selector:
    matchLabels:
    app: iperf3-server
template:
    metadata:
    labels:
        app: iperf3-server
    spec:
    affinity:
        nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
            preference:
            matchExpressions:
            - key: kubernetes.io/role
                operator: In
                values:
                - master
    #tolerations:
    #  - key: node-role.kubernetes.io/master
    #    operator: Exists
    #    effect: NoSchedule
    containers:
    - name: iperf3-server
        image: registry.paas/cmss/iperf3
        args: ['-s']
        ports:
        - containerPort: 5201
        name: server
    terminationGracePeriodSeconds: 0


---


apiVersion: v1
kind: Service
metadata:
name: iperf3-server
spec:
selector:
    app: iperf3-server
ports:
- protocol: TCP
    port: 5201
    targetPort: server
    
---
    
apiVersion: apps/v1
kind: DaemonSet
metadata:
name: iperf3-clients
labels:
    app: iperf3-client
spec:
selector:
    matchLabels:
    app: iperf3-client
template:
    metadata:
    labels:
        app: iperf3-client
    spec:
    #tolerations:
    #  - key: node-role.kubernetes.io/master
    #    operator: Exists
    #    effect: NoSchedule
    containers:
    - name: iperf3-client
        image: registry.paas/cmss/iperf3
        command: ['/bin/sh', '-c', 'sleep infinity']
        # To benchmark manually: kubectl exec iperf3-clients-jlfxq -- /bin/sh -c 'iperf3 -c iperf3-server'
    terminationGracePeriodSeconds: 0
</code></pre>
<pre><code class="language-bash">#  cat iperf3.sh
#!/usr/bin/env bash
set -eu

cd $(dirname $0)

steps/setup.sh
steps/run.sh $@
steps/cleanup.sh
</code></pre>
<pre><code class="language-bash">#  cat steps/cleanup.sh 
#!/usr/bin/env bash
set -eu


kubectl delete --cascade -f iperf3.yaml
</code></pre>
<pre><code class="language-bash">#  cat steps/setup.sh 
#!/usr/bin/env bash
set -eu


kubectl create -f iperf3.yaml

until $(kubectl get pods -l app=iperf3-server -o jsonpath='{.items[0].status.containerStatuses[0].ready}'); do
    echo &quot;Waiting for iperf3 server to start...&quot;
    sleep 5
done


echo &quot;Server is running&quot;
echo
</code></pre>
<pre><code class="language-bash">#  cat steps/run.sh 
#!/usr/bin/env bash
set -eu


CLIENTS=$(kubectl get pods -l app=iperf3-client -o name | cut -d'/' -f2)

for POD in ${CLIENTS}; do
    until $(kubectl get pod ${POD} -o jsonpath='{.status.containerStatuses[0].ready}'); do
        echo &quot;Waiting for ${POD} to start...&quot;
        sleep 5
    done

    HOST=$(kubectl get pod ${POD} -o jsonpath='{.status.hostIP}')
    echo ${POD}
    echo kubectl exec -it ${POD} -- iperf3 -c iperf3-server -T &quot;Client on ${HOST}&quot; $@

    kubectl exec -it ${POD} -- iperf3 -c iperf3-server -T &quot;Client on ${HOST}&quot; $@
    echo
done
</code></pre>
<p>Test log:</p>
<pre><code class="language-bash"># ./iperf3.sh -t 5 -p 5201 -M 2000 -P 125 -b 0 &gt;tt.log
# cat tt.log
deployment.apps/iperf3-server-deployment created
service/iperf3-server created
daemonset.apps/iperf3-clients created
Waiting for iperf3 server to start...
Server is running

iperf3-clients-d6n5s
kubectl exec -it iperf3-clients-d6n5s -- iperf3 -c iperf3-server -T Client on 192.168.10.3 -t 5 -p 5201 -M 2000 -P 125 -b 0
Client on 192.168.10.3:  Connecting to host iperf3-server, port 5201
Client on 192.168.10.3:  [  5] local 10.222.47.101 port 47900 connected to 10.233.32.233 port 5201
Client on 192.168.10.3:  [  7] local 10.222.47.101 port 47902 connected to 10.233.32.233 port 5201
Client on 192.168.10.3:  [  9] local 10.222.47.101 port 47904 connected to 10.233.32.233 port 5201
...(省略)
Client on 192.168.10.25:  [251]   0.00-5.00   sec  45.4 MBytes  76.1 Mbits/sec                  receiver
Client on 192.168.10.25:  [253]   0.00-5.00   sec  14.2 MBytes  23.8 Mbits/sec  415             sender
Client on 192.168.10.25:  [253]   0.00-5.00   sec  14.1 MBytes  23.6 Mbits/sec                  receiver
Client on 192.168.10.25:  [SUM]   0.00-5.00   sec  5.35 GBytes  9.20 Gbits/sec  62899             sender
Client on 192.168.10.25:  [SUM]   0.00-5.00   sec  5.29 GBytes  9.09 Gbits/sec                  receiver
Client on 192.168.10.25:  
Client on 192.168.10.25:  iperf Done.

deployment.apps &quot;iperf3-server-deployment&quot; deleted
service &quot;iperf3-server&quot; deleted
daemonset.apps &quot;iperf3-clients&quot; deleted
</code></pre>
<p>View the full log content above tt.log</p>
<pre><code>#  grep SUM tt.log 
Client on 192.168.10.3:  [SUM]   0.00-1.00   sec  1.11 GBytes  9.52 Gbits/sec  15808             
Client on 192.168.10.3:  [SUM]   1.00-2.00   sec  1.06 GBytes  9.13 Gbits/sec  11143             
Client on 192.168.10.3:  [SUM]   2.00-3.00   sec  1.07 GBytes  9.15 Gbits/sec  11298             
Client on 192.168.10.3:  [SUM]   3.00-4.00   sec  1.06 GBytes  9.14 Gbits/sec  10923             
Client on 192.168.10.3:  [SUM]   4.00-5.00   sec  1.07 GBytes  9.18 Gbits/sec  9166             
Client on 192.168.10.3:  [SUM]   0.00-5.00   sec  5.37 GBytes  9.22 Gbits/sec  58338             sender
Client on 192.168.10.3:  [SUM]   0.00-5.04   sec  5.35 GBytes  9.12 Gbits/sec                  receiver
Client on 192.168.10.9:  [SUM]   0.00-1.06   sec  3.04 GBytes  24.7 Gbits/sec   97             
Client on 192.168.10.9:  [SUM]   1.06-2.03   sec  2.59 GBytes  22.9 Gbits/sec    0             
Client on 192.168.10.9:  [SUM]   2.03-3.03   sec  3.02 GBytes  25.9 Gbits/sec   12             
Client on 192.168.10.9:  [SUM]   3.03-4.01   sec  2.56 GBytes  22.3 Gbits/sec  293             
Client on 192.168.10.9:  [SUM]   4.01-5.05   sec  3.05 GBytes  25.3 Gbits/sec  212             
Client on 192.168.10.9:  [SUM]   0.00-5.05   sec  14.3 GBytes  24.3 Gbits/sec  614             sender
Client on 192.168.10.9:  [SUM]   0.00-5.05   sec  14.2 GBytes  24.2 Gbits/sec                  receiver
Client on 192.168.10.5:  [SUM]   0.00-1.00   sec  1.11 GBytes  9.57 Gbits/sec  14776             
Client on 192.168.10.5:  [SUM]   1.00-2.00   sec  1.07 GBytes  9.16 Gbits/sec  12169             
Client on 192.168.10.5:  [SUM]   2.00-3.00   sec  1.07 GBytes  9.16 Gbits/sec  10104             
Client on 192.168.10.5:  [SUM]   3.00-4.00   sec  1.06 GBytes  9.13 Gbits/sec  10532             
Client on 192.168.10.5:  [SUM]   4.00-5.00   sec  1.06 GBytes  9.14 Gbits/sec  10034             
Client on 192.168.10.5:  [SUM]   0.00-5.00   sec  5.37 GBytes  9.23 Gbits/sec  57615             sender
Client on 192.168.10.5:  [SUM]   0.00-5.02   sec  5.33 GBytes  9.12 Gbits/sec                  receiver
Client on 192.168.10.25:  [SUM]   0.00-1.00   sec  1.11 GBytes  9.52 Gbits/sec  16004             
Client on 192.168.10.25:  [SUM]   1.00-2.00   sec  1.06 GBytes  9.11 Gbits/sec  13149             
Client on 192.168.10.25:  [SUM]   2.00-3.00   sec  1.06 GBytes  9.13 Gbits/sec  11389             
Client on 192.168.10.25:  [SUM]   3.00-4.00   sec  1.06 GBytes  9.11 Gbits/sec  11071             
Client on 192.168.10.25:  [SUM]   4.00-5.00   sec  1.06 GBytes  9.13 Gbits/sec  11286             
Client on 192.168.10.25:  [SUM]   0.00-5.00   sec  5.35 GBytes  9.20 Gbits/sec  62899             sender
Client on 192.168.10.25:  [SUM]   0.00-5.00   sec  5.29 GBytes  9.09 Gbits/sec                  receiver
</code></pre>
<p>Result:</p>
<ol>
<li>The concurrency number of the iperf3 tool is 1 to 125, which has no effect on the results, and is close to the bandwidth limit of a single 10G network card</li>
<li>If the client pod and server pod are in the same node, the data will not go out of the node, and the transmission bandwidth has nothing to do with the network card</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2020-05-27 17:32</p>
<h1 id="when-the-pod-is-created-specify-a-dir-on-the-host-of-the-working-node-host-to-assign-to-the-pod"><a class="header" href="#when-the-pod-is-created-specify-a-dir-on-the-host-of-the-working-node-host-to-assign-to-the-pod">When the POD is created, specify a dir on the host of the working node host to assign to the POD</a></h1>
<pre><code># cat cpu-ram-rc.yaml
apiVersion: v1
kind: ReplicationController
metadata:
name: cpu-ram-demo
spec:
replicas: 1
selector:
    name: cpu-ram-demo
template:
    metadata:
    labels:
        name: cpu-ram-demo
    spec:
    containers:
    - name: cpu-ram-demo-container
        image: registry.paas/library/nginx:1.15.9
        resources:
        requests:
            memory: &quot;200Mi&quot;
            cpu: &quot;2&quot;
        limits:
            memory: &quot;200Mi&quot;
            cpu: &quot;2&quot;
        volumeMounts:
        - mountPath: /etc/work
            name: host5490
    volumes:
    - hostPath:
            path: /docker_temp
        name: host5490
# kubectl create -f cpu-ram-rc.yaml
replicationcontroller/cpu-ram-demo created
# kubectl get pod|grep cpu-ram-demo
cpu-ram-demo-qmxf4                          1/1     Running            0          48s

# kubectl exec -it cpu-ram-demo-qmxf4 bash
root@cpu-ram-demo-qmxf4:/# cd /etc/work/
root@cpu-ram-demo-qmxf4:/etc/work# ls
root@cpu-ram-demo-qmxf4:/etc/work#

# cd /docker_temp/
# ls
#

# Create a new file `from_docker` in the corresponding directory of the container, which can be viewed in the corresponding directory on the host; create `from_host` in the corresponding directory of the host, and can be viewed in the corresponding directory of the container.
root@cpu-ram-demo-qmxf4:/etc/work# touch from_docker
root@cpu-ram-demo-qmxf4:/etc/work# ls
from_docker
root@cpu-ram-demo-qmxf4:/etc/work#

# ls
from_docker
# touch from_host
# ls
from_docker  from_host
#

root@cpu-ram-demo-qmxf4:/etc/work# ls
from_docker  from_host
root@cpu-ram-demo-qmxf4:/etc/work#
</code></pre>
<p>If a failure occurs, the POD will be reborn to other working nodes, and the contents originally mounted in the directory of the POD will be lost.</p>
<h1 id="configure-the-storage-volume-type-as-emptydir-when-creating-a-pod"><a class="header" href="#configure-the-storage-volume-type-as-emptydir-when-creating-a-pod">Configure the storage volume type as emptyDir when creating a POD</a></h1>
<pre><code># cat cpu-ram-rc-emptydir.yaml
apiVersion: v1
kind: ReplicationController
metadata:
name: cpu-ram-demo
spec:
replicas: 1
selector:
    name: cpu-ram-demo
template:
    metadata:
    labels:
        name: cpu-ram-demo
    spec:
    containers:
    - name: cpu-ram-demo-container
        image: registry.paas/library/nginx:1.15.9
        resources:
        requests:
            memory: &quot;200Mi&quot;
            cpu: &quot;2&quot;
        limits:
            memory: &quot;200Mi&quot;
            cpu: &quot;2&quot;
        volumeMounts:
        - mountPath: /etc/work
            name: host5490
    volumes:
    - name: host5490
        emptyDir: {}
# kubectl create -f cpu-ram-rc-emptydir.yaml
replicationcontroller/cpu-ram-demo created
# kubectl get pod|grep cpu-ram
cpu-ram-demo-h8cwf                          1/1     Running            0          32s
# kubectl describe pod cpu-ram-demo-h8cwf
Name:               cpu-ram-demo-h8cwf
Namespace:          default
Priority:           0
PriorityClassName:  &lt;none&gt;
Node:               paasn4/39.135.102.125
Start Time:         Wed, 27 May 2020 21:08:57 +0800
Labels:             name=cpu-ram-demo
Annotations:        &lt;none&gt;
Status:             Running
IP:                 10.222.90.218
Controlled By:      ReplicationController/cpu-ram-demo
Containers:
cpu-ram-demo-container:
    Container ID:   docker://0ffc339f7961b71c5db778cbbe7f738c78fdd5b57013e2b43a3ab2d488f463f7
    Image:          registry.paas/library/nginx:1.15.9
    Image ID:       docker-pullable://registry.paas/library/nginx@sha256:082b7224e857035c61eb4a802bfccf8392736953f78a99096acc7e3296739889
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
    Started:      Wed, 27 May 2020 21:08:59 +0800
    Ready:          True
    Restart Count:  0
    Limits:
    cpu:     2
    memory:  200Mi
    Requests:
    cpu:        2
    memory:     200Mi
    Environment:  &lt;none&gt;
    Mounts:
    /etc/work from host5490 (rw)
    /var/run/secrets/kubernetes.io/serviceaccount from default-token-vc2bx (ro)
Conditions:
Type              Status
Initialized       True
Ready             True
ContainersReady   True
PodScheduled      True
Volumes:
host5490:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
default-token-vc2bx:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vc2bx
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                node.kubernetes.io/unreachable:NoExecute for 300s
Events:
Type    Reason     Age        From               Message
----    ------     ----       ----               -------
Normal  Scheduled  68s        default-scheduler  Successfully assigned default/cpu-ram-demo-h8cwf to paasn4
Normal  Pulled     &lt;invalid&gt;  kubelet, paasn4    Container image &quot;registry.paas/library/nginx:1.15.9&quot; already present on machine
Normal  Created    &lt;invalid&gt;  kubelet, paasn4    Created container
Normal  Started    &lt;invalid&gt;  kubelet, paasn4    Started container
</code></pre>
<p>If a failure occurs, the POD will be reborn to other working nodes, and the contents originally mounted in the directory of the POD will be lost.</p>
<h1 id="configure-sercret-when-pod-is-created"><a class="header" href="#configure-sercret-when-pod-is-created">Configure sercret when POD is created</a></h1>
<pre><code># echo -n &quot;1f2d1e2e67df&quot; &gt; ./password.txt
# kubectl create secret generic db-user-pass --from-file=./password.txt
secret/db-user-pass created
# cat cpu-ram-rc-secret.yaml
apiVersion: v1
kind: ReplicationController
metadata:
name: cpu-ram-demo
spec:
replicas: 1
selector:
    name: cpu-ram-demo
template:
    metadata:
    labels:
        name: cpu-ram-demo
    spec:
    containers:
    - name: cpu-ram-demo-container
        image: registry.paas/library/nginx:1.15.9
        resources:
        requests:
            memory: &quot;200Mi&quot;
            cpu: &quot;2&quot;
        limits:
            memory: &quot;200Mi&quot;
            cpu: &quot;2&quot;
        volumeMounts:
        - name: foo
        mountPath: /etc/foo
        readOnly: true
    volumes:
    - name: foo
        secret:
        secretName: db-user-pass
# kubectl create -f cpu-ram-rc-secret.yaml
replicationcontroller/cpu-ram-demo created
# kubectl get pod|grep cpu-ram
cpu-ram-demo-bdnpz                          1/1     Running            0          27s
# kubectl exec -it cpu-ram-demo-bdnpz bash
root@cpu-ram-demo-bdnpz:/# cd /etc/foo/
root@cpu-ram-demo-bdnpz:/etc/foo# ls
password.txt
root@cpu-ram-demo-bdnpz:/etc/foo# cat password.txt
1f2d1e2e67dfroot@cpu-ram-demo-bdnpz:/etc/foo#
</code></pre>
<p>If a failure occurs, the content in the POD directory of the POD regenerated to other working nodes password.txtwill not be lost.</p>
<h1 id="configure-configmap-when-pod-is-created"><a class="header" href="#configure-configmap-when-pod-is-created">Configure configmap when POD is created</a></h1>
<pre><code># kubectl create configmap special-config --from-literal=special.how=very
configmap/special-config created
# cat cpu-ram-rc-configmap.yaml
apiVersion: v1
kind: ReplicationController
metadata:
name: cpu-ram-demo
spec:
replicas: 1
selector:
    name: cpu-ram-demo
template:
    metadata:
    labels:
        name: cpu-ram-demo
    spec:
    containers:
    - name: cpu-ram-demo-container
        image: registry.paas/library/nginx:1.15.9
        env:
        - name: SPECIAL_LEVEL_KEY
            valueFrom:
            configMapKeyRef:
                name: special-config
                key: special.how
        resources:
        requests:
            memory: &quot;200Mi&quot;
            cpu: &quot;2&quot;
        limits:
            memory: &quot;200Mi&quot;
            cpu: &quot;2&quot;
# kubectl create -f cpu-ram-rc-configmap.yaml
replicationcontroller/cpu-ram-demo created
# kubectl get pod|grep cpu-ram
cpu-ram-demo-lhwlc                          1/1     Running            0          36s
# kubectl exec -it cpu-ram-demo-lhwlc bash
root@cpu-ram-demo-lhwlc:/# env|grep very
SPECIAL_LEVEL_KEY=very
root@cpu-ram-demo-lhwlc:/#
</code></pre>
<p>If a failure occurs, the POD will not be lost if envthe POD is reborn to another working node .SPECIAL_LEVEL_KEY=very</p>
<h1 id="pod-automatic-expansion-and-contraction"><a class="header" href="#pod-automatic-expansion-and-contraction">pod automatic expansion and contraction</a></h1>
<pre><code># Create deployment nginx-test and service nginx-test, service exposes port 80
# kubectl run nginx-test --image=registry.paas/library/nginx:1.15.9 --requests=cpu=20m --expose --port=80
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
service/nginx-test created
deployment.apps/nginx-test created
# Create hHPA (horizontalpodautoscaler), the number of pods ranges from 1 to 10, once the cpu occupancy exceeds 5%. It will automatically expand the capacity, if the cpu is lower than 5%, it will automatically shrink the capacity
[root@paasm1 ~]# kubectl autoscale deployment nginx-test --cpu-percent=5 --min=1 --max=10
horizontalpodautoscaler.autoscaling/nginx-test autoscaled
[root@paasm1 ~]# kubectl get service
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.233.0.1     &lt;none&gt;        443/TCP   26d
nginx-test   ClusterIP   10.233.28.40   &lt;none&gt;        80/TCP    23s
# Before pressurization
# kubectl get hpa
NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx-test   Deployment/nginx-test   0%/5%     1         10        1          104s
# Log in to any node of the cluster to pressurize nginx-service
# while true;do curl 10.233.28.40 &gt;/dev/null 2&gt;&amp;1;done
# After pressurizing for a period of time, the number of pods changes from 1 to 10
# kubectl get hpa
NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx-test   Deployment/nginx-test   14%/5%    1         10        10         5m28s
# kubectl get pod
NAME                          READY   STATUS    RESTARTS   AGE
nginx-test-7745fc4958-94msx   1/1     Running   0          3m59s
nginx-test-7745fc4958-9fvzj   1/1     Running   0          3m43s
nginx-test-7745fc4958-bfzqt   1/1     Running   0          3m59s
nginx-test-7745fc4958-bp2hf   1/1     Running   0          4m14s
nginx-test-7745fc4958-ff7sm   1/1     Running   0          3m59s
nginx-test-7745fc4958-fsdq5   1/1     Running   0          3m59s
nginx-test-7745fc4958-mtwkb   1/1     Running   0          3m43s
nginx-test-7745fc4958-n9kgc   1/1     Running   0          6m13s
nginx-test-7745fc4958-pzwlg   1/1     Running   0          4m14s
nginx-test-7745fc4958-x5sw9   1/1     Running   0          4m14s
# Stop the pressurization operation, the number of pods changed from 10 to 1
# kubectl get hpa
NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx-test   Deployment/nginx-test   0%/5%     1         10        1          12m
# kubectl get pod
NAME                          READY   STATUS    RESTARTS   AGE
nginx-test-7745fc4958-n9kgc   1/1     Running   0          22m
</code></pre>
<h1 id="upgrade-and-rollback"><a class="header" href="#upgrade-and-rollback">Upgrade and Rollback</a></h1>
<pre><code># cat pod-30.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: nginx-test
#  annotations:
#    io.kubernetes.cri.untrusted-workload: &quot;true&quot;
spec:
replicas: 30
template:
    metadata:
        labels:
            app: nginx
    spec:
        containers:
            - name: nginx
            image: registry.paas/library/nginx:1.13
# Create deployments/nginx-test with 30 pods and the version of nginx is 1.13
# kubectl create -f pod-30.yaml
deployment.extensions/nginx-test created
# kubectl get deployment nginx-test
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
nginx-test   0/30    30           0           5s
# Creating...
# kubectl get deployment nginx-test
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
nginx-test   7/30    30           7           9s
# Created
# kubectl get deployment nginx-test
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
nginx-test   30/30   30           30          19s
# kubectl get pod|grep nginx-test
nginx-test-c6fcf8b85-2mhk8                  1/1     Running            0          43s
nginx-test-c6fcf8b85-4c62c                  1/1     Running            0          43s
nginx-test-c6fcf8b85-6kx9p                  1/1     Running            0          43s
nginx-test-c6fcf8b85-78nc6                  1/1     Running            0          43s
nginx-test-c6fcf8b85-79vfr                  1/1     Running            0          43s
nginx-test-c6fcf8b85-8l7dc                  1/1     Running            0          43s
nginx-test-c6fcf8b85-9n7dh                  1/1     Running            0          43s
nginx-test-c6fcf8b85-bcpj9                  1/1     Running            0          43s
nginx-test-c6fcf8b85-c9p4h                  1/1     Running            0          43s
nginx-test-c6fcf8b85-dp8rg                  1/1     Running            0          43s
nginx-test-c6fcf8b85-dsll8                  1/1     Running            0          43s
nginx-test-c6fcf8b85-fqcpb                  1/1     Running            0          43s
nginx-test-c6fcf8b85-fsh8h                  1/1     Running            0          43s
nginx-test-c6fcf8b85-h6v5l                  1/1     Running            0          43s
nginx-test-c6fcf8b85-jmtjr                  1/1     Running            0          43s
nginx-test-c6fcf8b85-krpj9                  1/1     Running            0          43s
nginx-test-c6fcf8b85-lhdf7                  1/1     Running            0          43s
nginx-test-c6fcf8b85-lqzgj                  1/1     Running            0          43s
nginx-test-c6fcf8b85-m6kfp                  1/1     Running            0          43s
nginx-test-c6fcf8b85-n8ccf                  1/1     Running            0          43s
nginx-test-c6fcf8b85-r4wrb                  1/1     Running            0          43s
nginx-test-c6fcf8b85-r72qk                  1/1     Running            0          43s
nginx-test-c6fcf8b85-rrd7l                  1/1     Running            0          43s
nginx-test-c6fcf8b85-shvmg                  1/1     Running            0          43s
nginx-test-c6fcf8b85-tdhz7                  1/1     Running            0          43s
nginx-test-c6fcf8b85-tlqcv                  1/1     Running            0          43s
nginx-test-c6fcf8b85-vkrf5                  1/1     Running            0          43s
nginx-test-c6fcf8b85-vwdtc                  1/1     Running            0          43s
nginx-test-c6fcf8b85-wdwcc                  1/1     Running            0          43s
nginx-test-c6fcf8b85-zf4nv                  1/1     Running            0          43s
# kubectl describe deployment nginx-test
Name:                   nginx-test
Namespace:              default
CreationTimestamp:      Tue, 09 Jun 2020 17:35:32 +0800
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=nginx
Replicas:               30 desired | 30 updated | 30 total | 30 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
Labels:  app=nginx
Containers:
nginx:
    Image:        registry.paas/library/nginx:1.13
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
Volumes:        &lt;none&gt;
Conditions:
Type           Status  Reason
----           ------  ------
Available      True    MinimumReplicasAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-test-c6fcf8b85 (30/30 replicas created)
Events:
Type    Reason             Age   From                   Message
----    ------             ----  ----                   -------
Normal  ScalingReplicaSet  61s   deployment-controller  Scaled up replica set nginx-test-c6fcf8b85 to 30
# Execute the upgrade command
# kubectl set image deployments nginx-test nginx=registry.paas/library/nginx:1.15.9
deployment.extensions/nginx-test image updated
# upgrade......
# kubectl get deployments/nginx-test
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
nginx-test   29/30   24           29          2m4s
# The upgrade is complete, and the nginx versions of 30 pods are all upgraded to 1.15.9
# kubectl get deployments/nginx-test
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
nginx-test   30/30   30           30          2m25s
# kubectl describe deployment nginx-test
Name:                   nginx-test
Namespace:              default
CreationTimestamp:      Tue, 09 Jun 2020 17:35:32 +0800
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=nginx
Replicas:               30 desired | 30 updated | 30 total | 30 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
Labels:  app=nginx
Containers:
nginx:
    Image:        registry.paas/library/nginx:1.15.9
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
Volumes:        &lt;none&gt;
Conditions:
Type           Status  Reason
----           ------  ------
Available      True    MinimumReplicasAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-test-6db76bdf4d (30/30 replicas created)
Events:
Type    Reason             Age                 From                   Message
----    ------             ----                ----                   -------
Normal  ScalingReplicaSet  2m20s               deployment-controller  Scaled up replica set nginx-test-c6fcf8b85 to 30
Normal  ScalingReplicaSet  48s                 deployment-controller  Scaled up replica set nginx-test-6db76bdf4d to 1
Normal  ScalingReplicaSet  48s                 deployment-controller  Scaled down replica set nginx-test-c6fcf8b85 to 29
Normal  ScalingReplicaSet  48s                 deployment-controller  Scaled up replica set nginx-test-6db76bdf4d to 2
Normal  ScalingReplicaSet  47s                 deployment-controller  Scaled down replica set nginx-test-c6fcf8b85 to 28
Normal  ScalingReplicaSet  47s                 deployment-controller  Scaled up replica set nginx-test-6db76bdf4d to 3
Normal  ScalingReplicaSet  46s                 deployment-controller  Scaled down replica set nginx-test-c6fcf8b85 to 27
Normal  ScalingReplicaSet  46s                 deployment-controller  Scaled up replica set nginx-test-6db76bdf4d to 4
Normal  ScalingReplicaSet  44s                 deployment-controller  Scaled down replica set nginx-test-c6fcf8b85 to 26
Normal  ScalingReplicaSet  35s (x16 over 44s)  deployment-controller  (combined from similar events): Scaled down replica set nginx-test-c6fcf8b85 to 18
# perform rollback
# kubectl rollout undo deployments nginx-test
deployment.extensions/nginx-test rolled back
# Rolling back...
# kubectl get deployments/nginx-test
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
nginx-test   29/30   14           29          3m53s
# kubectl describe deployment nginx-test
Name:                   nginx-test
Namespace:              default
CreationTimestamp:      Tue, 09 Jun 2020 17:35:32 +0800
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 3
Selector:               app=nginx
Replicas:               30 desired | 15 updated | 31 total | 29 available | 2 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
Labels:  app=nginx
Containers:
nginx:
    Image:        registry.paas/library/nginx:1.13
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
Volumes:        &lt;none&gt;
Conditions:
Type           Status  Reason
----           ------  ------
Available      True    MinimumReplicasAvailable
OldReplicaSets:  nginx-test-6db76bdf4d (15/15 replicas created)
NewReplicaSet:   nginx-test-c6fcf8b85 (16/16 replicas created)
Events:
Type    Reason             Age                     From                   Message
----    ------             ----                    ----                   -------
Normal  ScalingReplicaSet  3m56s                   deployment-controller  Scaled up replica set nginx-test-c6fcf8b85 to 30
Normal  ScalingReplicaSet  2m24s                   deployment-controller  Scaled up replica set nginx-test-6db76bdf4d to 1
Normal  ScalingReplicaSet  2m24s                   deployment-controller  Scaled down replica set nginx-test-c6fcf8b85 to 29
Normal  ScalingReplicaSet  2m24s                   deployment-controller  Scaled up replica set nginx-test-6db76bdf4d to 2
Normal  ScalingReplicaSet  2m23s                   deployment-controller  Scaled down replica set nginx-test-c6fcf8b85 to 28
Normal  ScalingReplicaSet  2m23s                   deployment-controller  Scaled up replica set nginx-test-6db76bdf4d to 3
Normal  ScalingReplicaSet  2m22s                   deployment-controller  Scaled down replica set nginx-test-c6fcf8b85 to 27
Normal  ScalingReplicaSet  2m22s                   deployment-controller  Scaled up replica set nginx-test-6db76bdf4d to 4
Normal  ScalingReplicaSet  2m20s                   deployment-controller  Scaled down replica set nginx-test-c6fcf8b85 to 26
Normal  ScalingReplicaSet  2m11s (x16 over 2m20s)  deployment-controller  (combined from similar events): Scaled down replica set nginx-test-c6fcf8b85 to 18
回滚完成，30个pod的nginx版本都回滚到1.13
# kubectl get deployments/nginx-test
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
nginx-test   30/30   30           30          4m35s
# kubectl describe deployment nginx-test
Name:                   nginx-test
Namespace:              default
CreationTimestamp:      Tue, 09 Jun 2020 17:35:32 +0800
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 3
Selector:               app=nginx
Replicas:               30 desired | 30 updated | 30 total | 30 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
Labels:  app=nginx
Containers:
nginx:
    Image:        registry.paas/library/nginx:1.13
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
Volumes:        &lt;none&gt;
Conditions:
Type           Status  Reason
----           ------  ------
Available      True    MinimumReplicasAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-test-c6fcf8b85 (30/30 replicas created)
Events:
Type    Reason             Age                    From                   Message
----    ------             ----                   ----                   -------
Normal  ScalingReplicaSet  4m37s                  deployment-controller  Scaled up replica set nginx-test-c6fcf8b85 to 30
Normal  ScalingReplicaSet  3m5s                   deployment-controller  Scaled up replica set nginx-test-6db76bdf4d to 1
Normal  ScalingReplicaSet  3m5s                   deployment-controller  Scaled down replica set nginx-test-c6fcf8b85 to 29
Normal  ScalingReplicaSet  3m5s                   deployment-controller  Scaled up replica set nginx-test-6db76bdf4d to 2
Normal  ScalingReplicaSet  3m4s                   deployment-controller  Scaled down replica set nginx-test-c6fcf8b85 to 28
Normal  ScalingReplicaSet  3m4s                   deployment-controller  Scaled up replica set nginx-test-6db76bdf4d to 3
Normal  ScalingReplicaSet  3m3s                   deployment-controller  Scaled down replica set nginx-test-c6fcf8b85 to 27
Normal  ScalingReplicaSet  3m3s                   deployment-controller  Scaled up replica set nginx-test-6db76bdf4d to 4
Normal  ScalingReplicaSet  3m1s                   deployment-controller  Scaled down replica set nginx-test-c6fcf8b85 to 26
Normal  ScalingReplicaSet  2m52s (x16 over 3m1s)  deployment-controller  (combined from similar events): Scaled down replica set nginx-test-c6fcf8b85 to 18
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2020-05-22 16:24</p>
<p>Find the Pid corresponding to the application, and check that it is not bound to a fixed core, but randomly selected in a cpu pool.</p>
<pre><code>[root@paasm1 ~]# cat cpu-ram.yaml
apiVersion: v1
kind: Pod
metadata:
name: cpu-ram-demo
spec:
containers:
- name: cpu-ram-demo-container
    image: registry.paas/library/nginx:1.15.9
    resources:
    requests:
        memory: &quot;200Mi&quot;
        cpu: &quot;2&quot;
    limits:
        memory: &quot;200Mi&quot;
        cpu: &quot;2&quot;
    volumeMounts:
    - mountPath: /etc/work
        name: host5490
volumes:
- hostPath:
        path: /docker_temp
    name: host5490
[root@paasm1 ~]# kubectl create -f cpu-ram.yaml

[root@paasn4 ~]# docker ps|grep demo
9fe80681cc9d        8c9ca4d17702                              &quot;nginx -g 'daemon of…&quot;   25 seconds ago      Up 24 seconds                           k8s_cpu-ram-demo-container_cpu-ram-demo_default_380104ec-9bfc-11ea-a000-04bd7053eff0_0
ae41406867d7        registry.paas/cmss/pause-amd64:3.1        &quot;/pause&quot;                 26 seconds ago      Up 25 seconds                           k8s_POD_cpu-ram-demo_default_380104ec-9bfc-11ea-a000-04bd7053eff0_0
[root@paasn4 ~]# docker inspect 9fe80681cc9d|grep Pid
            &quot;Pid&quot;: 118177,
            &quot;PidMode&quot;: &quot;&quot;,
            &quot;PidsLimit&quot;: 0,
[root@paasn4 ~]# taskset -c -p 118177
pid 118177's current affinity list: 0-39
</code></pre>
<p>Enable the container worker node to support the cpu core binding feature:</p>
<pre><code>[root@paasn4 ~]# kubectl delete -f cpu-ram.yaml
[root@paasn4 ~]# kubectl drain paasn4
[root@paasn4 ~]# systemctl stop kubelet
[root@paasn4 ~]# systemctl status kubelet
[root@paasn4 ~]# mv /dcos/kubelet/cpu_manager_state /dcos/kubelet/bak_cpu_manager_state
# Modify the startup parameters of kubelet, add: --cpu-manager-policy=static and --kube-reserved=cpu=1 (reserve some resources for the cpu pool)
[root@paasn4 ~]# vim /etc/systemd/system/kubelet.service
[root@paasn4 ~]# systemctl daemon-reload
[root@paasn4 ~]# systemctl start kubelet
[root@paasn4 ~]# systemctl status kubelet
[root@paasn4 ~]# kubectl uncordon  paasn4
</code></pre>
<p>Find the Pid corresponding to the application and check that it is bound to the fixed core.</p>
<pre><code>[root@paasn4 ~]# docker ps|grep demo
8da22cd45eda        8c9ca4d17702                              &quot;nginx -g 'daemon of…&quot;   23 seconds ago      Up 22 seconds                           k8s_cpu-ram-demo-container_cpu-ram-demo_default_ef1a0d28-9bfc-11ea-a000-04bd7053eff0_0
c81430f7ca4e        registry.paas/cmss/pause-amd64:3.1        &quot;/pause&quot;                 24 seconds ago      Up 22 seconds                           k8s_POD_cpu-ram-demo_default_ef1a0d28-9bfc-11ea-a000-04bd7053eff0_0
[root@paasn4 ~]# docker inspect 8da22cd45eda|grep Pid
            &quot;Pid&quot;: 123203,
            &quot;PidMode&quot;: &quot;&quot;,
            &quot;PidsLimit&quot;: 0,
[root@paasn4 ~]# taskset -c -p 123203
pid 123203's current affinity list: 1,21
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2020-05-22 15:18</p>
<p>Docker Container Capabilities</p>
<p>In the docker run command, we can pass --cap-addand --cap-dropto add to the container linux Capabilities. The Capabilities listed in the table below are added by docker to the container by default, and users can --cap-dropremove one or more of them.</p>
<table><thead><tr><th>Docker’s capabilities</th><th>Linux capabilities</th><th>Capability Description</th></tr></thead><tbody>
<tr><td>SETPCAP</td><td>CAP_SETPCAP</td><td>Modify process capabilities.</td></tr>
<tr><td>MKNOD</td><td>CAP_MKNOD</td><td>Create special files using mknod(2).</td></tr>
<tr><td>AUDIT_WRITE</td><td>CAP_AUDIT_WRITE</td><td>Write records to kernel auditing log.</td></tr>
<tr><td>CHOWN</td><td>CAP_CHOWN</td><td>Make arbitrary changes to file UIDs and GIDs (see chown(2)).</td></tr>
<tr><td>NET_RAW</td><td>CAP_NET_RAW</td><td>Use RAW and PACKET sockets.</td></tr>
<tr><td>DAC_OVERRIDE</td><td>CAP_DAC_OVERRIDE</td><td>Bypass file read, write, and execute permission checks.</td></tr>
<tr><td>FOWNER</td><td>CAP_FOWNER</td><td>Bypass permission checks on operations that normally require the file system UID of the process to match the UID of the file.</td></tr>
<tr><td>FSETID</td><td>CAP_FSETID</td><td>Don’t clear set-user-ID and set-group-ID permission bits when a file is modified.</td></tr>
<tr><td>KILL</td><td>CAP_KILL</td><td>Bypass permission checks for sending signals.</td></tr>
<tr><td>SETGID</td><td>CAP_SETGID</td><td>Make arbitrary manipulations of process GIDs and supplementary GID list.</td></tr>
<tr><td>SETUID</td><td>CAP_SETUID</td><td>Make arbitrary manipulations of process UIDs.</td></tr>
<tr><td>NET_BIND_SERVICE</td><td>CAP_NET_BIND_SERVICE</td><td>Bind a socket to internet domain privileged ports (port numbers less than 1024).</td></tr>
<tr><td>SYS_CHROOT</td><td>CAP_SYS_CHROOT</td><td>Use chroot(2), change root directory.</td></tr>
<tr><td>SETFCAP</td><td>CAP_SETFCAP</td><td>Set file capabilities.</td></tr>
</tbody></table>
<p>The ones listed in the table below Capabilitiesare dockerdeleted by default Capabilities, and users can --cap-addadd one or more of them.</p>
<table><thead><tr><th>Docker’s capabilities</th><th>Linux capabilities</th><th>Capability Description</th></tr></thead><tbody>
<tr><td>SYS_MODULE</td><td>CAP_SYS_MODULE</td><td>Load and unload kernel modules.</td></tr>
<tr><td>SYS_RAWIO</td><td>CAP_SYS_RAWIO</td><td>Perform I/O port operations (iopl(2) and ioperm(2)).</td></tr>
<tr><td>SYS_PACCT</td><td>CAP_SYS_PACCT</td><td>Use acct(2), switch process accounting on or off.</td></tr>
<tr><td>SYS_ADMIN</td><td>CAP_SYS_ADMIN</td><td>Perform a range of system administration operations.</td></tr>
<tr><td>SYS_NICE</td><td>CAP_SYS_NICE</td><td>Raise process nice value (nice(2), setpriority(2)) and change the nice value for arbitrary processes.</td></tr>
<tr><td>SYS_RESOURCE</td><td>CAP_SYS_RESOURCE</td><td>Override resource Limits.</td></tr>
<tr><td>SYS_TIME</td><td>CAP_SYS_TIME</td><td>Set system clock (settimeofday(2), stime(2), adjtimex(2)); set real-time (hardware) clock.</td></tr>
<tr><td>SYS_TTY_CONFIG</td><td>CAP_SYS_TTY_CONFIG</td><td>Use vhangup(2); employ various privileged ioctl(2) operations on virtual terminals.</td></tr>
<tr><td>AUDIT_CONTROL</td><td>CAP_AUDIT_CONTROL</td><td>Enable and disable kernel auditing; change auditing filter rules; retrieve auditing status and filtering rules.</td></tr>
<tr><td>MAC_OVERRIDE</td><td>CAP_MAC_OVERRIDE</td><td>Allow MAC configuration or state changes. Implemented for the Smack LSM.</td></tr>
<tr><td>MAC_ADMIN</td><td>CAP_MAC_ADMIN</td><td>Override Mandatory Access Control (MAC). Implemented for the Smack Linux Security Module (LSM).</td></tr>
<tr><td>NET_ADMIN</td><td>CAP_NET_ADMIN</td><td>Perform various network-related operations.</td></tr>
<tr><td>SYSLOG</td><td>CAP_SYSLOG</td><td>Perform privileged syslog(2) operations.</td></tr>
<tr><td>DAC_READ_SEARCH</td><td>CAP_DAC_READ_SEARCH</td><td>Bypass file read permission checks and directory read and execute permission checks.</td></tr>
<tr><td>LINUX_IMMUTABLE</td><td>CAP_LINUX_IMMUTABLE</td><td>Set the FS_APPEND_FL and FS_IMMUTABLE_FL i-node flags.</td></tr>
<tr><td>NET_BROADCAST</td><td>CAP_NET_BROADCAST</td><td>Make socket broadcasts, and listen to multicasts.</td></tr>
<tr><td>IPC_LOCK</td><td>CAP_IPC_LOCK</td><td>Lock memory (mlock(2), mlockall(2), mmap(2), shmctl(2)).</td></tr>
<tr><td>IPC_OWNER</td><td>CAP_IPC_OWNER</td><td>Bypass permission checks for operations on System V IPC objects.</td></tr>
<tr><td>SYS_PTRACE</td><td>CAP_SYS_PTRACE</td><td>Trace arbitrary processes using ptrace(2).</td></tr>
<tr><td>SYS_BOOT</td><td>CAP_SYS_BOOT</td><td>Use reboot(2) and kexec_load(2), reboot and load a new kernel for later execution.</td></tr>
<tr><td>LEASE</td><td>CAP_LEASE</td><td>Establish leases on arbitrary files (see fcntl(2)).</td></tr>
<tr><td>WAKE_ALARM</td><td>CAP_WAKE_ALARM</td><td>Trigger something that will wake up the system.</td></tr>
<tr><td>BLOCK_SUSPEND</td><td>CAP_BLOCK_SUSPEND</td><td>Employ features that can block system suspend.</td></tr>
</tbody></table>
<p>For example, we can modify the network interface by adding NET_ADMIN Capability to the container. The corresponding docker run command is as follows:</p>
<pre><code>$ docker run -it --rm --cap-add=NET_ADMIN ubuntu:14.04 ip link add dummy0 type dummy
</code></pre>
<p>In Kubernetes' definition of Pod, users can add/drop Capabilitiesadd Pod.spec.containers.sercurityContext.capabilitiesthe Capabilities list to add and the Capabilities list to drop.</p>
<pre><code>[root@paasm1 ~]# cat pause.yaml
apiVersion: v1
kind: Pod
metadata:
name: pause
spec:
containers:
    - name:  pause
    image: registry.paas/cmss/busybox
    command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ]

[root@paasm1 ~]# kubectl create -f pause.yaml
pod/pause created
[root@paasm1 ~]# kubectl get pod
NAME                                        READY   STATUS             RESTARTS   AGE
docker-paasn1                               0/1     Pending            0          19d
independent-jaguar-nginx-647f7998cd-gstrj   0/1     ImagePullBackOff   0          45h
pause                                       1/1     Running            0          27s
r00tf0rm3                                   0/1     ErrImagePull       0          19d
[root@paasm1 ~]# kubectl exec pause -it sh
/ # vi
/ # date
Fri May 22 06:44:13 UTC 2020
/ # date -s 09:09
date: can't set date: Operation not permitted
Fri May 22 09:09:00 UTC 2020
/ # [root@paasm1 ~]#
</code></pre>
<p>Pod pause cannot modify the system time</p>
<pre><code>[root@paasm1 ~]# cat pause1.yaml
apiVersion: v1
kind: Pod
metadata:
name: pause1
spec:
containers:
    - name:  pause1
    image: registry.paas/cmss/busybox
    command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ]
    securityContext:
        capabilities:
        add: [&quot;NET_ADMIN&quot;,&quot;SYS_TIME&quot;]

[root@paasm1 ~]# kubectl create -f pause1.yaml
pod/pause1 created
[root@paasm1 ~]# kubectl get pod
NAME                                        READY   STATUS             RESTARTS   AGE
docker-paasn1                               0/1     Pending            0          19d
independent-jaguar-nginx-647f7998cd-gstrj   0/1     ImagePullBackOff   0          45h
pause                                       1/1     Running            0          7m42s
pause1                                      1/1     Running            0          30s
r00tf0rm3                                   0/1     ErrImagePull       0          19d
[root@paasm1 ~]# kubectl exec pause1 -it sh
/ # date
Fri May 22 01:13:12 UTC 2020
/ # date -s 20:20
Fri May 22 20:20:00 UTC 2020
/ # date
Fri May 22 20:20:01 UTC 2020
/ # [root@paasm1 ~]#
</code></pre>
<p>Pod pause1 can modify the system time.</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2020-05-22 13:52</p>
<p>Run the increased server memory stress test script</p>
<pre><code>#!/bin/bash
mkdir /tmp/skyfans/memory -p
mount -t tmpfs -o size=10240M tmpfs /tmp/skyfans/memory
dd if=/dev/zero of=/tmp/skyfans/memory/block
sleep 60
rm /tmp/skyfans/memory/block
umount /tmp/skyfans/memory
rmdir /tmp/skyfans/memory
</code></pre>
<p>It is available on the vm and the server, but an error is reported when the mount is executed in the container: mount: permission denied
needs to enable privileged.
Privileged was introduced into docker around version 0.6.
With this parameter, the root inside the container has real root authority.
Otherwise, the root inside the container is just an ordinary user outside the authority.
The container started by privileged can see many devices on the host and can execute mount.
Even allows launching docker containers inside docker containers.</p>
<p>docker solution: add –privileged when docker run starts the container, such as: docker run -v /home/liurizhou/temp:/home/liurizhou/temp –privileged my_images:latest /bin.bash</p>
<p>k8s solution: add in containers: on securityContext: privileged: true runAsUser: 0</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: mem-nginx
spec:
  containers:
  - name: mem-nginx-container
    image: registry.paas/library/nginx:1.15.9
    resources:
      requests:
        memory: &quot;200Mi&quot;
        cpu: &quot;2&quot;
      limits:
        memory: &quot;200Mi&quot;
        cpu: &quot;2&quot;
    volumeMounts:
      - mountPath: /etc/work
        name: host5490
    securityContext:
      privileged: true
      runAsUser: 0
  volumes:
  - hostPath:
        path: /docker_temp
    name: host5490
  nodeSelector:
    disktype: abc
</code></pre>
<p>Then execute the memory pressurization script, and mount can be executed.
As long as the node has enough memory resources, the container can use more memory than it requested, but the container is not allowed to use resources beyond its limit. If a container allocates more memory than the limit, the container will be terminated first. If the container continues to use more memory than the limit, the container will be terminated. If a terminated container is allowed to be restarted, the kubelet will restart the container.
For example, the upper limit in the above yaml file is 200M. After the memory pressure exceeds 200M, the pod will trigger OOMKilled to be terminated, and a new pod will be created.</p>
<pre><code>[root@paasm1 ~]# kubectl describe pod mem-nginx
Name:               mem-nginx
Namespace:          default
Priority:           0
PriorityClassName:  &lt;none&gt;
Node:               paasn3/39.135.102.123
Start Time:         Thu, 21 May 2020 18:08:50 +0800
Labels:             &lt;none&gt;
Annotations:        &lt;none&gt;
Status:             Running
IP:                 10.222.72.25
Containers:
mem-nginx-container:
    Container ID:   docker://e103444b362d34076290839d0c1ab593e2a79f17d8b373c787f1cb5ab2960942
    Image:          registry.paas/library/nginx:1.15.9
    Image ID:       docker-pullable://registry.paas/library/nginx@sha256:082b7224e857035c61eb4a802bfccf8392736953f78a99096acc7e3296739889
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
    Started:      Fri, 22 May 2020 11:20:13 +0800
    Last State:     Terminated
    Reason:       OOMKilled
    Exit Code:    0
    Started:      Fri, 22 May 2020 11:12:10 +0800
    Finished:     Fri, 22 May 2020 11:20:11 +0800
    Ready:          True
    Restart Count:  2
    Limits:
    cpu:     2
    memory:  200Mi
    Requests:
    cpu:        2
    memory:     200Mi
    Environment:  &lt;none&gt;
    Mounts:
    /etc/work from host5490 (rw)
    /var/run/secrets/kubernetes.io/serviceaccount from default-token-vc2bx (ro)
Conditions:
Type              Status
Initialized       True
Ready             True
ContainersReady   True
PodScheduled      True
Volumes:
host5490:
    Type:          HostPath (bare host directory volume)
    Path:          /docker_temp
    HostPathType:
default-token-vc2bx:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vc2bx
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  disktype=abc
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                node.kubernetes.io/unreachable:NoExecute for 300s
Events:
Type    Reason          Age                 From             Message
----    ------          ----                ----             -------
Normal  SandboxChanged  2m3s (x2 over 10m)  kubelet, paasn3  Pod sandbox changed, it will be killed and re-created.
Normal  Killing         2m2s (x2 over 10m)  kubelet, paasn3  Killing container with id docker://mem-nginx-container:Need to kill Pod
Normal  Pulled          2m (x3 over 17h)    kubelet, paasn3  Container image &quot;registry.paas/library/nginx:1.15.9&quot; already present on machine
Normal  Created         2m (x3 over 17h)    kubelet, paasn3  Created container
Normal  Started         2m (x3 over 17h)    kubelet, paasn3  Started container
</code></pre>
<p>docker ps to view the container and found that k8s_mem-nginx-container_mem-nginx_default_116a11b8-9b4b-11ea-a000-04bd7053eff0_1it has becomek8s_mem-nginx-container_mem-nginx_default_116a11b8-9b4b-11ea-a000-04bd7053eff0_2</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="summary-1"><a class="header" href="#summary-1">Summary</a></h1>
<ul>
<li>
<p><a href="./SUMMARY.html">Catalog: Cloud Native</a></p>
<ul>
<li>
<p><a href="./cloud-native/deploy-kubevirt.html">Deploy Kubernetes + KubeVirt and the basic use of KubeVirt</a> ------ release time :2022-05-06 14:17</p>
</li>
<li>
<p><a href="./cloud-native/docker-java-demo.html">docker hello-world project</a> ------ release time :2022-05-02 10:13</p>
</li>
<li>
<p><a href="./cloud-native/deploy-cinder-csi-plugin.html">Several problems encountered in deploying cinder-csi-plugin</a> ------ release time :2022-04-24 11:00</p>
</li>
<li>
<p><a href="./cloud-native/go1.18-workspace.html">New in Go 1.18 - Workspaces</a> ------ release time :2022-04-13 20:17</p>
</li>
<li>
<p><a href="./cloud-native/spring-boot-async-demo.html">spring-boot asynchronous example</a> ------ release time :2022-03-25 11:24</p>
</li>
<li>
<p><a href="./cloud-native/plg.html">PLG implements Kubernetes Pod log collection and display</a> ------ release time :2021-12-01 17:48</p>
</li>
<li>
<p><a href="./cloud-native/fluentd.html">Fluentd implements Kubernetes Pod log collection</a> ------ release time :2021-11-30 16:36</p>
</li>
<li>
<p><a href="./cloud-native/java-dev-memo-1.html">Little records in Java project development (1)</a> ------ release time :2021-11-23 15:04</p>
</li>
<li>
<p><a href="./cloud-native/jdk-bug.html">JDK bug - Encrypt Private Key failed unrecognized algorithm name PBEWithSHA1AndDESede</a> ------ release time :2021-11-23 14:22</p>
</li>
<li>
<p><a href="./cloud-native/tenant-cmp.html">Implementation of the function of creating resource pool tenants on the cloud management platform</a> ------ release time :2021-09-13 14:22</p>
</li>
<li>
<p><a href="./cloud-native/test-service-performance-2.html">iperf3 tests the four-layer performance of Kubernetes Service (Part 2)</a> ------ release time :2021-08-10 07:06</p>
</li>
<li>
<p><a href="./cloud-native/test-service-performance-1.html">iperf3 tests the four-layer performance of Kubernetes Service (Part 1)</a> ------ release time :2021-08-09 17:06</p>
</li>
<li>
<p><a href="./cloud-native/k8s-cmd.html">k8s common command</a> ------ release time :2020-05-27 17:32</p>
</li>
<li>
<p><a href="./cloud-native/cpu-binding.html">k8s supports container core binding</a> ------ release time :2020-05-22 16:24</p>
</li>
<li>
<p><a href="./cloud-native/capability.html">k8s supports Capability mechanism</a> ------ release time :2020-05-22 15:18</p>
</li>
<li>
<p><a href="./cloud-native/oom.html">k8s OOMkilled container exceeding memory limit</a> ------ release time :2020-05-22 13:52</p>
</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Openstack</a></p>
<ul>
<li>
<p><a href="./openstack/deploy-openstack-stein.html">Problems encountered in Openstack Stein deployment</a> ------ release time :2019-09-20 18:49</p>
</li>
<li>
<p><a href="./openstack/sriov2ovs.html">sriov computing node to ovs computing node script</a> ------ release time :2019-04-19 16:50</p>
</li>
<li>
<p><a href="./openstack/placement.html">Openstack Placement</a> ------ release time :2019-04-18 09:14</p>
</li>
<li>
<p><a href="./openstack/power-arch.html">POWER architecture server as computing node</a> ------ release time :2018-10-21 16:21</p>
</li>
<li>
<p><a href="./openstack/sriov.html">OpenStack practices SR-IOV computing nodes</a> ------ release time :2018-10-14 05:15</p>
</li>
<li>
<p><a href="./openstack/openstack-op-4.html">Openstack operation and maintenance records (4)</a> ------ release time :2020-01-01 19:13</p>
</li>
<li>
<p><a href="./openstack/openstack-op-3.html">Openstack operation and maintenance records (3)</a> ------ release time :2018-09-29 20:28</p>
</li>
<li>
<p><a href="./openstack/openstack-op-2.html">Openstack operation and maintenance records (2)</a> ------ release time :2018-08-28 14:48</p>
</li>
<li>
<p><a href="./openstack/openstack-op-1.html">Openstack operation and maintenance records (1)</a> ------ release time :2018-08-26 18:38</p>
</li>
<li>
<p><a href="./openstack/novnc-problem.html">OpenStack Pike dashboard noVNC cannot be accessed problem</a> ------ release time :2018-04-14 05:18</p>
</li>
<li>
<p><a href="./openstack/openstack-local-yum.html">Openstack Pike local yum source construction</a> ------ release time :2017-12-28 22:20</p>
</li>
<li>
<p><a href="./openstack/cpu-binding.html">CPU binding</a> ------ release time :2017-07-20 18:30</p>
</li>
<li>
<p><a href="./openstack/resize-fail.html">Investigation of the cause of resize failure</a> ------ release time :2017-07-04 10:20</p>
</li>
<li>
<p><a href="./openstack/mount-cloud-disk.html">Mount cloud disk</a> ------ release time :2017-06-30 05:16</p>
</li>
<li>
<p><a href="./openstack/ocata-nova-evacuate-bug.html">Ocata nova evacuate bug</a> ------ release time :2017-06-28 18:24</p>
</li>
<li>
<p><a href="./openstack/compute-node-ha.html">compute node ha mainstream open source implementation</a> ------ release time :2017-06-22 09:16</p>
</li>
<li>
<p><a href="./openstack/collectd-influxdb.html">Deployment and usage of Collectd and InfluxDB</a> ------ release time :2017-06-15 06:47</p>
</li>
<li>
<p><a href="./openstack/live-migration-local.html">Live Migration on Local Storage</a> ------ release time :2017-06-14 15:25</p>
</li>
<li>
<p><a href="./openstack/fast-evacuation.html">Add fast evacuation function to VM HA procedure</a> ------ release time :2017-06-10 03:27</p>
</li>
<li>
<p><a href="./openstack/vm-activation-failure.html">Windows virtual machine activation failure problem</a> ------ release time :2017-05-04 22:21</p>
</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Point of view</a></p>
<ul>
<li><a href="./point-of-view/huawei-and-operators.html">Huawei and telecom operators</a> ------ release time :2017-03-10 21:25</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Open source small project</a></p>
<ul>
<li><a href="./open-source-small-project/firewall-tool.html">Open source a node.js firewall tool</a> ------ release time :2022-05-07 09:41</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Tools</a></p>
<ul>
<li>
<p><a href="./tools/github-action.html">Github Action Supplementary Introduction</a> ------ release time :2022-04-12 13:04</p>
</li>
<li>
<p><a href="./tools/blog-cicd.html">Github Action Upgrade backend cloud website CICD process</a> ------ release time :2022-04-03 00:17</p>
</li>
<li>
<p><a href="./tools/github-action-demo.html">Try the Github Action CI/CD process (create a React project and package it for deployment)</a> ------ release time :2022-02-25 11:54</p>
</li>
<li>
<p><a href="./tools/notion-widget.html">Notion's page insert widget Widget</a> ------ release time :2022-01-06 11:06</p>
</li>
<li>
<p><a href="./tools/git-dir.html">Internal structure of .git directory</a> ------ release time :2023-01-18 18:37</p>
</li>
<li>
<p><a href="./tools/xshell-skill.html">Some Skills of using xshell tools</a> ------ release time :2016-07-05 22:57</p>
</li>
<li>
<p><a href="./tools/gerrit-install.html">gerrit install</a> ------ release time :2017-06-28 15:51</p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2019-09-20 18:49</p>
<p>Stein, the 19th version of OpenStack, supports 5G and edge computing.
OpenStack Stein enhances bare metal and network management performance, and enables faster startup of Kubernetes clusters. It also provides network upgrade functions for edge computing and NFV use cases, and enhances resource management and tracking performance.</p>
<ul>
<li><strong>OS version: CentOS 7.7 ( CentOS Linux release 7.7.1908 )</strong></li>
<li><strong>Openstack version: Stein</strong></li>
</ul>
<h1 id="after-nova-compute-is-deployed-on-the-compute-node-it-gets-stuck-at-startup"><a class="header" href="#after-nova-compute-is-deployed-on-the-compute-node-it-gets-stuck-at-startup">After nova-compute is deployed on the compute node, it gets stuck at startup</a></h1>
<p>View the nova-compute.log log, report message queue error</p>
<pre><code>2019-09-20 13:38:32.411 68483 INFO os_vif [-] Loaded VIF plugins: ovs, linux_bridge, noop
2019-09-20 13:38:32.878 68483 ERROR oslo.messaging._drivers.impl_rabbit [req-b955a570-83bc-4a14-966f-2eefe5a82579 - - - - -] Connection failed: [Errno 113] EHOSTUNREACH (retrying in 2.0 seconds): error: [Errno 113] EHOSTUNREACH
2019-09-20 13:38:34.888 68483 ERROR oslo.messaging._drivers.impl_rabbit [req-b955a570-83bc-4a14-966f-2eefe5a82579 - - - - -] Connection failed: [Errno 113] EHOSTUNREACH (retrying in 4.0 seconds): error: [Errno 113] EHOSTUNREACH
2019-09-20 13:38:38.899 68483 ERROR oslo.messaging._drivers.impl_rabbit [req-b955a570-83bc-4a14-966f-2eefe5a82579 - - - - -] Connection failed: [Errno 113] EHOSTUNREACH (retrying in 6.0 seconds): error: [Errno 113] EHOSTUNREACH
2019-09-20 13:38:44.912 68483 ERROR oslo.messaging._drivers.impl_rabbit [req-b955a570-83bc-4a14-966f-2eefe5a82579 - - - - -] Connection failed: [Errno 113] EHOSTUNREACH (retrying in 8.0 seconds): error: [Errno 113] EHOSTUNREACH
2019-09-20 13:38:52.931 68483 ERROR oslo.messaging._drivers.impl_rabbit [req-b955a570-83bc-4a14-966f-2eefe5a82579 - - - - -] Connection failed: [Errno 113] EHOSTUNREACH (retrying in 10.0 seconds): error: [Errno 113] EHOSTUNREACH
</code></pre>
<p>Check the nova configuration file, the rabbitmq configuration is correct, log in to the controller node, check the nova service log, and there is no message queue error. Compare the configuration of the controller node and the compute node rabbitmq, the same, the controller node does not report an error, and the compute node reports an error. The message queue service is deployed on the controller node. It may be caused by the firewall that the nova service of the compute node cannot access the mq service of the controller node. Sure enough, the firewall is not closed, and the problem is solved after closing it.</p>
<h1 id="after-deploying-nova-compute-on-the-compute-node-execute-nova-service-list-the-service-of-the-compute-node-is-normal-but-the-nova-log-of-the-compute-node-reports-an-error-which-is-related-to-resources-it-seems-to-be-related-to-the-placement-service"><a class="header" href="#after-deploying-nova-compute-on-the-compute-node-execute-nova-service-list-the-service-of-the-compute-node-is-normal-but-the-nova-log-of-the-compute-node-reports-an-error-which-is-related-to-resources-it-seems-to-be-related-to-the-placement-service">After deploying nova-compute on the compute node, execute nova service-list. The service of the compute node is normal, but the nova log of the compute node reports an error, which is related to resources. It seems to be related to the placement service.</a></h1>
<pre><code>2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager [req-128f79a5-8b18-471b-9967-aae3cfde3043 - - - - -] Error updating resources for node compute.: ResourceProviderRetrievalFailed: Failed to get resource provider with UUID 092506db-b8fe-49d3-a962-9182e0025dea
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager Traceback (most recent call last):
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 8148, in _update_available_resource_for_node
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     startup=startup)
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py&quot;, line 744, in update_available_resource
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     self._update_available_resource(context, resources, startup=startup)
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/oslo_concurrency/lockutils.py&quot;, line 328, in inner
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     return f(*args, **kwargs)
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py&quot;, line 825, in _update_available_resource
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     self._update(context, cn, startup=startup)
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py&quot;, line 1032, in _update
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     self._update_to_placement(context, compute_node, startup)
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/retrying.py&quot;, line 68, in wrapped_f
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     return Retrying(*dargs, **dkw).call(f, *args, **kw)
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/retrying.py&quot;, line 223, in call
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     return attempt.get(self._wrap_exception)
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/retrying.py&quot;, line 261, in get
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     six.reraise(self.value[0], self.value[1], self.value[2])
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/retrying.py&quot;, line 217, in call
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py&quot;, line 958, in _update_to_placement
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     context, compute_node.uuid, name=compute_node.hypervisor_hostname)
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/nova/scheduler/client/report.py&quot;, line 873, in get_provider_tree_and_ensure_root
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     parent_provider_uuid=parent_provider_uuid)
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/nova/scheduler/client/report.py&quot;, line 655, in _ensure_resource_provider
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     rps_to_refresh = self._get_providers_in_tree(context, uuid)
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/nova/scheduler/client/report.py&quot;, line 71, in wrapper
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     return f(self, *a, **k)
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager   File &quot;/usr/lib/python2.7/site-packages/nova/scheduler/client/report.py&quot;, line 522, in _get_providers_in_tree
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager     raise exception.ResourceProviderRetrievalFailed(uuid=uuid)
2019-09-20 14:20:24.477 69378 ERROR nova.compute.manager ResourceProviderRetrievalFailed: Failed to get resource provider with UUID 092506db-b8fe-49d3-a962-9182e0025dea
</code></pre>
<p>Search the Internet for this problem, which is related to permissions</p>
<pre><code># vim /etc/httpd/conf.d/00-placement-api.conf
# add
&lt;Directory /usr/bin&gt;
    &lt;IfVersion &gt;= 2.4&gt;
        Require all granted
    &lt;/IfVersion&gt;
    &lt;IfVersion &lt; 2.4&gt;
        Order allow,deny
        Allow from all
    &lt;/IfVersion&gt;
&lt;/Directory&gt;
# su -s /bin/sh -c &quot;placement-manage db sync&quot; placement
# systemctl restart httpd
</code></pre>
<p>After modification, the problem is solved</p>
<pre><code># sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
</code></pre>
<p>Add in /etc/sysctl.conf:</p>
<pre><code>net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
# When executing sysctl -p, it appears:
[root@localhost ~]# sysctl -p
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directory
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory

# Solution:

[root@localhost ~]# modprobe br_netfilter
[root@localhost ~]# sysctl -p
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
</code></pre>
<p>The module fails after restarting. The following is the script to automatically load the module at startup</p>
<p>Create a new rc.sysinit file in /etc/</p>
<pre><code>cat /etc/rc.sysinit

#!/bin/bash
for file in /etc/sysconfig/modules/*.modules ; do
[ -x $file ] &amp;&amp; $file
done

# Create a new file in the /etc/sysconfig/modules/ directory as follows

cat /etc/sysconfig/modules/br_netfilter.modules
modprobe br_netfilter

# add permissions


chmod 755 br_netfilter.modules

# The module is automatically loaded after restart

[root@localhost ~]# lsmod |grep br_netfilter
br_netfilter           22209  0
bridge                136173  1 br_netfilter
</code></pre>
<h1 id="the-openstack-virtual-machine-instance-is-stuck-at-system-boot-and-cannot-start-the-operating-system"><a class="header" href="#the-openstack-virtual-machine-instance-is-stuck-at-system-boot-and-cannot-start-the-operating-system">The openstack virtual machine instance is stuck at system boot and cannot start the operating system</a></h1>
<p>show</p>
<pre><code>booting from hard disk...
GRUB`
</code></pre>
<p>Whether it is the cirros image downloaded from the Internet, or the linux and windows images created by installing and uploading by yourself, it cannot be started. It has been stuck for a while, and then moved to a physical machine. Install linux directly on the bare metal, and then install openstack. Everything is normal, and the virtual machine instance Both start normally, (windows need to install virtio driver).</p>
<p>Going back to solve the problem that the openstack installed on the virtual machine on vmware cannot start the instance operating system, and confirm the solution direction, it is the problem of the virtual disk format and driver. Through the method of virsh edit XXXX, it can be seen that the virtual machine cannot be started Is using the virtio driver</p>
<p>change it to Then start the virtual machine with virsh start XXX, it can be started normally, but soon, the instance is automatically shut down in less than 1 minute, for example, no matter whether you modify virsh edit XXX or modify the virtual machine /etc/libvirt/qemu/instan-00000002.xml The machine definition file is automatically restored to the original configuration file after the instance is started on the openstack interface.</p>
<p>Finally, I found a way to directly modify the parameter properties of the image file and specify the properties of the hard disk and network card:</p>
<pre><code># openstack image set  --property hw_disk_bus=ide  --property hw_vif_model=e1000 &lt;image-id&gt;
</code></pre>
<p>This command modifies the hard disk attribute to ide, and the network card attribute to e1000</p>
<p>Then use this modified image to generate a virtual machine instance, ok, the system can be booted normally, and the virtual hard disk can be recognized.</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2019-04-19 16:50</p>
<p>Because sometimes it is necessary to change the functions of the computing nodes, convert the sriov computing nodes into ovs computing nodes in batches.</p>
<p>Arrange the manually modified commands one by one to form a script, and then use the ansible tool to run the following script in batches to convert the sriov computing node to the ovs computing node.</p>
<pre><code># cat sriov2ovs.bash

# stop physical network card sriov vf
echo '0' &gt; /sys/class/net/ens1f0/device/sriov_numvfs
sed -i '/sriov_numvfs/d' /etc/rc.d/rc.local

# stop and disable neutron-sriov-nic-agent service， because the neutron ovs service is to be enabled
systemctl disable neutron-sriov-nic-agent
systemctl stop neutron-sriov-nic-agent

# Delete nova.conf sriov passthrough whitelist configuration
sed -i '/passthrough_whitelist/d' /etc/nova/nova.conf
systemctl restart openstack-nova-compute

# configure physical network card
# configure bond
(cat &lt;&lt;HERE
TYPE=Ethernet
BOOTPROTO=none
NAME=ens1f0
DEVICE=ens1f0
ONBOOT=yes
MASTER=bond1
SLAVE=yes

HERE
)&gt; /etc/sysconfig/network-scripts/ifcfg-ens1f0

(cat &lt;&lt;HERE
TYPE=Ethernet
BOOTPROTO=none
NAME=ens1f1
DEVICE=ens1f1
ONBOOT=yes
MASTER=bond1
SLAVE=yes

HERE
)&gt; /etc/sysconfig/network-scripts/ifcfg-ens1f1

(cat &lt;&lt;HERE
BOOTPROTO=none
DEVICE=bond1
ONBOOT=yes
## MTU=1500
TYPE=Bond
BONDING_OPTS=&quot;mode=active-backup miimon=100&quot;

HERE
)&gt; /etc/sysconfig/network-scripts/ifcfg-bond1

# Modify the verification configuration of the source address of the data packet
sed -i '/net\.ipv4\.conf\.all\.rp_filter/d' /etc/rc.d/rc.local
sed -i '/net\.ipv4\.conf\.default\.rp_filter/d' /etc/rc.d/rc.local
sed -i '$a\net.ipv4.conf.all.rp_filter=0' /etc/sysctl.conf
sed -i '$a\net.ipv4.conf.default.rp_filter=0' /etc/sysctl.conf
sysctl -p

# instal neutron-openvswitch,etc services
yum install -y openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch openvswitch

# config file openvswitch_agent.ini, configure the mapping from the physical network card to the bridge, and restart the network-related services
# neutron.conf
# /etc/neutron/plugins/ml2/openvswitch_agent.ini
(cat &lt;&lt;HERE
[DEFAULT]

[agent]
extensions = qos

[ovs]
integration_bridge = br-int
bridge_mappings = physnet1:br-prv

[securitygroup]
enable_ipset = true
enable_security_group = true
firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver

HERE
)&gt; /etc/neutron/plugins/ml2/openvswitch_agent.ini

systemctl enable  openvswitch.service
systemctl restart  openvswitch.service
systemctl status  openvswitch.service

# Add the bridge and network card connection that previously configured the mapping of the physical network card to the bridge
ovs-vsctl add-br br-prv
ovs-vsctl add-port br-prv bond1

systemctl enable neutron-openvswitch-agent.service
systemctl restart neutron-openvswitch-agent.service</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2019-04-18 09:14</p>
<p>The main content of this article is transferred from the article of Huawei Fan Guixiong.  </p>
<h1 id="background"><a class="header" href="#background">background</a></h1>
<p>Private cloud users, especially private cloud users undergoing traditional IT architecture transformation, generally have a variety of stock resource systems, and connecting with these systems will complicate the OpenStack resource system.</p>
<p>From the user's point of view, you may want to:</p>
<p>As a user of shared storage solutions, you would expect Nova and Horizon to correctly report the total and usage information of shared storage disk resources.
As an advanced Neutron user, it is expected to use an external third-party routing network function. I hope that Nova can grasp and use a specific network port to associate with a specific subnet pool to ensure that the virtual machine can be started on the subnet pool.
As an advanced Cinder user, I hope that when I specify cinder volume-id in the nova boot command, Nova can know which computing nodes are associated with the Cinder storage pool where the Request Volume is located.
Therefore, in addition to processing internal resources such as computing node CPU, memory, PCI devices, and local disks, OpenStack often needs to manage external resources such as storage services provided by SDS and NFS, and network services provided by SDN.</p>
<p>But in the past, Nova can only handle resources provided by computing nodes. Nova Resource Tracker assumes that all resources come from computing nodes. Therefore, when reporting resource status periodically, Resource Tracker will simply add statistics on the total resources and usage of the computing node list. Obviously, this cannot meet the complex production requirements mentioned above, and also violates the openness principle that OpenStack has always been proud of. And as the definition of OpenStack is further upgraded by the community to &quot;an open source infrastructure integration engine&quot;, it means that the resource system of OpenStack will be composed of more external resource types.</p>
<p>Therefore, when resource types and providers become diverse, a highly abstract and simple and unified management method is naturally required to allow users and codes to easily use, manage, and monitor the entire OpenStack system resources. This is Placement (layout) ).</p>
<h1 id="project-description"><a class="header" href="#project-description">Project Description</a></h1>
<p>Placement shoulders such a historical mission. It was first introduced into the openstack/nova repo in the Newton version and incubated in the form of API, so it is often called Placement API. It participates in the scheduling process of nova-scheduler selecting the target host, is responsible for tracking and recording the Inventory and Usage of the Resource Provider, and uses different Resource Classes to classify resource types, and uses different Resource Traits to mark resource characteristics.</p>
<p>The Ocata version of the Placement API is optional, and users are advised to enable and replace CpuFilter, CoreFilter, and DiskFilter. The Pike version is mandatory to start the Placement API service, otherwise the nova-compute service cannot run normally.</p>
<p>Placement API started the openstack/nova repo spin-off process, transformed from Placement API to OpenStack Placement, and became a separate project in the Stein release.</p>
<h1 id="community-development"><a class="header" href="#community-development">community development</a></h1>
<p>Starting from the S version, Placement has released the first official version 1.0.0. The Placement code is hosted in its own warehouse and managed as an independent OpenStack project.</p>
<p>Most of the changes in the S version are not user-oriented, but the migration of internal data from nova to placement.</p>
<p>The S version has a total of 338 commits, the following is the company's participation.</p>
<p>https://www.stackalytics.com/?module=placement&amp;release=stein&amp;metric=commits</p>
<pre><code>Contribution Summary
Commits: 338
LOCs: 61838
Do not merge (-2): 2
Patch needs further work (-1): 154
Looks good (+1): 313
Looks good for core (+2): 649
Approve: 305
Abandon: 0
Change Requests: 350 (30 of them abandoned)
Patch Sets: 1320
Draft Blueprints: 0
Completed Blueprints: 0
Filed Bugs: 0
Resolved Bugs: 0
Emails: 0
Translations: 0
</code></pre>
<p>#working principle
nova-compute reports resource information to placement. Nova-scheduler still uses partially reserved filters and weights while asking placement for nodes that satisfy a series of resource requests. (Currently placement only replaces the commonly used filters of nova-scheduler's CpuFilter, CoreFilter and DiskFilter)</p>
<p>Nova-scheduler makes two calls to placement-api. For the first time, nova-scheduler obtains a set of Allocation Candidates (allocation candidates) from placement-api. The so-called Allocation Candidates are Resource Providers that can meet resource requirements.</p>
<p>EXAMPLE:</p>
<pre><code>GET /allocation_candidates?resources=VCPU:1,MEMORY_MB:2048,DISK_GB:100
</code></pre>
<p>NOTE: The implementation of obtaining Allocation Candidates is a series of complex database cascading query and filtering operations, using query params as filtering conditions. This example passes the vCPU, RAM, and Disk resources required by the Launch Instance. In addition, the required and member_of parameters can also be provided to specify the Resource Traits and Resource Provider Aggregate characteristics, making the acquisition of Allocation Candidates more flexible. See Allocation candidates for more details.</p>
<pre><code>[root@control01 ~]# openstack allocation candidate list --resource VCPU=1,MEMORY_MB=2048,DISK_GB=10 --required HW_CPU_X86_SSE2
+---+----------------------------------+--------------------------------------+----------------------------------------------+--------------------------------------------------------------+
| # | allocation                       | resource provider                    | inventory used/capacity                      | traits                                                       |
+---+----------------------------------+--------------------------------------+----------------------------------------------+--------------------------------------------------------------+
| 1 | VCPU=1,MEMORY_MB=2048,DISK_GB=10 | 5c5a578f-51b0-481c-b38c-7aaa3394e585 | VCPU=5/512,MEMORY_MB=3648/60670,DISK_GB=7/49 | HW_CPU_X86_SSE2,HW_CPU_X86_SSE,HW_CPU_X86_MMX,HW_CPU_X86_SVM |
+---+----------------------------------+--------------------------------------+----------------------------------------------+--------------------------------------------------------------+
</code></pre>
<p>The JSON object with a list of allocation requests and a JSON object of provider summary objects returned by placement-api to nova-scheduler has the following data structure, the key lies in the two fields of allocation_requests and provider_summaries, which also play an important role in the subsequent Scheduler Filters logic role.</p>
<pre><code>{
&quot;allocation_requests&quot;: [
    &lt;ALLOCATION_REQUEST_1&gt;,
    ...
    &lt;ALLOCATION_REQUEST_N&gt;
],
&quot;provider_summaries&quot;: {
    &lt;COMPUTE_NODE_UUID_1&gt;: &lt;PROVIDER_SUMMARY_1&gt;,
    ...
    &lt;COMPUTE_NODE_UUID_N&gt;: &lt;PROVIDER_SUMMARY_N&gt;,
}
}
</code></pre>
<p>allocation_requests: Contains a list of all resource providers that can meet the requirements and their expected allocation resources.</p>
<pre><code>&quot;allocation_requests&quot;: [
        {
            &quot;allocations&quot;: {
                &quot;a99bad54-a275-4c4f-a8a3-ac00d57e5c64&quot;: {
                    &quot;resources&quot;: {
                        &quot;DISK_GB&quot;: 100
                    }
                },
                &quot;35791f28-fb45-4717-9ea9-435b3ef7c3b3&quot;: {
                    &quot;resources&quot;: {
                        &quot;VCPU&quot;: 1,
                        &quot;MEMORY_MB&quot;: 1024
                    }
                }
            }
        },
        {
            &quot;allocations&quot;: {
                &quot;a99bad54-a275-4c4f-a8a3-ac00d57e5c64&quot;: {
                    &quot;resources&quot;: {
                        &quot;DISK_GB&quot;: 100
                    }
                },
                &quot;915ef8ed-9b91-4e38-8802-2e4224ad54cd&quot;: {
                    &quot;resources&quot;: {
                        &quot;VCPU&quot;: 1,
                        &quot;MEMORY_MB&quot;: 1024
                    }
                }
            }
        }
    ],
</code></pre>
<p>provider_summaries: Contains the total resources and usage information of all resource providers that meet the requirements.</p>
<pre><code>&quot;provider_summaries&quot;: {
        &quot;a99bad54-a275-4c4f-a8a3-ac00d57e5c64&quot;: {
            &quot;resources&quot;: {
                &quot;DISK_GB&quot;: {
                    &quot;used&quot;: 0,
                    &quot;capacity&quot;: 1900
                }
            },
            &quot;traits&quot;: [&quot;MISC_SHARES_VIA_AGGREGATE&quot;],
            &quot;parent_provider_uuid&quot;: null,
            &quot;root_provider_uuid&quot;: &quot;a99bad54-a275-4c4f-a8a3-ac00d57e5c64&quot;
        },
        &quot;35791f28-fb45-4717-9ea9-435b3ef7c3b3&quot;: {
            &quot;resources&quot;: {
                &quot;VCPU&quot;: {
                    &quot;used&quot;: 0,
                    &quot;capacity&quot;: 384
                },
                &quot;MEMORY_MB&quot;: {
                    &quot;used&quot;: 0,
                    &quot;capacity&quot;: 196608
                }
            },
            &quot;traits&quot;: [&quot;HW_CPU_X86_SSE2&quot;, &quot;HW_CPU_X86_AVX2&quot;],
            &quot;parent_provider_uuid&quot;: null,
            &quot;root_provider_uuid&quot;: &quot;35791f28-fb45-4717-9ea9-435b3ef7c3b3&quot;
        },
        &quot;915ef8ed-9b91-4e38-8802-2e4224ad54cd&quot;: {
            &quot;resources&quot;: {
                &quot;VCPU&quot;: {
                    &quot;used&quot;: 0,
                    &quot;capacity&quot;: 384
                },
                &quot;MEMORY_MB&quot;: {
                    &quot;used&quot;: 0,
                    &quot;capacity&quot;: 196608
                }
            },
            &quot;traits&quot;: [&quot;HW_NIC_SRIOV&quot;],
            &quot;parent_provider_uuid&quot;: null,
            &quot;root_provider_uuid&quot;: &quot;915ef8ed-9b91-4e38-8802-2e4224ad54cd&quot;
        },
        &quot;f5120cad-67d9-4f20-9210-3092a79a28cf&quot;: {
            &quot;resources&quot;: {
                &quot;SRIOV_NET_VF&quot;: {
                    &quot;used&quot;: 0,
                    &quot;capacity&quot;: 8
                }
            },
            &quot;traits&quot;: [],
            &quot;parent_provider_uuid&quot;: &quot;915ef8ed-9b91-4e38-8802-2e4224ad54cd&quot;,
            &quot;root_provider_uuid&quot;: &quot;915ef8ed-9b91-4e38-8802-2e4224ad54cd&quot;
        }
    }
</code></pre>
<p>NOTE: It can be seen that SRIOV_NET_VF is also regarded as a resource type, provided by a dedicated resource provider.</p>
<p>After nova-scheduler obtains the Allocation Candidates, it further uses the Filtered and Weighed mechanisms to finally determine the target host. Then deduct (claim_resources) the resource usage of the resource provider corresponding to the target host according to the data of allocation requests and provider summaries, which is what nova-scheduler calls placement-api for the second time. Review the contents of the allocations tables:</p>
<pre><code>MariaDB [nova_api]&gt; select * from allocations;
+---------------------+------------+----+----------------------+--------------------------------------+-------------------+------+
| created_at          | updated_at | id | resource_provider_id | consumer_id                          | resource_class_id | used |
+---------------------+------------+----+----------------------+--------------------------------------+-------------------+------+
| 2018-08-01 10:52:15 | NULL       |  7 |                    1 | f8d55035-389c-47b8-beea-02f00f25f5d9 |                 0 |    1 |
| 2018-08-01 10:52:15 | NULL       |  8 |                    1 | f8d55035-389c-47b8-beea-02f00f25f5d9 |                 1 |  512 |
| 2018-08-01 10:52:15 | NULL       |  9 |                    1 | f8d55035-389c-47b8-beea-02f00f25f5d9 |                 2 |    1 |
+---------------------+------------+----+----------------------+--------------------------------------+-------------------+------+
# consumer_id consumer
# resource_class_id resource type
# resource_provider_id resource provider
# used allocated amount
# The above records indicate that 1 vCPU, RAM 512MB, and Disk 1GB are allocated to the virtual machine
Obviously, the Consumer consumer is the virtual machine to be created.
</code></pre>
<h1 id="software-installation"><a class="header" href="#software-installation">Software Installation</a></h1>
<p>1 Create a database</p>
<p>To create a database, complete the following steps:</p>
<p><strong>Connect to the database server as root using the database access client:</strong></p>
<pre><code>$ mysql -u root -p
</code></pre>
<p><strong>Grant appropriate access to the database:</strong></p>
<pre><code>MariaDB [(none)]&gt; CREATE DATABASE placement;
</code></pre>
<p><strong>Exit the database access client.</strong></p>
<p>2 Configure users and endpoints</p>
<p>1)Source admin credentials to access admin-only CLI commands:</p>
<pre><code>$ .admin-openrc
</code></pre>
<p>2)Create the Placement service user PLACEMENT_PASS with your choice:</p>
<p>$ openstack user create --domain default --password-prompt placement</p>
<pre><code>User Password:
Repeat User Password:
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | default                          |
| enabled             | True                             |
| id                  | fa742015a6494a949f67629884fc7ec8 |
| name                | placement                        |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
</code></pre>
<p>3)Add the Placement user to the service project with the admin role:</p>
<pre><code>$ openstack role add –project service –user placement admin
</code></pre>
<p>4)Create a Placement API entry in the service catalog:</p>
<pre><code>$ openstack service create --name placement \
--description &quot;Placement API&quot; placement

+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | Placement API                    |
| enabled     | True                             |
| id          | 2d1a27022e6e4185b86adac4444c495f |
| name        | placement                        |
| type        | placement                        |
+-------------+----------------------------------+
</code></pre>
<p>5)Create the Placement API service endpoint:</p>
<pre><code>$ openstack endpoint create --region RegionOne \
placement public http://controller:8778

+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 2b1b2637908b4137a9c2e0470487cbc0 |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 2d1a27022e6e4185b86adac4444c495f |
| service_name | placement                        |
| service_type | placement                        |
| url          | http://controller:8778           |
+--------------+----------------------------------+

$ openstack endpoint create --region RegionOne \
placement internal http://controller:8778

+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 02bcda9a150a4bd7993ff4879df971ab |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 2d1a27022e6e4185b86adac4444c495f |
| service_name | placement                        |
| service_type | placement                        |
| url          | http://controller:8778           |
+--------------+----------------------------------+

$ openstack endpoint create --region RegionOne \
placement admin http://controller:8778

+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 3d71177b9e0f406f98cbff198d74b182 |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 2d1a27022e6e4185b86adac4444c495f |
| service_name | placement                        |
| service_type | placement                        |
| url          | http://controller:8778           |
+--------------+----------------------------------+
</code></pre>
<p>3 Install and configure components</p>
<p>1)Installation package:</p>
<pre><code>yum install openstack-placement-api
</code></pre>
<p>Edit the /etc/placement/placement.conf file and complete the following:</p>
<p>2)In the [placement_database] section, configure database access:</p>
<pre><code>[placement_database]

…
connection = mysql+pymysql://placement:PLACEMENT_DBPASS@controller/placement
replace PLACEMENT_DBPASS with the password you chose for the placement database.
</code></pre>
<p>3)In the [api] and [keystone_authtoken] sections, configure the identity service access:</p>
<pre><code>[api]
# ...
auth_strategy = keystone

[keystone_authtoken]
# ...
auth_url = http://controller:5000/v3
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = placement
password = PLACEMENT_PASS
</code></pre>
<p>4 Populate the placement database:</p>
<pre><code>su -s /bin/sh -c &quot;placement-manage db sync&quot; placement
</code></pre>
<p>5 Complete the installation</p>
<p>Restart the httpd service</p>
<pre><code>systemctl restart httpd
</code></pre>
<p>Instructions
1 Resource provider aggregates function</p>
<p>Resource provider aggregates is a function similar to Host Aggregate. When obtaining Allocation Candidates, it supports obtaining from a specific Aggregate through the member_of request query parameter. Resource provider aggregates are very suitable for production scenarios with different types of host aggregation (eg high-performance host aggregation, large storage capacity host aggregation).</p>
<p>Create resource provider aggregates</p>
<pre><code>[root@control01 ~]# openstack aggregate create --zone nova host_aggregate_1
+-------------------+----------------------------+
| Field             | Value                      |
+-------------------+----------------------------+
| availability_zone | nova                       |
| created_at        | 2018-12-08T05:49:55.051678 |
| deleted           | False                      |
| deleted_at        | None                       |
| id                | 1                          |
| name              | host_aggregate_1           |
| updated_at        | None                       |
+-------------------+----------------------------+

[root@control01 ~]# openstack aggregate add host host_aggregate_1 control01
+-------------------+---------------------------------+
| Field             | Value                           |
+-------------------+---------------------------------+
| availability_zone | nova                            |
| created_at        | 2018-12-08T05:49:55.000000      |
| deleted           | False                           |
| deleted_at        | None                            |
| hosts             | [u'control01']                  |
| id                | 1                               |
| metadata          | {u'availability_zone': u'nova'} |
| name              | host_aggregate_1                |
| updated_at        | None                            |
+-------------------+---------------------------------+

[root@control01 ~]# openstack aggregate show host_aggregate_1
+-------------------+----------------------------+
| Field             | Value                      |
+-------------------+----------------------------+
| availability_zone | nova                       |
| created_at        | 2018-12-08T05:49:55.000000 |
| deleted           | False                      |
| deleted_at        | None                       |
| hosts             | [u'control01']             |
| id                | 1                          |
| name              | host_aggregate_1           |
| properties        |                            |
| updated_at        | None                       |
+-------------------+----------------------------+

[root@control01 ~]# openstack resource provider list
+--------------------------------------+-----------+------------+--------------------------------------+----------------------+
| uuid                                 | name      | generation | root_provider_uuid                   | parent_provider_uuid |
+--------------------------------------+-----------+------------+--------------------------------------+----------------------+
| 5c5a578f-51b0-481c-b38c-7aaa3394e585 | control01 |         26 | 5c5a578f-51b0-481c-b38c-7aaa3394e585 | None                 |
+--------------------------------------+-----------+------------+--------------------------------------+----------------------+

[root@control01 ~]# openstack resource provider aggregate list 5c5a578f-51b0-481c-b38c-7aaa3394e585
+--------------------------------------+
| uuid                                 |
+--------------------------------------+
| 5eea7084-0207-44f0-bbeb-c759e8c766a1 |
+--------------------------------------+
</code></pre>
<p>List allocation candidates filter by aggregates</p>
<pre><code># REQ
curl -i &quot;http://172.18.22.222/placement/allocation_candidates?resources=VCPU:1,MEMORY_MB:512,DISK_GB:5&amp;member_of=5eea7084-0207-44f0-bbeb-c759e8c766a1&quot; \
-X GET \
-H 'Content-type: application/json' \
-H 'Accept: application/json' \
-H 'X-Auth-Project-Id: admin' \
-H 'OpenStack-API-Version: placement 1.21' \
-H 'X-Auth-Token:gAAAAABcC12qN3GdLvjYXSSUODi7Dg9jTHUfcnF7I_ljmcffZjs3ignipGLj6iqDvDJ1gXkzGIDW6rRRNcXary-wPfgsb3nCWRIEiAS8LrReI4SYL1KfQiGW7j92b6zTz7RoSEBXACQ9z7UUVfeJ06n8WqVMBaSob4BeFIuHiVKpYCJNv7LR6cI'

# RESP
{
    &quot;provider_summaries&quot;: {
        &quot;5c5a578f-51b0-481c-b38c-7aaa3394e585&quot;: {
            &quot;traits&quot;: [&quot;HW_CPU_X86_SSE2&quot;, &quot;HW_CPU_X86_SSE&quot;, &quot;HW_CPU_X86_MMX&quot;, &quot;HW_CPU_X86_SVM&quot;],
            &quot;resources&quot;: {
                &quot;VCPU&quot;: {
                    &quot;used&quot;: 5,
                    &quot;capacity&quot;: 512
                },
                &quot;MEMORY_MB&quot;: {
                    &quot;used&quot;: 3648,
                    &quot;capacity&quot;: 60670
                },
                &quot;DISK_GB&quot;: {
                    &quot;used&quot;: 7,
                    &quot;capacity&quot;: 49
                }
            }
        }
    },
    &quot;allocation_requests&quot;: [{
        &quot;allocations&quot;: {
            &quot;5c5a578f-51b0-481c-b38c-7aaa3394e585&quot;: {
                &quot;resources&quot;: {
                    &quot;VCPU&quot;: 1,
                    &quot;MEMORY_MB&quot;: 512,
                    &quot;DISK_GB&quot;: 5
                }
            }
        }
    }]
}
</code></pre>
<p>2 Resource traits function</p>
<p>Resource traits feature label function, used to identify the feature properties of Resource Provider, each Resource Provider has its own default traits, and also supports custom traits for the specified Resource Provider.</p>
<p>Resource traits is a very flexible design, similar to the role of &quot;tags&quot;. Users can build a &quot;tag cloud&quot; and decide to put a &quot;tag&quot; on a Resource Provider. It is an auxiliary tool for resource classification requirements.</p>
<pre><code>List traits
curl -i &quot;http://172.18.22.222/placement/traits&quot; \
-X GET \
-H 'Content-type: application/json' \
-H 'Accept: application/json' \
-H 'X-Auth-Project-Id: admin' \
-H 'OpenStack-API-Version: placement 1.21' \
-H 'X-Auth-Token:gAAAAABcC12qN3GdLvjYXSSUODi7Dg9jTHUfcnF7I_ljmcffZjs3ignipGLj6iqDvDJ1gXkzGIDW6rRRNcXary-wPfgsb3nCWRIEiAS8LrReI4SYL1KfQiGW7j92b6zTz7RoSEBXACQ9z7UUVfeJ06n8WqVMBaSob4BeFIuHiVKpYCJNv7LR6cI'

Create custom traits
curl -i &quot;http://172.18.22.222/placement/traits/CUSTOM_FANGUIJU_HOST&quot; \
-X PUT \
-H 'Content-type: application/json' \
-H 'Accept: application/json' \
-H 'X-Auth-Project-Id: admin' \
-H 'OpenStack-API-Version: placement 1.21' \
-H 'X-Auth-Token:gAAAAABcC12qN3GdLvjYXSSUODi7Dg9jTHUfcnF7I_ljmcffZjs3ignipGLj6iqDvDJ1gXkzGIDW6rRRNcXary-wPfgsb3nCWRIEiAS8LrReI4SYL1KfQiGW7j92b6zTz7RoSEBXACQ9z7UUVfeJ06n8WqVMBaSob4BeFIuHiVKpYCJNv7LR6cI'
NOTE：自定义 traits 建议以 CUSTOM_ 开头。

Update all traits for a specific resource provider
curl -i &quot;http://172.18.22.222/placement/resource_providers/5c5a578f-51b0-481c-b38c-7aaa3394e585/traits&quot; \
-X PUT \
-H 'Content-type: application/json' \
-H 'Accept: application/json' \
-H 'X-Auth-Project-Id: admin' \
-H 'OpenStack-API-Version: placement 1.21' \
-H 'X-Auth-Token:gAAAAABcC12qN3GdLvjYXSSUODi7Dg9jTHUfcnF7I_ljmcffZjs3ignipGLj6iqDvDJ1gXkzGIDW6rRRNcXary-wPfgsb3nCWRIEiAS8LrReI4SYL1KfQiGW7j92b6zTz7RoSEBXACQ9z7UUVfeJ06n8WqVMBaSob4BeFIuHiVKpYCJNv7LR6cI' \
-d '{&quot;resource_provider_generation&quot;: 28, &quot;traits&quot;: [&quot;HW_CPU_X86_SSE2&quot;, &quot;HW_CPU_X86_SSE&quot;, &quot;HW_CPU_X86_MMX&quot;, &quot;HW_CPU_X86_SVM&quot;, &quot;CUSTOM_FANGUIJU_HOST&quot;]}'

Return all traits associated with a specific resource provider
[root@control01 ~]# openstack resource provider trait list 5c5a578f-51b0-481c-b38c-7aaa3394e585
+----------------------+
| name                 |
+----------------------+
| HW_CPU_X86_SSE2      |
| HW_CPU_X86_SSE       |
| HW_CPU_X86_MMX       |
| HW_CPU_X86_SVM       |
| CUSTOM_FANGUIJU_HOST |
+----------------------+
</code></pre>
<p>List allocation candidates filter by traits</p>
<pre><code># REQ
curl -i &quot;http://172.18.22.222/placement/allocation_candidates?resources=VCPU:1,MEMORY_MB:512,DISK_GB:5&amp;member_of=5eea7084-0207-44f0-bbeb-c759e8c766a1&amp;required=CUSTOM_FANGUIJU_HOST&quot; \
-X GET \
-H 'Content-type: application/json' \
-H 'Accept: application/json' \
-H 'X-Auth-Project-Id: admin' \
-H 'OpenStack-API-Version: placement 1.21' \
-H 'X-Auth-Token:gAAAAABcC12qN3GdLvjYXSSUODi7Dg9jTHUfcnF7I_ljmcffZjs3ignipGLj6iqDvDJ1gXkzGIDW6rRRNcXary-wPfgsb3nCWRIEiAS8LrReI4SYL1KfQiGW7j92b6zTz7RoSEBXACQ9z7UUVfeJ06n8WqVMBaSob4BeFIuHiVKpYCJNv7LR6cI'

# RESP
{
    &quot;provider_summaries&quot;: {
        &quot;5c5a578f-51b0-481c-b38c-7aaa3394e585&quot;: {
            &quot;traits&quot;: [&quot;HW_CPU_X86_SSE2&quot;, &quot;HW_CPU_X86_SSE&quot;, &quot;HW_CPU_X86_MMX&quot;, &quot;HW_CPU_X86_SVM&quot;, &quot;CUSTOM_FANGUIJU_HOST&quot;],
            &quot;resources&quot;: {
                &quot;VCPU&quot;: {
                    &quot;used&quot;: 5,
                    &quot;capacity&quot;: 512
                },
                &quot;MEMORY_MB&quot;: {
                    &quot;used&quot;: 3648,
                    &quot;capacity&quot;: 60670
                },
                &quot;DISK_GB&quot;: {
                    &quot;used&quot;: 7,
                    &quot;capacity&quot;: 49
                }
            }
        }
    },
    &quot;allocation_requests&quot;: [{
        &quot;allocations&quot;: {
            &quot;5c5a578f-51b0-481c-b38c-7aaa3394e585&quot;: {
                &quot;resources&quot;: {
                    &quot;VCPU&quot;: 1,
                    &quot;MEMORY_MB&quot;: 512,
                    &quot;DISK_GB&quot;: 5
                }
            }
        }
    }]
}
</code></pre>
<h1 id="summarize"><a class="header" href="#summarize">Summarize</a></h1>
<p>The project is still relatively young, and many functions have not been fully implemented. The Stein version has become an independent project for the first time. Although placement only replaces several commonly used filters of nova-scheduler's CpuFilter, CoreFilter and DiskFilter, it is expected that it will eventually replace nova in the future. -scheduler service and can conveniently use, manage, and monitor the system resources of the entire OpenStack.</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2018-10-21 16:21</p>
<h1 id="target"><a class="header" href="#target">Target</a></h1>
<p>Install computing components on the power machine, use the power machine as a computing node, and test the basic functions of Openstack.</p>
<h1 id="environment"><a class="header" href="#environment">environment</a></h1>
<p>1 control node (x86 machine)
5 computing nodes (4 x86 machines, 1 power machine)</p>
<h1 id="problems-with-installing-dependencies"><a class="header" href="#problems-with-installing-dependencies">Problems with installing dependencies</a></h1>
<p>Even if the yum source is configured as the yun source of the power architecture, there are still some dependencies that cannot be found.
Power machine installation components and the dependencies required by the components will encounter various problems that yum dependencies cannot find.</p>
<p>Solution:</p>
<ol>
<li>Manual pip installation for python packages regardless of architecture</li>
<li>It needs to be compiled into Power architecture compiler. Or search online, or compile and install on power.</li>
</ol>
<h1 id="there-is-a-problem-with-the-power-machines-support-for-the-ide"><a class="header" href="#there-is-a-problem-with-the-power-machines-support-for-the-ide">There is a problem with the power machine's support for the IDE</a></h1>
<p>2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f] libvirtError: unsupported configuration: IDE controllers are unsupported for this QEMU machine
binary Time to solve by changing the IDE to virtio</p>
<h1 id="python-script-execution-error"><a class="header" href="#python-script-execution-error">python script execution error</a></h1>
<p>{&quot;message&quot;: &quot;Build of instance 15d9db88-d0a9-40a8-83e9-9ede3001b112 was re-scheduled: 'module' object has no attribute 'to_utf8'&quot;, &quot;code&quot;: 500, &quot;details&quot;: &quot;File &quot;/usr /lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 2258, in _do_build_and_run_instance
will report an error when executed. Some are because the python package installed by pip automatically installs dependencies, and the relationship between versions is disordered, so it needs to be uninstalled Current version, reinstall the required version.</p>
<h1 id="vnc-cant-be-used"><a class="header" href="#vnc-cant-be-used">vnc can't be used</a></h1>
<p>{&quot;message&quot;: &quot;Build of instance a1feb48a-b5f5-48ab-93a7-838bb46573fb was re-scheduled: internal error: process exited while connecting to monitor: 2017-05-18T10:33:34.222333Z qemu-kvm: Cirrus VGA available”, “code”: 500, “details”: “ File &quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 2258, in _do_build_and_run_instance
By modifying the libvirt source code, recompile Solved by the way of installation.</p>
<h1 id="keyboard-input-not-responding"><a class="header" href="#keyboard-input-not-responding">keyboard input not responding</a></h1>
<p>The virtual machine generated with the ubuntu14.04 image does not respond to keyboard input.
There is no such problem when switching to ubuntu16.04. There must be more problems with Centos6 and 7. The official website of the Centos image file is not downloaded.</p>
<h1 id="nova-tests-several-common-operations-on-power-machines"><a class="header" href="#nova-tests-several-common-operations-on-power-machines">Nova tests several common operations on POWER machines</a></h1>
<p>Tested several common operations of Nova on vm life cycle management, such as generation, restart, suspend, resume, shutdown, delete. The test is OK.</p>
<p>Tested to mount and unmount volumes on vm. The test is OK.</p>
<p>Tested Nova's resource statistics on the power machine, and the monitoring of the heartbeat of the nova service on the power machine. The test is OK.</p>
<h1 id="in-conclusion"><a class="header" href="#in-conclusion">in conclusion</a></h1>
<p>Computing components are deployed on power machines. There are many problems, it is difficult to deploy, the operation is too risky, and there are too many uncontrollable factors.</p>
<p>Linux(Centos)+openstack is installed on a power machine, there is no mature solution, or there is no precedent. It costs too much to try and research by yourself. It is professional research and development, and technology is not very likely to go in this direction. Generally, it is left to the community. And Linux (Centos) community fedora, PowerPC is downgraded to a secondary architecture by fedora, after x86 and arm architecture</p>
<p>https://fedoraproject.org/wiki/Architectures/ARM/Planning/Primary</p>
<p>It is assumed that the computing components can run on the power machine intact, but the power architecture machine can only run the power architecture virtual machine, and there are very few customer virtual machines with the power architecture.</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2018-10-14 05:15</p>
<h1 id="sr-iov-function-introduction"><a class="header" href="#sr-iov-function-introduction">SR-IOV function introduction</a></h1>
<p>SR-IOV enables a single functional unit (for example, an Ethernet port) to look like multiple independent physical devices, that is, a physical device supporting the SR-IOV function can be configured as multiple functional units.</p>
<p>SR-IOV has two functions:</p>
<p>Physical Functions (PF): This is a complete PCIe device with SR-IOV capability. PFs can be discovered, managed and configured like normal PCI devices.
Virtual Functions (VF): Simple PCIe functions that can only handle I/O. Each VF is separated from the PF. Each physical hardware has a limit on the number of VFs. A PF can be virtualized into multiple VFs for allocation to multiple virtual machines.
Hypervisor can assign one or more VFs to a virtual machine. At a certain time, a VF can only be assigned to a virtual machine. A virtual machine can have multiple VFs. From the perspective of the operating system of the virtual machine, a VF network card looks the same as a normal network card. The SR-IOV driver is implemented in the kernel.
Example of NIC SR-IOV:</p>
<p><img src="openstack/2023-01-18-17-50-26.png" alt="" /></p>
<h1 id="virtualize-the-pf-of-the-sriov-computing-node-into-multiple-vfs"><a class="header" href="#virtualize-the-pf-of-the-sriov-computing-node-into-multiple-vfs">Virtualize the PF of the sriov computing node into multiple VFs</a></h1>
<p>The sriov computing node selects optical port 1 (eg enp5s0f1) as the sriov network card</p>
<ol>
<li>
<p>On the computing node, set the BIOS to support VT-d for Intel, which can be verified by cat /proc/cpuinfo | grep vmx</p>
</li>
<li>
<p>Configure the /etc/default/grub file of the computing node, add intel_iommu=on to GRUB_CMDLINE_LINUX to activate the VT-d function, and restart the physical machine (intel 82599 series network card is used in this environment)</p>
<p>$ cat /etc/default/grub
GRUB_TIMEOUT=5
GRUB_DISTRIBUTOR=&quot;$(sed 's, release .*$,,g' /etc/system-release)&quot;
GRUB_DEFAULT=saved
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT=&quot;console&quot;
GRUB_CMDLINE_LINUX=&quot;crashkernel=auto rd.lvm.lv=bclinux/root rd.lvm.lv=bclinux/swap intel_iommu=on rhgb quiet&quot;
GRUB_DISABLE_RECOVERY=&quot;true&quot;</p>
<p>$ grub2-mkconfig --output=/boot/grub2/grub.cfg
$ reboot</p>
</li>
<li>
<p>In this environment, the 82599 network card supports a maximum of 64 vfs. It is planned to virtualize 16 vfs on each sriov computing node for use. The command is as follows:</p>
<h1 id="echo-16--sysclassnetenp5s0f1devicesriov_numvfs"><a class="header" href="#echo-16--sysclassnetenp5s0f1devicesriov_numvfs">echo '16' &gt; /sys/class/net/enp5s0f1/device/sriov_numvfs</a></h1>
<h1 id="lspci--grep-ethernet"><a class="header" href="#lspci--grep-ethernet">lspci | grep Ethernet</a></h1>
<p>05:00.0 Ethernet controller [0200]: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection [8086:10fb] (rev 01)
05:00.1 Ethernet controller [0200]: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection [8086:10fb] (rev 01)
05:10.1 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:10.3 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:10.5 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:10.7 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:11.1 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:11.3 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:11.5 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:11.7 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:12.1 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:12.3 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:12.5 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:12.7 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:13.1 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:13.3 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:13.5 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
05:13.7 Ethernet controller [0200]: Intel Corporation 82599 Ethernet Controller Virtual Function [8086:10ed] (rev 01)
08:00.0 Ethernet controller [0200]: Intel Corporation I350 Gigabit Network Connection [8086:1521] (rev 01)
08:00.1 Ethernet controller [0200]: Intel Corporation I350 Gigabit Network Connection [8086:1521] (rev 01)</p>
<h1 id="ip-link-show-enp5s0f1"><a class="header" href="#ip-link-show-enp5s0f1">ip link show enp5s0f1</a></h1>
<p>5: enp5s0f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT qlen 1000
link/ether 6c:92:bf:04:66:01 brd ff:ff:ff:ff:ff:ff
vf 0 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 1 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 2 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 3 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 4 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 5 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 6 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 7 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 8 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 9 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 10 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 11 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 12 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 13 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 14 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off
vf 15 MAC 00:00:00:00:00:00, spoof checking on, link-state auto, trust off, query_rss off</p>
</li>
<li>
<p>To ensure that vfs still exists after restarting, you need to write the command to set the number of vfs into /etc/rc.local, the command is as follows</p>
<h1 id="echo-echo-16--sysclassnetenp5s0f1devicesriov_numvfs--etcrclocal"><a class="header" href="#echo-echo-16--sysclassnetenp5s0f1devicesriov_numvfs--etcrclocal">echo &quot;echo '16' &gt; /sys/class/net/enp5s0f1/device/sriov_numvfs&quot; &gt;&gt; /etc/rc.local</a></h1>
<h1 id="chmod-x-etcrclocal-etcrcdrclocal"><a class="header" href="#chmod-x-etcrclocal-etcrcdrclocal">chmod +x /etc/rc.local /etc/rc.d/rc.local</a></h1>
</li>
<li>
<p>Modify the nova.conf file of the control node to allow nova-schedule to support pci passthrough filtering</p>
<h1 id="vim-etcnovanovaconf"><a class="header" href="#vim-etcnovanovaconf">vim /etc/nova/nova.conf</a></h1>
<p>enable_filters = RetryFilter, AvailabilityZoneFilter, RamFilter, ComputeFilter, ComputeCapabilitiesFilter, ImagePropertiesFilter, ServerGroupAntiAffinityFilter, ServerGroupAffinityFilter, PciPassthroughFilter
available_filters = nova.scheduler.filters.all_filters</p>
<h1 id="systemctl-restart-openstack-nova-scheduler"><a class="header" href="#systemctl-restart-openstack-nova-scheduler">systemctl restart openstack-nova-scheduler</a></h1>
</li>
</ol>
<h1 id="configure-sriov-virtual-machine-computing-nodes"><a class="header" href="#configure-sriov-virtual-machine-computing-nodes">Configure SRIOV virtual machine computing nodes</a></h1>
<ol>
<li>Install neutron-sriov-agent</li>
</ol>
<pre><code class="language-bash">    # yum install openstack-neutron-sriov-nic-agent openstack-neutron
</code></pre>
<ol start="2">
<li>Configure neutron.conf</li>
</ol>
<p>to copy the control node neutron.conf configuration and modify the corresponding parameters, mainly to modify the local management network ip parameters</p>
<ol start="3">
<li>Configure sriov_agent.ini</li>
</ol>
<pre><code class="language-bash">    # vim /etc/neutron/plugins/ml2/sriov_agent.ini

    [securitygroup]
    firewall_driver = neutron.agent.firewall.NoopFirewallDriver

    [sriov_nic]
    physical_device_mappings = default: enp5s0f1
    exclude_devices =
</code></pre>
<ol start="4">
<li>Start the neutron-sriov-agent service</li>
</ol>
<pre><code class="language-bash">    # systemctl enable neutron-sriov-nic-agent
    # systemctl start neutron-sriov-nic-agent
</code></pre>
<ol start="5">
<li>Modify the nova.conf file of the node and restart nova-compute</li>
</ol>
<pre><code class="language-bash">    # vim /etc/nova/nova.conf
    passthrough_whitelist = {&quot;devname&quot;: &quot;enp5s0f1&quot;, &quot;physical_network&quot;: &quot;physnet1&quot;}

    # systemctl restart openstack-nova-compute
</code></pre>
<p>If you need to configure multiple SRIOV network cards</p>
<pre><code>[root@controller ~]# vim /etc/nova/nova.conf
[pci]
passthrough_whitelist = {&quot;devname&quot;: &quot;enp129s0f0&quot;, &quot;physical_network&quot;: &quot;physnet1&quot;}
passthrough_whitelist = {&quot;devname&quot;: &quot;enp130s0f0&quot;, &quot;physical_network&quot;: &quot;physnet1&quot;}
[root@SRIOV03 ~]# vim /etc/neutron/plugins/ml2/sriov_agent.ini 
[sriov_nic]
physical_device_mappings = physnet1:enp129s0f0,physnet1:enp130s0f0
</code></pre>
<h1 id="test-srivo-non-affinity-functions"><a class="header" href="#test-srivo-non-affinity-functions">Test srivo non-affinity functions</a></h1>
<p>That is: set the two vnics of the vm in the same non-affinity group, and the two vnics of the generated sriov virtual machine will be distributed on two different physical SRIOV network cards</p>
<pre><code>[root@controller ~]# openstack port create --vnic-type direct --binding-profile anti_affinity_group=gwgh729 --network 40fa3655-1dc6-4fa1-8a21-507ae2a92cc5 sriov-port-1
+-----------------------+-----------------------------------------------------------------------------+
| Field                 | Value                                                                       |
+-----------------------+-----------------------------------------------------------------------------+
| admin_state_up        | UP                                                                          |
| allowed_address_pairs |                                                                             |
| binding_host_id       |                                                                             |
| binding_profile       | anti_affinity_group='gwgh729'                                               |
| binding_vif_details   |                                                                             |
| binding_vif_type      | unbound                                                                     |
| binding_vnic_type     | direct                                                                      |
| created_at            | 2018-10-11T07:11:06Z                                                        |
| data_plane_status     | None                                                                        |
| description           |                                                                             |
| device_id             |                                                                             |
| device_owner          |                                                                             |
| dns_assignment        | None                                                                        |
| dns_name              | None                                                                        |
| extra_dhcp_opts       |                                                                             |
| fixed_ips             | ip_address='192.168.5.48', subnet_id='8e026845-10d0-486b-9e34-45f2f2f01ada' |
| id                    | cb96ed5a-9bbc-46f7-9a44-fdc45e11b25b                                        |
| ip_address            | None                                                                        |
| mac_address           | fa:16:3e:c8:f7:9f                                                           |
| name                  | sriov-port-1                                                                |
| network_id            | 40fa3655-1dc6-4fa1-8a21-507ae2a92cc5                                        |
| option_name           | None                                                                        |
| option_value          | None                                                                        |
| port_security_enabled | True                                                                        |
| project_id            | 39db8ee1aced4109aa7ea45c9b45aa97                                            |
| qos_policy_id         | None                                                                        |
| revision_number       | 3                                                                           |
| security_group_ids    | 46a06d50-4ed7-495f-9458-eab6843495a8                                        |
| status                | DOWN                                                                        |
| subnet_id             | None                                                                        |
| tags                  |                                                                             |
| trunk_details         | None                                                                        |
| updated_at            | 2018-10-11T07:11:07Z                                                        |
+-----------------------+-----------------------------------------------------------------------------+
[root@controller ~]# openstack port create --vnic-type direct --binding-profile anti_affinity_group=gwgh729 --network 40fa3655-1dc6-4fa1-8a21-507ae2a92cc5 sriov-port-2
+-----------------------+-----------------------------------------------------------------------------+
| Field                 | Value                                                                       |
+-----------------------+-----------------------------------------------------------------------------+
| admin_state_up        | UP                                                                          |
| allowed_address_pairs |                                                                             |
| binding_host_id       |                                                                             |
| binding_profile       | anti_affinity_group='gwgh729'                                               |
| binding_vif_details   |                                                                             |
| binding_vif_type      | unbound                                                                     |
| binding_vnic_type     | direct                                                                      |
| created_at            | 2018-10-11T07:11:17Z                                                        |
| data_plane_status     | None                                                                        |
| description           |                                                                             |
| device_id             |                                                                             |
| device_owner          |                                                                             |
| dns_assignment        | None                                                                        |
| dns_name              | None                                                                        |
| extra_dhcp_opts       |                                                                             |
| fixed_ips             | ip_address='192.168.5.60', subnet_id='8e026845-10d0-486b-9e34-45f2f2f01ada' |
| id                    | d8ae3337-b98a-4523-9665-ed7773c6da90                                        |
| ip_address            | None                                                                        |
| mac_address           | fa:16:3e:23:0e:45                                                           |
| name                  | sriov-port-2                                                                |
| network_id            | 40fa3655-1dc6-4fa1-8a21-507ae2a92cc5                                        |
| option_name           | None                                                                        |
| option_value          | None                                                                        |
| port_security_enabled | True                                                                        |
| project_id            | 39db8ee1aced4109aa7ea45c9b45aa97                                            |
| qos_policy_id         | None                                                                        |
| revision_number       | 3                                                                           |
| security_group_ids    | 46a06d50-4ed7-495f-9458-eab6843495a8                                        |
| status                | DOWN                                                                        |
| subnet_id             | None                                                                        |
| tags                  |                                                                             |
| trunk_details         | None                                                                        |
| updated_at            | 2018-10-11T07:11:17Z                                                        |
+-----------------------+-----------------------------------------------------------------------------+
[root@controller ~]# nova boot han-sriov-antiaffinity --image 4ce53c0e-b3f5-4463-bdcd-20ee0f74fec6 --flavor 071b6288-8087-4d2e-959a-0b3bbb7fc565 --nic port-id=cb96ed5a-9bbc-46f7-9a44-fdc45e11b25b --nic port-id=d8ae3337-b98a-4523-9665-ed7773c6da90 --availability-zone sriov:SRIOV03
+--------------------------------------+------------------------------------------------------+
| Property                             | Value                                                |
+--------------------------------------+------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                               |
| OS-EXT-AZ:availability_zone          | sriov                                                |
| OS-EXT-SRV-ATTR:host                 | -                                                    |
| OS-EXT-SRV-ATTR:hostname             | han-sriov-antiaffinity                               |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                                    |
| OS-EXT-SRV-ATTR:instance_name        |                                                      |
| OS-EXT-SRV-ATTR:kernel_id            |                                                      |
| OS-EXT-SRV-ATTR:launch_index         | 0                                                    |
| OS-EXT-SRV-ATTR:ramdisk_id           |                                                      |
| OS-EXT-SRV-ATTR:reservation_id       | r-3vjlmls4                                           |
| OS-EXT-SRV-ATTR:root_device_name     | -                                                    |
| OS-EXT-SRV-ATTR:user_data            | -                                                    |
| OS-EXT-STS:power_state               | 0                                                    |
| OS-EXT-STS:task_state                | scheduling                                           |
| OS-EXT-STS:vm_state                  | building                                             |
| OS-SRV-USG:launched_at               | -                                                    |
| OS-SRV-USG:terminated_at             | -                                                    |
| accessIPv4                           |                                                      |
| accessIPv6                           |                                                      |
| adminPass                            | Z255abC53sLN                                         |
| config_drive                         |                                                      |
| created                              | 2018-10-11T07:12:43Z                                 |
| description                          | -                                                    |
| flavor:disk                          | 10                                                   |
| flavor:ephemeral                     | 0                                                    |
| flavor:extra_specs                   | {}                                                   |
| flavor:original_name                 | 2u4g10g                                              |
| flavor:ram                           | 4096                                                 |
| flavor:swap                          | 0                                                    |
| flavor:vcpus                         | 2                                                    |
| hostId                               |                                                      |
| host_status                          |                                                      |
| id                                   | ea80e14b-2b9a-4a31-ad27-94366fa600bf                 |
| image                                | centos7-10-08 (4ce53c0e-b3f5-4463-bdcd-20ee0f74fec6) |
| key_name                             | -                                                    |
| locked                               | False                                                |
| metadata                             | {}                                                   |
| name                                 | han-sriov-antiaffinity                               |
| os-extended-volumes:volumes_attached | []                                                   |
| progress                             | 0                                                    |
| security_groups                      | default                                              |
| status                               | BUILD                                                |
| tags                                 | []                                                   |
| tenant_id                            | 39db8ee1aced4109aa7ea45c9b45aa97                     |
| updated                              | 2018-10-11T07:12:43Z                                 |
| user_id                              | f6b12716402343e6beacdaa265e4e6c5                     |
+--------------------------------------+------------------------------------------------------+
[root@controller ~]# nova show han-sriov-antiaffinity
+--------------------------------------+----------------------------------------------------------+
| Property                             | Value                                                    |
+--------------------------------------+----------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                   |
| OS-EXT-AZ:availability_zone          | sriov                                                    |
| OS-EXT-SRV-ATTR:host                 | SRIOV03                                                  |
| OS-EXT-SRV-ATTR:hostname             | han-sriov-antiaffinity                                   |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | SRIOV03                                                  |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000cfd                                        |
| OS-EXT-SRV-ATTR:kernel_id            |                                                          |
| OS-EXT-SRV-ATTR:launch_index         | 0                                                        |
| OS-EXT-SRV-ATTR:ramdisk_id           |                                                          |
| OS-EXT-SRV-ATTR:reservation_id       | r-3vjlmls4                                               |
| OS-EXT-SRV-ATTR:root_device_name     | /dev/vda                                                 |
| OS-EXT-SRV-ATTR:user_data            | -                                                        |
| OS-EXT-STS:power_state               | 1                                                        |
| OS-EXT-STS:task_state                | -                                                        |
| OS-EXT-STS:vm_state                  | active                                                   |
| OS-SRV-USG:launched_at               | 2018-10-11T07:14:49.000000                               |
| OS-SRV-USG:terminated_at             | -                                                        |
| accessIPv4                           |                                                          |
| accessIPv6                           |                                                          |
| config_drive                         |                                                          |
| created                              | 2018-10-11T07:12:42Z                                     |
| description                          | -                                                        |
| flavor:disk                          | 10                                                       |
| flavor:ephemeral                     | 0                                                        |
| flavor:extra_specs                   | {}                                                       |
| flavor:original_name                 | 2u4g10g                                                  |
| flavor:ram                           | 4096                                                     |
| flavor:swap                          | 0                                                        |
| flavor:vcpus                         | 2                                                        |
| hostId                               | c9ae967a850ab4bf64f157ac3c05ef0adb86743d3a1f86115cda7f04 |
| host_status                          | UP                                                       |
| id                                   | ea80e14b-2b9a-4a31-ad27-94366fa600bf                     |
| image                                | centos7-10-08 (4ce53c0e-b3f5-4463-bdcd-20ee0f74fec6)     |
| key_name                             | -                                                        |
| locked                               | False                                                    |
| metadata                             | {}                                                       |
| name                                 | han-sriov-antiaffinity                                   |
| os-extended-volumes:volumes_attached | []                                                       |
| progress                             | 0                                                        |
| security_groups                      | default                                                  |
| status                               | ACTIVE                                                   |
| tags                                 | []                                                       |
| tenant_id                            | 39db8ee1aced4109aa7ea45c9b45aa97                         |
| updated                              | 2018-10-11T07:13:37Z                                     |
| user_id                              | f6b12716402343e6beacdaa265e4e6c5                         |
| vlan811-common network               | 192.168.5.48, 192.168.5.60                               |
+--------------------------------------+----------------------------------------------------------+

[root@SRIOV03 ~]# ip link show enp130s0f0|grep fa:16:3e:c8:f7:9f 
[root@SRIOV03 ~]# ip link show enp129s0f0|grep fa:16:3e:c8:f7:9f 
    vf 0 MAC fa:16:3e:c8:f7:9f, vlan 811, spoof checking on, link-state auto, trust off, query_rss off
[root@SRIOV03 ~]# ip link show enp130s0f0|grep fa:16:3e:23:0e:45
    vf 0 MAC fa:16:3e:23:0e:45, vlan 811, spoof checking on, link-state auto, trust off, query_rss off
[root@SRIOV03 ~]# ip link show enp129s0f0|grep fa:16:3e:23:0e:45
[root@SRIOV03 ~]# 
</code></pre>
<p>The vnic with mac address fa:16:3e:c8:f7:9f falls on enp129s0f0, and the vnic with mac address fa:16:3e:23:0e:45 falls on enp130s0f0</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2020-01-01 19:13</p>
<h1 id="automatically-clear-mirror-cache"><a class="header" href="#automatically-clear-mirror-cache">Automatically clear mirror cache</a></h1>
<p>Create a virtual machine on a computing node. If you create a secondary virtual machine on this node for the first time, the image file will be copied to the computing node directory /var/lib/nova/instances/_base. In the long run, this directory will take up relatively large disk space and needs to be cleaned up.</p>
<p>The cache directory can be automatically cleaned up by modifying the nova configuration file.</p>
<p>Compare the difference before and after setting remove_unused_base_images=True, that is, redundant images will not be automatically cleared before setting, but after setting and restarting the nova-compute service, redundant image files will be automatically cleared, and cloud hosts can be generated normally.</p>
<p><strong>remove_unused_base_images=True before setting:</strong></p>
<p><img src="openstack/2023-01-17-15-41-24.png" alt="" /></p>
<p>be32a717ae7ea3aa039131689864a325fd634c92 is an unused image file.</p>
<p>After waiting for a while, the file did not change.</p>
<p><strong>remove_unused_base_images=True After setting and restarting the nova-compute service:</strong></p>
<p><img src="openstack/2023-01-17-15-41-48.png" alt="" /></p>
<p>image_cache_manager_interval=5, remove_unused_original_minimum_age_seconds=3 are settings to shorten the test time. The actual production environment uses the default value of 40 minutes and the set value of 24 7 hours. Now it is changed to 5 seconds and 3 seconds. That is to say, without this setting to shorten the test time, the production environment is to check the image cache in 40 minutes, and the useless base image will be deleted after 247 hours.</p>
<p><img src="openstack/2023-01-17-15-41-58.png" alt="" /></p>
<p>automatically deleted soon.
Check /var/log/nova/nova-compute.log, and also find the log that automatically deletes redundant mirrors:</p>
<pre><code>2017-02-07 16:01:51.678 14200 INFO nova.virt.libvirt.imagecache [req-ee1a7ad5-a021-4aad-b653-7db14191536e - - - - -] Active base files: /var/lib/nova/instances/_base/0522bc602608d45758d815b01a6899ff3e1e3e27 /var/lib/nova/instances/_base/dc1ed4ad70a573f2acea085b068b61f3cb99e195 /var/lib/nova/instances/_base/be32a717ae7ea3aa039131689864a325fd634c1c
2017-02-07 16:01:51.678 14200 INFO nova.virt.libvirt.imagecache [req-ee1a7ad5-a021-4aad-b653-7db14191536e - - - - -] Removable base files: /var/lib/nova/instances/_base/be32a717ae7ea3aa039131689864a325fd634c92
2017-02-07 16:01:51.679 14200 INFO nova.virt.libvirt.imagecache [req-ee1a7ad5-a021-4aad-b653-7db14191536e - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/be32a717ae7ea3aa039131689864a325fd634c92
</code></pre>
<p>Dashboard neutron reports error Invalid service catalog service: network</p>
<p><strong>Phenomenon:</strong>
The neutron service is good, and the command line creates a network to view the network normally, but the dashboard cannot view the network-related pages. The
dashboard neutron reports an error Invalid service catalog service: network</p>
<p><strong>Solution:</strong>
It should be that Keystone is not properly configured. As a result, no relevant Catalog information was found.</p>
<pre><code>[root@EXTENV-194-18-2-11 ~]# openstack catalog list
+-----------+-----------+-----------------------------------------------+
| Name      | Type      | Endpoints                                     |
+-----------+-----------+-----------------------------------------------+
| placement | placement | RegionOne                                     |
|           |           |   internal: http://nova-ha-vip:8778           |
|           |           | RegionOne                                     |
|           |           |   admin: http://nova-ha-vip:8778              |
|           |           | RegionOne                                     |
|           |           |   public: http://nova-ha-vip:8778             |
|           |           |                                               |
| keystone  | identity  | RegionOne                                     |
|           |           |   public: http://keystone-ha-vip:5000/v3/     |
|           |           | RegionOne                                     |
|           |           |   internal: http://keystone-ha-vip:35357/v3/  |
|           |           | RegionOne                                     |
|           |           |   admin: http://keystone-ha-vip:35357/v3/     |
|           |           |                                               |
| glance    | image     | RegionOne                                     |
|           |           |   admin: http://glance-ha-vip:9292            |
|           |           | RegionOne                                     |
|           |           |   internal: http://glance-ha-vip:9292         |
|           |           | RegionOne                                     |
|           |           |   public: http://glance-ha-vip:9292           |
|           |           |                                               |
| nova      | compute   | RegionOne                                     |
|           |           |   public: http://nova-ha-vip:8774/v2.1        |
|           |           | RegionOne                                     |
|           |           |   admin: http://nova-ha-vip:8774/v2.1         |
|           |           | RegionOne                                     |
|           |           |   internal: http://nova-ha-vip:8774/v2.1      |
|           |           |                                               |
| neutron   | network   |                                               |
| neutron   | network   | RegionOne                                     |
|           |           |   public: http://neutron-server-ha-vip:9696   |
|           |           | RegionOne                                     |
|           |           |   admin: http://neutron-server-ha-vip:9696    |
|           |           | RegionOne                                     |
|           |           |   internal: http://neutron-server-ha-vip:9696 |
|           |           |                                               |
+-----------+-----------+-----------------------------------------------+
</code></pre>
<p>So just delete the first neutron data without url, but found that there is only openstack catalog list, no openstack catalog delete command. Later, I checked the keystone configuration file keystone.conf and found the following configuration.</p>
<p>from the configuration file. The catalog is the data read from mysql, and then the dirty data is found from the service table in the keystone library of mysql, and then I know to use openstack service delete to delete the 'dirty data', and the problem is solved.</p>
<pre><code>MariaDB [keystone]&gt; select * from service;
+----------------------------------+-----------+---------+-------------------------------------------------------------+
| id                               | type      | enabled | extra                                                       |
+----------------------------------+-----------+---------+-------------------------------------------------------------+
| 520f6bf8564240be9678c4ef25305cad | placement |       1 | {&quot;description&quot;: &quot;OpenStack Placement&quot;, &quot;name&quot;: &quot;placement&quot;} |
| 960580852a594c078e68fe3683e35db5 | identity  |       1 | {&quot;name&quot;: &quot;keystone&quot;}                                        |
| 98ed18fcd8104732919bb5869a5a6dc2 | image     |       1 | {&quot;description&quot;: &quot;OpenStack Image&quot;, &quot;name&quot;: &quot;glance&quot;}        |
| abef1b9469d94d3ab9f27c8ed72a5a48 | compute   |       1 | {&quot;description&quot;: &quot;OpenStack Compute&quot;, &quot;name&quot;: &quot;nova&quot;}        |
| e37085e8fb2a49c0921c2d24f5e4f9b5 | network   |       1 | {&quot;description&quot;: &quot;OpenStack Networking&quot;, &quot;name&quot;: &quot;neutron&quot;}  |
| f1b661407ce04f79bc24605fa59bb74c | network   |       1 | {&quot;description&quot;: &quot;OpenStack Networking&quot;, &quot;name&quot;: &quot;neutron&quot;}  |
+----------------------------------+-----------+---------+-------------------------------------------------------------+
6 rows in set (0.00 sec)

MariaDB [keystone]&gt; select * from endpoint;
+----------------------------------+--------------------+-----------+----------------------------------+-----------------------------------+-------+---------+-----------+
| id                               | legacy_endpoint_id | interface | service_id                       | url                               | extra | enabled | region_id |
+----------------------------------+--------------------+-----------+----------------------------------+-----------------------------------+-------+---------+-----------+
| 142cb619cd2242828b0c9394d5baaea1 | NULL               | public    | f1b661407ce04f79bc24605fa59bb74c | http://neutron-server-ha-vip:9696 | {}    |       1 | RegionOne |
| 2252d3ef840b4c5aa1184ebe8d6094f1 | NULL               | public    | abef1b9469d94d3ab9f27c8ed72a5a48 | http://nova-ha-vip:8774/v2.1      | {}    |       1 | RegionOne |
| 476654c6e7dd4d22b290de451e3afda0 | NULL               | admin     | abef1b9469d94d3ab9f27c8ed72a5a48 | http://nova-ha-vip:8774/v2.1      | {}    |       1 | RegionOne |
| 562a5d5443af4dfab6760204d0adf3bf | NULL               | internal  | 520f6bf8564240be9678c4ef25305cad | http://nova-ha-vip:8778           | {}    |       1 | RegionOne |
| 58bd5f09811a4ebcb62a4b51fb7ae444 | NULL               | admin     | f1b661407ce04f79bc24605fa59bb74c | http://neutron-server-ha-vip:9696 | {}    |       1 | RegionOne |
| 600811f8ccaf42669d4d83b897af3933 | NULL               | admin     | 520f6bf8564240be9678c4ef25305cad | http://nova-ha-vip:8778           | {}    |       1 | RegionOne |
| 80683f619efb41dcbb6796ea04f16159 | NULL               | internal  | f1b661407ce04f79bc24605fa59bb74c | http://neutron-server-ha-vip:9696 | {}    |       1 | RegionOne |
| 8e0a684607294a729f87d7d8b1a639ca | NULL               | public    | 520f6bf8564240be9678c4ef25305cad | http://nova-ha-vip:8778           | {}    |       1 | RegionOne |
| 9ef0f18d891e45608ffc41985dc6afa6 | NULL               | public    | 960580852a594c078e68fe3683e35db5 | http://keystone-ha-vip:5000/v3/   | {}    |       1 | RegionOne |
| a0b10cb04a5b4ca3859aaf2ea4ca2a3b | NULL               | admin     | 98ed18fcd8104732919bb5869a5a6dc2 | http://glance-ha-vip:9292         | {}    |       1 | RegionOne |
| c53979becccc44f1813e9f50a619af7e | NULL               | internal  | 960580852a594c078e68fe3683e35db5 | http://keystone-ha-vip:35357/v3/  | {}    |       1 | RegionOne |
| dadbb8dc218245bbba8c9a34237413ec | NULL               | internal  | 98ed18fcd8104732919bb5869a5a6dc2 | http://glance-ha-vip:9292         | {}    |       1 | RegionOne |
| f4034b8c086a451caed52ac51a761fb0 | NULL               | public    | 98ed18fcd8104732919bb5869a5a6dc2 | http://glance-ha-vip:9292         | {}    |       1 | RegionOne |
| fc150884825544baaf4912f14e76f51a | NULL               | internal  | abef1b9469d94d3ab9f27c8ed72a5a48 | http://nova-ha-vip:8774/v2.1      | {}    |       1 | RegionOne |
| fc7132052063438895674fd7b840db68 | NULL               | admin     | 960580852a594c078e68fe3683e35db5 | http://keystone-ha-vip:35357/v3/  | {}    |       1 | RegionOne |
+----------------------------------+--------------------+-----------+----------------------------------+-----------------------------------+-------+---------+-----------+
15 rows in set (0.00 sec)

[root@EXTENV-194-18-2-11 ~]#  openstack service list
+----------------------------------+-----------+-----------+
| ID                               | Name      | Type      |
+----------------------------------+-----------+-----------+
| 520f6bf8564240be9678c4ef25305cad | placement | placement |
| 960580852a594c078e68fe3683e35db5 | keystone  | identity  |
| 98ed18fcd8104732919bb5869a5a6dc2 | glance    | image     |
| abef1b9469d94d3ab9f27c8ed72a5a48 | nova      | compute   |
| e37085e8fb2a49c0921c2d24f5e4f9b5 | neutron   | network   |
| f1b661407ce04f79bc24605fa59bb74c | neutron   | network   |
+----------------------------------+-----------+-----------+
[root@EXTENV-194-18-2-11 ~]# openstack service delete e37085e8fb2a49c0921c2d24f5e4f9b5
[root@EXTENV-194-18-2-11 ~]# systemctl restart httpd.service memcached.service
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2018-09-29 20:28</p>
<p>Executing the glance command reports an http 503 error</p>
<pre><code>HTTPServiceUnavailable: 503 Service Unavailable: No server is available to handle this request. (HTTP 503)
503 Service Unavailable: No server is available to handle this request. (HTTP 503)
</code></pre>
<p>My colleague encountered problems when installing Openstack Pike.</p>
<p>By looking at the glance log, I found that rabbitmq was not working. Systemctl status did not work. The previous deployment documents failed, so I started the next work. Now the problem that needs to be solved becomes why rabbitmq not working properly</p>
<p>After reading the log of /var/log/rabbitmq/, there is such an error report</p>
<pre><code>=ERROR REPORT==== 7-Sep-2018::11:09:01 ===
Failed to start Ranch listener {acceptor,{0,0,0,0,0,0,0,0},5672} in ranch_tcp:listen([{port,
                                                                                    5672},
                                                                                    {ip,
                                                                                    {0,
                                                                                        0,
                                                                                        0,
                                                                                        0,
                                                                                        0,
                                                                                        0,
                                                                                        0,
                                                                                        0}},
                                                                                    inet6,
                                                                                    {backlog,
                                                                                    128},
                                                                                    {nodelay,
                                                                                    true},
                                                                                    {linger,
                                                                                    {true,
                                                                                        0}},
                                                                                    {exit_on_close,
                                                                                    false}]) for reason eaddrinuse (address already in use)
</code></pre>
<p>Check the usage of port 5672, # netstat -tunpl found that haproxy is using
rabbitmq's own cluster management, it is not necessary to use
haproxy to open the configuration file of haoroxy, and found that the following content has not been noticed</p>
<pre><code>listen rabbitmq
option tcpka
bind 10.144.85.238:5672
server rabbitmq1 10.144.85.93:5672 check inter 5000 rise 2 fall 3
server rabbitmq2 10.144.85.162:5672 check inter 5000 rise 2 fall 3 backup
server rabbitmq3 10.144.85.163:5672 check inter 5000 rise 2 fall 3 backup
mode tcp
timeout client 48h
timeout server 48h
timeout connect 120s
balance roundrobin
</code></pre>
<p>After commenting out, restart the haproxy service, and then execute # netstat -tunpl, no haproxy process is occupying port 5672</p>
<p>Restart the rabbitmq service, still report an error, continue to look at the log, the following error is reported</p>
<pre><code>=WARNING REPORT==== 7-Sep-2018::13:48:17 ===
Error while waiting for Mnesia tables: {timeout_waiting_for_tables,
                                        [rabbit_user,rabbit_user_permission,
                                        rabbit_vhost,rabbit_durable_route,
                                        rabbit_durable_exchange,
                                        rabbit_runtime_parameters,
                                        rabbit_durable_queue]}
</code></pre>
<p>The explanation is as follows:</p>
<pre><code>Alternatively, perhaps your mnesia dir (/var/lib/rabbitmq/mnesia/rabbit) 
got into a a weird state. Try clearing it. 
</code></pre>
<p>That is, the distributed database mnesia is abnormal.</p>
<p>Solution:
(1) Clear the database files under /var/lib/rabbitmq/mnesia/
(2) Restart the node</p>
<pre><code>Sep 27 16:00:00 controller1 haproxy-systemd-wrapper[22727]: [ALERT] 269/160000 (22729) : Error(s) found in configuration file : /etc/haproxy/haproxy.cfg
Sep 27 16:00:00 controller1 haproxy-systemd-wrapper[22727]: [WARNING] 269/160000 (22729) : parsing [/etc/haproxy/haproxy.cfg:16] : 'option httplog' not usable with proxy 'rabbitmq' (needs 'mode http'). Falling back to 'option tcplog'.
Sep 27 16:00:00 controller1 haproxy-systemd-wrapper[22727]: [WARNING] 269/160000 (22729) : config : 'option forwardfor' ignored for proxy 'rabbitmq' as it requires HTTP mode.
Sep 27 16:00:00 controller1 haproxy-systemd-wrapper[22727]: [WARNING] 269/160000 (22729) : config : 'option forwardfor' ignored for proxy 'rdb_mysql' as it requires HTTP mode.
Sep 27 16:00:00 controller1 haproxy-systemd-wrapper[22727]: [WARNING] 269/160000 (22729) : stats socket will not work as expected in multi-process mode (nbproc &gt; 1), you should force process binding globally using 'stats bind-process' or per socket ...rocess' attribute.
Sep 27 16:00:00 controller1 haproxy-systemd-wrapper[22727]: [ALERT] 269/160000 (22729) : Fatal errors found in configuration.
</code></pre>
<p>Change the httplog of haproxy.cfg to tcplog</p>
<pre><code>Sep 27 16:03:19 controller1 haproxy-systemd-wrapper[23196]: [ALERT] 269/160319 (23197) : parsing [/etc/haproxy/haproxy.cfg:18] : unknown keyword 'optionhttp-server-close' in 'defaults' section
Sep 27 16:03:19 controller1 haproxy-systemd-wrapper[23196]: [ALERT] 269/160319 (23197) : Error(s) found in configuration file : /etc/haproxy/haproxy.cfg
Sep 27 16:03:19 controller1 haproxy-systemd-wrapper[23196]: [WARNING] 269/160319 (23197) : config : 'option forwardfor' ignored for proxy 'rabbitmq' as it requires HTTP mode.
Sep 27 16:03:19 controller1 haproxy-systemd-wrapper[23196]: [WARNING] 269/160319 (23197) : config : 'option forwardfor' ignored for proxy 'rdb_mysql' as it requires HTTP mode.
Sep 27 16:03:19 controller1 haproxy-systemd-wrapper[23196]: [WARNING] 269/160319 (23197) : stats socket will not work as expected in multi-process mode (nbproc &gt; 1), you should force process binding globally using 'stats bind-process' or per socket ...rocess' attribute.
</code></pre>
<p>One space is missing, change optionhttp-server-close to option http-server-close</p>
<pre><code>Sep 27 16:04:18 controller1 systemd[1]: Starting HAProxy Load Balancer...
Sep 27 16:04:18 controller1 haproxy-systemd-wrapper[23335]: haproxy-systemd-wrapper: executing /usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -Ds
Sep 27 16:04:18 controller1 haproxy-systemd-wrapper[23335]: [WARNING] 269/160418 (23336) : config : 'option forwardfor' ignored for proxy 'rabbitmq' as it requires HTTP mode.
Sep 27 16:04:18 controller1 haproxy-systemd-wrapper[23335]: [WARNING] 269/160418 (23336) : config : 'option forwardfor' ignored for proxy 'rdb_mysql' as it requires HTTP mode.
Sep 27 16:04:18 controller1 haproxy-systemd-wrapper[23335]: [WARNING] 269/160418 (23336) : stats socket will not work as expected in multi-process mode (nbproc &gt; 1), you should force process binding globally using 'stats bind-process' or per socket ...rocess' attribute.
Sep 27 16:04:18 controller1 haproxy-systemd-wrapper[23335]: [ALERT] 269/160418 (23336) : Starting proxy rabbitmq: cannot bind socket [194.18.2.10:5672]
</code></pre>
<p>5672 is the port number of rabbitmq. Rabbitmq has its own cluster management and does not need haproxy. Edit haproxy.conf to watch out for the rabbitmq section</p>
<pre><code>2018-09-28 10:43:13.996 32177 ERROR nova Traceback (most recent call last):
2018-09-28 10:43:13.996 32177 ERROR nova   File &quot;/usr/bin/nova-novncproxy&quot;, line 10, in &lt;module&gt;
2018-09-28 10:43:13.996 32177 ERROR nova     sys.exit(main())
2018-09-28 10:43:13.996 32177 ERROR nova   File &quot;/usr/lib/python2.7/site-packages/nova/cmd/novncproxy.py&quot;, line 41, in main
2018-09-28 10:43:13.996 32177 ERROR nova     port=CONF.vnc.novncproxy_port)
2018-09-28 10:43:13.996 32177 ERROR nova   File &quot;/usr/lib/python2.7/site-packages/nova/cmd/baseproxy.py&quot;, line 67, in proxy
2018-09-28 10:43:13.996 32177 ERROR nova     RequestHandlerClass=websocketproxy.NovaProxyRequestHandler
2018-09-28 10:43:13.996 32177 ERROR nova   File &quot;/usr/lib/python2.7/site-packages/websockify/websocket.py&quot;, line 973, in start_server
2018-09-28 10:43:13.996 32177 ERROR nova     tcp_keepintvl=self.tcp_keepintvl)
2018-09-28 10:43:13.996 32177 ERROR nova   File &quot;/usr/lib/python2.7/site-packages/websockify/websocket.py&quot;, line 742, in socket
2018-09-28 10:43:13.996 32177 ERROR nova     sock.listen(100)
2018-09-28 10:43:13.996 32177 ERROR nova   File &quot;/usr/lib64/python2.7/socket.py&quot;, line 224, in meth
2018-09-28 10:43:13.996 32177 ERROR nova     return getattr(self._sock,name)(*args)
2018-09-28 10:43:13.996 32177 ERROR nova error: [Errno 98] Address already in use



[root@controller1 nova]# netstat -tunpl |grep 6080
tcp        0      0 194.18.2.10:6080        0.0.0.0:*               LISTEN      24144/haproxy  


nova.conf
[vnc]
novncproxy_host = 194.18.2.11
vncserver_proxyclient_address = 194.18.2.11
vncserver_listen = 194.18.2.11
</code></pre>
<p>Change the management vip to the local management ip</p>
<pre><code>2018-09-29 10:41:55.593 17797 ERROR nova   File &quot;/usr/lib/python2.7/site-packages/amqp/connection.py&quot;, line 473, in on_inbound_method
2018-09-29 10:41:55.593 17797 ERROR nova     method_sig, payload, content,
2018-09-29 10:41:55.593 17797 ERROR nova   File &quot;/usr/lib/python2.7/site-packages/amqp/abstract_channel.py&quot;, line 142, in dispatch_method
2018-09-29 10:41:55.593 17797 ERROR nova     listener(*args)
2018-09-29 10:41:55.593 17797 ERROR nova   File &quot;/usr/lib/python2.7/site-packages/amqp/connection.py&quot;, line 595, in _on_close
2018-09-29 10:41:55.593 17797 ERROR nova     (class_id, method_id), ConnectionError)
2018-09-29 10:41:55.593 17797 ERROR nova AccessRefused: (0, 0): (403) ACCESS_REFUSED - Login was refused using authentication mechanism AMQPLAIN. For details see the broker logfile.
</code></pre>
<p>The rabbitmq user configured by openstack does not exist or the password is wrong, or the authority is insufficient.</p>
<pre><code>[root@controller1 ~]# rabbitmqctl set_policy ha-all '^(?!amq\.).*' '{&quot;ha-mode&quot;: &quot;all&quot;}'
Setting policy &quot;ha-all&quot; for pattern &quot;^(?!amq\\.).*&quot; to &quot;{\&quot;ha-mode\&quot;: \&quot;all\&quot;}&quot; with priority &quot;0&quot; ...
[root@controller1 ~]# rabbitmqctl add_user rabbitMQ123 123456
Creating user &quot;rabbitMQ123&quot; ...
[root@controller1 ~]# rabbitmqctl set_permissions rabbitMQ123 &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;
Setting permissions for user &quot;rabbitMQ123&quot; in vhost &quot;/&quot; ...
[root@controller1 ~]# 
</code></pre>
<pre><code class="language-bash">[root@compute1 neutron]# systemctl status neutron-openvswitch-agent.service
● neutron-openvswitch-agent.service - OpenStack Neutron Open vSwitch Agent
   Loaded: loaded (/usr/lib/systemd/system/neutron-openvswitch-agent.service; enabled; vendor preset: disabled)
   Active: failed (Result: start-limit) since Sat 2018-09-29 16:07:57 CST; 4min 5s ago
  Process: 23272 ExecStart=/usr/bin/neutron-openvswitch-agent --config-file /usr/share/neutron/neutron-dist.conf --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/openvswitch_agent.ini --config-dir /etc/neutron/conf.d/common --config-dir /etc/neutron/conf.d/neutron-openvswitch-agent --log-file /var/log/neutron/openvswitch-agent.log (code=exited, status=1/FAILURE)
  Process: 23266 ExecStartPre=/usr/bin/neutron-enable-bridge-firewall.sh (code=exited, status=0/SUCCESS)
 Main PID: 23272 (code=exited, status=1/FAILURE)


utron-openvswitch-agent: sys.exit(main())
Sep 29 16:07:58 compute3 neutron-openvswitch-agent: File &quot;/usr/lib/python2.7/site-packages/neutron/cmd/eventlet/plugins/ovs_neutron_agent.py&quot;, line 20, in main
Sep 29 16:07:58 compute3 neutron-openvswitch-agent: agent_main.main()
Sep 29 16:07:58 compute3 neutron-openvswitch-agent: File &quot;/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/openvswitch/agent/main.py&quot;, line 42, in main
Sep 29 16:07:58 compute3 neutron-openvswitch-agent: common_config.init(sys.argv[1:])
Sep 29 16:07:58 compute3 neutron-openvswitch-agent: File &quot;/usr/lib/python2.7/site-packages/neutron/common/config.py&quot;, line 78, in init
Sep 29 16:07:58 compute3 neutron-openvswitch-agent: **kwargs)
Sep 29 16:07:58 compute3 neutron-openvswitch-agent: File &quot;/usr/lib/python2.7/site-packages/oslo_config/cfg.py&quot;, line 2473, in __call__
Sep 29 16:07:58 compute3 neutron-openvswitch-agent: self._namespace._files_permission_denied)
Sep 29 16:07:58 compute3 neutron-openvswitch-agent: oslo_config.cfg.ConfigFilesPermissionDeniedError: Failed to open some config files: /etc/neutron/plugins/ml2/openvswitch_agent.ini
</code></pre>
<p>File permission attribution error.</p>
<pre><code class="language-bash">[root@controller1 neutron]# ll /etc/neutron/plugins/ml2/openvswitch_agent.ini
-rw-r----- 1 root neutron 241 Sep 29 15:47 /etc/neutron/plugins/ml2/openvswitch_agent.ini

[root@compute2 ~]# ll /etc/neutron/plugins/ml2/openvswitch_agent.ini
-rw-r----- 1 root root 242 Sep 29 16:07 /etc/neutron/plugins/ml2/openvswitch_agent.ini
</code></pre>
<pre><code>2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [req-6812937f-30f2-4f49-8303-d1425ebb1357 5563b2f9c80a40729c969df2f3ed71ef 1ada390dc6fb4fe790bcf38777822682 - default default] [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525] Instance failed to spawn: TypeError: 'NoneType' object is not iterable
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525] Traceback (most recent call last):
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 2192, in _build_resources
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     yield resources
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 2007, in _build_and_run_instance
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     block_device_info=block_device_info)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 2802, in spawn
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     block_device_info=block_device_info)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 3240, in _create_image
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     fallback_from_host)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 3331, in _create_and_inject_local_root
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     instance, size, fallback_from_host)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 6987, in _try_fetch_image_cache
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     size=size)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/imagebackend.py&quot;, line 241, in cache
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     *args, **kwargs)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/imagebackend.py&quot;, line 595, in create_image
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     prepare_template(target=base, *args, **kwargs)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/oslo_concurrency/lockutils.py&quot;, line 271, in inner
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     return f(*args, **kwargs)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/imagebackend.py&quot;, line 237, in fetch_func_sync
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     fetch_func(target=target, *args, **kwargs)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/utils.py&quot;, line 446, in fetch_image
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     images.fetch_to_raw(context, image_id, target)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/images.py&quot;, line 144, in fetch_to_raw
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     fetch(context, image_href, path_tmp)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/images.py&quot;, line 135, in fetch
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     IMAGE_API.download(context, image_href, dest_path=path)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/image/api.py&quot;, line 184, in download
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     dst_path=dest_path)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/image/glance.py&quot;, line 366, in download
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     {'path': dst_path, 'exception': ex})
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/oslo_utils/excutils.py&quot;, line 220, in __exit__
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     self.force_reraise()
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/oslo_utils/excutils.py&quot;, line 196, in force_reraise
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     six.reraise(self.type_, self.value, self.tb)
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]   File &quot;/usr/lib/python2.7/site-packages/nova/image/glance.py&quot;, line 350, in download
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525]     for chunk in image_chunks:
2018-09-29 16:27:58.982 18928 ERROR nova.compute.manager [instance: dc3db2cd-4473-4d13-9ecf-4f602e457525] TypeError: 'NoneType' object is not iterable
</code></pre>
<p>glance uses local storage, glance-api is enabled on three nodes, and haproxy is configured.
Correspond to the location of haproxy, just comment out the two nodes</p>
<pre><code>listen glance_api
bind 194.18.2.10:9292
balance roundrobin
option httplog
server glance1 194.18.2.11:9292 check inter 2000 rise 2 fall 3
#server glance2 194.18.2.12:9292 check inter 2000 rise 2 fall 3
#server glance3 194.18.2.13:9292 check inter 2000 rise 2 fall 3
</code></pre>
<pre><code class="language-bash">2018-09-30 15:38:28.505 17135 ERROR cinder   File &quot;/usr/lib/python2.7/site-packages/pymysql/cursors.py&quot;, line 322, in _query
2018-09-30 15:38:28.505 17135 ERROR cinder     conn.query(q)
2018-09-30 15:38:28.505 17135 ERROR cinder   File &quot;/usr/lib/python2.7/site-packages/pymysql/connections.py&quot;, line 856, in query
2018-09-30 15:38:28.505 17135 ERROR cinder     self._affected_rows = self._read_query_result(unbuffered=unbuffered)
2018-09-30 15:38:28.505 17135 ERROR cinder   File &quot;/usr/lib/python2.7/site-packages/pymysql/connections.py&quot;, line 1057, in _read_query_result
2018-09-30 15:38:28.505 17135 ERROR cinder     result.read()
2018-09-30 15:38:28.505 17135 ERROR cinder   File &quot;/usr/lib/python2.7/site-packages/pymysql/connections.py&quot;, line 1340, in read
2018-09-30 15:38:28.505 17135 ERROR cinder     first_packet = self.connection._read_packet()
2018-09-30 15:38:28.505 17135 ERROR cinder   File &quot;/usr/lib/python2.7/site-packages/pymysql/connections.py&quot;, line 1014, in _read_packet
2018-09-30 15:38:28.505 17135 ERROR cinder     packet.check_error()
2018-09-30 15:38:28.505 17135 ERROR cinder   File &quot;/usr/lib/python2.7/site-packages/pymysql/connections.py&quot;, line 393, in check_error
2018-09-30 15:38:28.505 17135 ERROR cinder     err.raise_mysql_exception(self._data)
2018-09-30 15:38:28.505 17135 ERROR cinder   File &quot;/usr/lib/python2.7/site-packages/pymysql/err.py&quot;, line 107, in raise_mysql_exception
2018-09-30 15:38:28.505 17135 ERROR cinder     raise errorclass(errno, errval)
2018-09-30 15:38:28.505 17135 ERROR cinder ProgrammingError: (pymysql.err.ProgrammingError) (1146, u&quot;Table 'cinder.services' doesn't exist&quot;) [SQL: u'SELECT services.created_at AS services_created_at, services.deleted_at AS services_deleted_at, services.deleted AS services_deleted, services.id AS services_id, services.cluster_name AS services_cluster_name, services.host AS services_host, services.`binary` AS services_binary, services.updated_at AS services_updated_at, services.topic AS services_topic, services.report_count AS services_report_count, services.disabled AS services_disabled, services.availability_zone AS services_availability_zone, services.disabled_reason AS services_disabled_reason, services.modified_at AS services_modified_at, services.rpc_current_version AS services_rpc_current_version, services.object_current_version AS services_object_current_version, services.replication_status AS services_replication_status, services.active_backend_id AS services_active_backend_id, services.frozen AS services_frozen \nFROM services \nWHERE services.deleted = false AND services.`binary` = %(binary_1)s'] [parameters: {u'binary_1': 'cinder-scheduler'}]
</code></pre>
<p>cinder api, volume, scheduler three log files all report the above error, like cinder service connection database error</p>
<p>First check whether there is any error in the configuration of the database connection information in cinder.conf, and confirm that the configuration is correct.</p>
<p>The port number of the cinder service cannot be found</p>
<pre><code>[root@controller2 opt]# netstat -tunpl |grep 8776
[root@controller2 opt]# 
</code></pre>
<p>I can't find the table data of cinder even after logging in to the database</p>
<pre><code>MariaDB [(none)]&gt; use cinder;
Database changed
MariaDB [cinder]&gt; show tables;
Empty set (0.01 sec)

MariaDB [cinder]&gt;
</code></pre>
<p>After re-executing the following command, it returns to normal. Maybe the command was not executed or the wrong node was executed.</p>
<pre><code># su -s /bin/sh -c &quot;cinder-manage db sync&quot; cinder
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2018-08-28 14:48</p>
<h1 id="start-openvswitch-error"><a class="header" href="#start-openvswitch-error">Start openvswitch error</a></h1>
<pre><code>[root@SRIOV03 ml2]#  systemctl start openvswitch.service
A dependency job for openvswitch.service failed. See 'journalctl -xe' for details.
[root@SRIOV03 ml2]# journalctl -xe
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ovsdb-server.service has begun starting up.
Aug 28 10:33:00 SRIOV03 runuser[6958]: PAM audit_open() failed: Permission denied
Aug 28 10:33:00 SRIOV03 ovs-ctl[6929]: runuser: System error
Aug 28 10:33:00 SRIOV03 ovs-ctl[6929]: /etc/openvswitch/conf.db does not exist ... (warning).
Aug 28 10:33:00 SRIOV03 runuser[6960]: PAM audit_open() failed: Permission denied
Aug 28 10:33:00 SRIOV03 ovs-ctl[6929]: Creating empty database /etc/openvswitch/conf.db runuser: System error
Aug 28 10:33:00 SRIOV03 ovs-ctl[6929]: [FAILED]
Aug 28 10:33:00 SRIOV03 systemd[1]: ovsdb-server.service: control process exited, code=exited status=1
Aug 28 10:33:00 SRIOV03 systemd[1]: Failed to start Open vSwitch Database Unit.
</code></pre>
<p>It may be that selinux is active, turn it off</p>
<pre><code># vim /etc/selinux/config 
SELINUX=enforcing -&gt; SELINUX=disabled
# setenforce 0
# getenforce 
Permissive
# systemctl restart openvswitch.service
</code></pre>
<h1 id="generate-vm-and-report-ethtool-ens4f0-error"><a class="header" href="#generate-vm-and-report-ethtool-ens4f0-error">Generate vm and report ethtool ens4f0 error</a></h1>
<pre><code>2018-08-28 11:19:51.568 8034 DEBUG oslo_concurrency.processutils [req-cfeb820d-1167-4c32-a336-8fed1120556b - - - - -] u'ethtool ens4f0' failed. Not Retrying. execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:452
2018-08-28 11:19:51.568 8034 WARNING nova.virt.extrautils [req-cfeb820d-1167-4c32-a336-8fed1120556b - - - - -] When exec ethtool network card, exception occurs: Unexpected error while running command.

/etc/nova/nova.conf
nw_interface_name = ens4f0
</code></pre>
<p>Just change it to a data plane network card</p>
<h1 id="create-a-vm-and-report-qemu-unexpectedly-closed-the-monitor-error"><a class="header" href="#create-a-vm-and-report-qemu-unexpectedly-closed-the-monitor-error">Create a vm and report qemu unexpectedly closed the monitor error</a></h1>
<pre><code>| fault                                | {&quot;message&quot;: &quot;internal error: qemu unexpectedly closed the monitor: 2018-08-28T04:08:03.821855Z qemu-kvm: -chardev pty,id=charserial0,logfile=/dev/fdset/2,logappend=on: char device redirected to /dev/pts/3 (label charserial0) |
|                                      | 2018-08-28T04:08:04.024833Z qemu-kvm: -vnc &quot;, &quot;code&quot;: 500, &quot;details&quot;: &quot;  File \&quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py\&quot;, line 1856, in _do_build_and_run_instance                                             |
|                                      |     filter_properties)                                                                                                                                                                                                           |
|                                      |   File \&quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py\&quot;, line 2086, in _build_and_run_instance                                                                                                                       |
|                                      |     instance_uuid=instance.uuid, reason=six.text_type(e))  
</code></pre>
<pre><code class="language-bash"># vim /var/log/libvirt/libvirtd.log 
2018-08-28 04:29:31.302+0000: 11230: error : virNetSocketReadWire:1808 : End of file while reading data: Input/output error
2018-08-28 04:36:25.462+0000: 11235: info : qemuDomainDefineXMLFlags:7286 : Creating domain 'instance-000007bc'
2018-08-28 04:36:25.779+0000: 11230: error : qemuMonitorIORead:588 : Unable to read from monitor: Connection reset by peer
2018-08-28 04:36:25.780+0000: 11230: error : qemuProcessReportLogError:1862 : internal error: qemu unexpectedly closed the monitor: 2018-08-28T04:36:25.720940Z qemu-kvm: -chardev pty,id=charserial0,logfile=/dev/fdset/2,logappend=on: char device redirected to /dev/pts/2 (label charserial0)
2018-08-28T04:36:25.778613Z qemu-kvm: -vnc 10.144.85.92:0: Failed to start VNC server: Failed to bind socket: Cannot assign requested address
2018-08-28 04:36:26.148+0000: 11239: info : qemuDomainUndefineFlags:7401 : Undefining domain 'instance-000007bc'
</code></pre>
<p>Because the vnc ip configuration error in nova.conf</p>
<pre><code>[vnc]
enabled = True
novncproxy_host = 10.144.85.92
vncserver_proxyclient_address = 10.144.85.92
vncserver_listen = 10.144.85.92
novncproxy_base_url = http://10.144.85.238:6080/vnc_auto.html 
</code></pre>
<h1 id="the-vm-can-be-generated-on-the-new-node-but-the-ip-cannot-be-obtained-when-logging-in-to-the-vm"><a class="header" href="#the-vm-can-be-generated-on-the-new-node-but-the-ip-cannot-be-obtained-when-logging-in-to-the-vm">The vm can be generated on the new node, but the ip cannot be obtained when logging in to the vm</a></h1>
<p>It may be that the switch vlan is not allowed</p>
<h1 id="create-virtual-machines-concurrently-some-fail"><a class="header" href="#create-virtual-machines-concurrently-some-fail">Create virtual machines concurrently, some fail</a></h1>
<p>The error seen in the log is that rabbitmq cannot connect to the port</p>
<p>It is initially determined that rabbitmq cannot work normally due to excessive pressure on rabbitmq. Take some steps now to improve the situation.</p>
<p>①. Change the rabbitmq node 2 and node 3 of the rabbitmq cluster from disc mode to ram mode.</p>
<p>②Distribute the rabbitmq pressure to 3 rabbitmq nodes. By looking at the rabbitmq log, it is found that before the modification, the rabbitmq pressure is mainly on node01, and the other two nodes will hardly process message requests.</p>
<p>③ Turn off the rate mode function of rabbitmq monitoring. The rate mode function monitors the message output rate in the message queue. Closing has no effect on services.</p>
<p>④ [glance] api_servers=vip:9292 configured before nova-compute, vip is the address of the management network, when creating virtual machines and other concurrency, there will be image downloads occupying the management network channel, causing network sensitive service messages such as rabbitmq to be blocked, and even messages Timeout, therefore, configure its api_servers as the storage address of the control node, and change the mirror service of the control node from monitoring the management network to monitoring the storage network.</p>
<p>After taking the above measures, a test was carried out</p>
<ol>
<li>Tested that 100 virtual machines were created concurrently, and none of the virtual machines failed.</li>
<li>Tested to create 100 volumes concurrently, none of them failed</li>
</ol>
<h1 id="a-control-node-memory-usage-is-too-high-alarm"><a class="header" href="#a-control-node-memory-usage-is-too-high-alarm">A control node memory usage is too high alarm</a></h1>
<p>A control node has a high memory usage alarm. It is found that the rabbitmq process is abnormal. The backlog of messages in the message queue causes the memory to increase and cannot be released. Restart the rabbitmq process to solve the problem. To solve the problem, the rabbitmq configuration file needs to be modified to make the backlog The messages are stored on disk rather than in memory.</p>
<h1 id="the-resize-operation-on-the-cloud-host-is-not-successful"><a class="header" href="#the-resize-operation-on-the-cloud-host-is-not-successful">The resize operation on the cloud host is not successful</a></h1>
<p>Resize a vm, that is, change from a small flavor to a large flavor, without success</p>
<p>Check the nova-compute.log of the node where the cloud host is located</p>
<pre><code>2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher   File &quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 6959, in _error_out_instance_on_exception
2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher     raise error.inner_exception
2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher ResizeError: Resize error: not able to execute ssh command: Unexpected error while running command.
2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher Command: ssh 172.16.231.26 -p 22 mkdir -p /var/lib/nova/instances/54091e90-55f5-4f4d-8f66-31fbc787584f
2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher Exit code: 255
2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher Stdout: u''
2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher Stderr: u'@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\r\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\r\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\r\nIt is also possible that a host key has just been changed.\r\nThe fingerprint for the ECDSA key sent by the remote host is\nc8:0b:98:9d:8d:d8:14:89:e6:fe:97:66:22:4e:57:13.\r\nPlease contact your system administrator.\r\nAdd correct host key in /var/lib/nova/.ssh/known_hosts to get rid of this message.\r\nOffending ECDSA key in /var/lib/nova/.ssh/known_hosts:34\r\nPassword authentication is disabled to avoid man-in-the-middle attacks.\r\nKeyboard-interactive authentication is disabled to avoid man-in-the-middle attacks.\r\nPermission denied (publickey,gssapi-keyex,gssapi-with-mic,password).\r\n'
</code></pre>
<p>Analyze the reasons</p>
<p>It may be that the host key of the nova user of the computing node has changed</p>
<p>Countermeasures</p>
<p>For all computing nodes, delete /var/lib/nova/.ssh/known_hosts rows related to computing nodes, log in to each other with nova user ssh again, add a host key and ensure that ssh between nova user computing nodes is mutual trust</p>
<h1 id="after-the-ceilometer-is-installed-start-the-service-and-the-status-is-wrong"><a class="header" href="#after-the-ceilometer-is-installed-start-the-service-and-the-status-is-wrong">After the ceilometer is installed, start the service and the status is wrong</a></h1>
<pre><code>[root@NFJD-TESTN-COMPUTE-2 ceilometer-2015.1.5]# systemctl restart openstack-ceilometer-compute.service
[root@NFJD-TESTN-COMPUTE-2 ceilometer-2015.1.5]# systemctl status openstack-ceilometer-compute.service
● openstack-ceilometer-compute.service - OpenStack ceilometer compute agent
Loaded: loaded (/usr/lib/systemd/system/openstack-ceilometer-compute.service; enabled; vendor preset: disabled)
Active: failed (Result: exit-code) since Fri 2016-12-23 09:39:11 CST; 2s ago
Process: 8420 ExecStart=/usr/bin/ceilometer-agent-compute --logfile /var/log/ceilometer/compute.log (code=exited, status=1/FAILURE)
Main PID: 8420 (code=exited, status=1/FAILURE)

Dec 23 09:39:11 NFJD-TESTN-COMPUTE-2 ceilometer-agent-compute[8420]: File &quot;/usr/lib/python2.7/site-packages/ceilometer/cmd/polling.py&quot;, line 82, in main_compute
Dec 23 09:39:11 NFJD-TESTN-COMPUTE-2 ceilometer-agent-compute[8420]: service.prepare_service()
Dec 23 09:39:11 NFJD-TESTN-COMPUTE-2 ceilometer-agent-compute[8420]: File &quot;/usr/lib/python2.7/site-packages/ceilometer/service.py&quot;, line 117, in prepare_service
Dec 23 09:39:11 NFJD-TESTN-COMPUTE-2 ceilometer-agent-compute[8420]: cfg.CONF(argv[1:], project='ceilometer', validate_default_values=True)
Dec 23 09:39:11 NFJD-TESTN-COMPUTE-2 ceilometer-agent-compute[8420]: File &quot;/usr/lib/python2.7/site-packages/oslo_config/cfg.py&quot;, line 1860, in __call__
Dec 23 09:39:11 NFJD-TESTN-COMPUTE-2 ceilometer-agent-compute[8420]: self._namespace._files_permission_denied)
Dec 23 09:39:11 NFJD-TESTN-COMPUTE-2 ceilometer-agent-compute[8420]: oslo_config.cfg.ConfigFilesPermissionDeniedError: Failed to open some config files: /etc/ceilometer/ceilometer.conf
Dec 23 09:39:11 NFJD-TESTN-COMPUTE-2 systemd[1]: openstack-ceilometer-compute.service: main process exited, code=exited, status=1/FAILURE
Dec 23 09:39:11 NFJD-TESTN-COMPUTE-2 systemd[1]: Unit openstack-ceilometer-compute.service entered failed state.
Dec 23 09:39:11 NFJD-TESTN-COMPUTE-2 systemd[1]: openstack-ceilometer-compute.service failed.
</code></pre>
<p>The conf file has no permission, modify the permission, and the status is ok. There is no permission because the root user scp the configuration file from another available computing node.</p>
<pre><code>[root@NFJD-TESTN-COMPUTE-2 ceilometer-2015.1.5]# chown -R ceilometer:ceilometer /etc/ceilometer
[root@NFJD-TESTN-COMPUTE-2 ceilometer]# systemctl restart openstack-ceilometer-compute.service
[root@NFJD-TESTN-COMPUTE-2 ceilometer]# systemctl status openstack-ceilometer-compute.service
● openstack-ceilometer-compute.service - OpenStack ceilometer compute agent
Loaded: loaded (/usr/lib/systemd/system/openstack-ceilometer-compute.service; enabled; vendor preset: disabled)
Active: active (running) since Fri 2016-12-23 09:42:32 CST; 2s ago
Main PID: 9459 (ceilometer-agen)
CGroup: /system.slice/openstack-ceilometer-compute.service
        └─9459 /usr/bin/python /usr/bin/ceilometer-agent-compute --logfile /var/log/ceilometer/compute.log

Dec 23 09:42:32 NFJD-TESTN-COMPUTE-2 systemd[1]: Started OpenStack ceilometer compute agent.
Dec 23 09:42:32 NFJD-TESTN-COMPUTE-2 systemd[1]: Starting OpenStack ceilometer compute agent...
</code></pre>
<blockquote>
<p>file permission problem, if the configuration file has no configuration file permission after replacement, for example, the file owner who was originally root:nova is replaced by root:root, there will be problems that the service cannot run normally.</p>
</blockquote>
<h1 id="the-version-is-too-new-to-cause-an-error-in-referencing-the-package"><a class="header" href="#the-version-is-too-new-to-cause-an-error-in-referencing-the-package">The version is too new to cause an error in referencing the package</a></h1>
<pre><code>| fault                                | {&quot;message&quot;: &quot;Build of instance 15d9db88-d0a9-40a8-83e9-9ede3001b112 was re-scheduled: 'module' object has no attribute 'to_utf8'&quot;, &quot;code&quot;: 500, &quot;details&quot;: &quot;  File \&quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py\&quot;, line 2258, in _do_build_and_run_instance |


2017-05-25 11:30:34.529 32687 TRACE nova.compute.manager [instance: 15d9db88-d0a9-40a8-83e9-9ede3001b112]   File &quot;/usr/lib/python2.7/site-packages/nova/utils.py&quot;, line 207, in execute
2017-05-25 11:30:34.529 32687 TRACE nova.compute.manager [instance: 15d9db88-d0a9-40a8-83e9-9ede3001b112]     return processutils.execute(*cmd, **kwargs)
2017-05-25 11:30:34.529 32687 TRACE nova.compute.manager [instance: 15d9db88-d0a9-40a8-83e9-9ede3001b112]   File &quot;/usr/lib64/python2.7/site-packages/oslo_concurrency/processutils.py&quot;, line 287, in execute
2017-05-25 11:30:34.529 32687 TRACE nova.compute.manager [instance: 15d9db88-d0a9-40a8-83e9-9ede3001b112]     process_input = encodeutils.to_utf8(process_input)
2017-05-25 11:30:34.529 32687 TRACE nova.compute.manager [instance: 15d9db88-d0a9-40a8-83e9-9ede3001b112] AttributeError: 'module' object has no attribute 'to_utf8'
2017-05-25 11:30:34.529 32687 TRACE nova.compute.manager [instance: 15d9db88-d0a9-40a8-83e9-9ede3001b112]
2017-05-25 11:30:34.531 32687 INFO nova.compute.manager [req-22c24896-bcad-4464-a2ab-c019c45ac0e3 2a2a6d4700034e53a198dac0a71aab6e fe3e50d5fc274d29948ff7fd46b214dc - - -] [instance: 15d9db88-d0a9-40a8-83e9-9ede3001b112] Terminating instance


[root@compute-power ~]# vim /usr/lib64/python2.7/site-packages/oslo_concurrency/processutils.py
284     cwd = kwargs.pop('cwd', None)
285     process_input = kwargs.pop('process_input', None)
286     if process_input is not None:
287         process_input = encodeutils.to_utf8(process_input)


[root@compute-power ~]# pip show oslo.concurrency
Name: oslo.concurrency
Version: 3.20.0
Summary: Oslo Concurrency library
Home-page: http://docs.openstack.org/developer/oslo.concurrency
Author: OpenStack
Author-email: openstack-dev@lists.openstack.org
License: UNKNOWN
Location: /usr/lib64/python2.7/site-packages
Requires: pbr, oslo.config, fasteners, oslo.i18n, oslo.utils, six, enum34


[root@bc36-compute-3 oslo_concurrency]# pip show oslo.concurrency
---
Name: oslo.concurrency
Version: 1.8.2
Location: /usr/lib/python2.7/site-packages
Requires: 
...
</code></pre>
<p>The oslo.concurrency version is too new, so replace it with an old version to solve the problem.</p>
<h1 id="vm-error"><a class="header" href="#vm-error">vm error</a></h1>
<p>Error message:</p>
<pre><code>Flavor's disk is too small for requested image. Flavor disk is 16106127360 bytes, image is 21474836480 bytes.].
</code></pre>
<p>It means that the disk space of the Flavor (cloud host type) used when creating the vm does not meet the image mirroring requirements!</p>
<h1 id="no-valid-host-was-found"><a class="header" href="#no-valid-host-was-found">No valid host was found</a></h1>
<p>2016-11-01 01:28:38.889 51843 WARNING nova.scheduler.utils [req-9eb2b8ec-216b-4073-95bd-1fbb51844faf 52ba7917bb284af7ad6ac313b7e8e948 0cd3632df93d48d6b2c24c67f70e56b8 - - -] Failed to compute_task_build_instances: No valid host was found. There are not enough hosts available.</p>
<p>The main reasons for this problem are:</p>
<ul>
<li>It is caused by insufficient memory, insufficient CPU resources, and insufficient hard disk space resources of the computing node; if the type and specification of the cloud host are reduced, it can be found that the creation will be successful.</li>
<li>The network configuration is incorrect, resulting in the failure to obtain the ip when creating a virtual machine; the network is unreachable or caused by a firewall.</li>
<li>The openstack-nova-compute service status problem. You can try to restart the nova-related services of the control node and the openstack-nova-compute service of the computing node; check the nova.conf configuration of the control node and computing node for improper configuration.</li>
<li>There are many reasons for this error report. For details, check the detailed analysis of the logs under /var/log/nova. The focus is nova-compute.log, nova-conductor.log logs</li>
</ul>
<h1 id="in-the-logs-of-nova-scheduler-and-nova-compute-you-can-see-the-error-message-imagenotfound-image-37aaedc7-6fe6-4fc8-b110-408d166b8e51-could-not-be-found"><a class="header" href="#in-the-logs-of-nova-scheduler-and-nova-compute-you-can-see-the-error-message-imagenotfound-image-37aaedc7-6fe6-4fc8-b110-408d166b8e51-could-not-be-found">In the logs of nova-scheduler and nova-compute, you can see the error message &quot;ImageNotFound: Image 37aaedc7-6fe6-4fc8-b110-408d166b8e51 could not be found&quot;!</a></h1>
<pre><code>root@controller ~]# /etc/init.d/openstack-glance-api status
openstack-glance-api (pid  2222) is running...
[root@controller ~]# /etc/init.d/openstack-glance-registry status
openstack-glance-registry (pid  2694) is running...
</code></pre>
<p>normal status</p>
<pre><code>[root@controller ~]# glance image-list
+--------------------------------------+---------------+-------------+------------------+-------------+--------+
| ID                                   | Name          | Disk Format | Container Format | Size        | Status |
+--------------------------------------+---------------+-------------+------------------+-------------+--------+
| 37aaedc7-6fe6-4fc8-b110-408d166b8e51 | cirrors       | qcow2       | bare             | 13200896    | active |
+--------------------------------------+---------------+-------------+------------------+-------------+--------+
</code></pre>
<p>It works normally, try to upload a mirror image, and you can also create a vm normally, what is the reason? ?</p>
<p>Because in the operation and maintenance process, the default path of the modified glance is changed from /var/lib/glance/images to /data1/glance, and the images under /var/lib/glance/images are mv to /data1/glance At this time, although the data has passed before, the metadata information of the image is firmly recorded in the image_locations table of glance. Check it out:</p>
<pre><code>mysql&gt; select * from glance.image_locations where image_id='37aaedc7-6fe6-4fc8-b110-408d166b8e51'\G;                                                                        
*************************** 1. row ***************************
        id: 37
image_id: 37aaedc7-6fe6-4fc8-b110-408d166b8e51
    value: file:///var/lib/glance/images/37aaedc7-6fe6-4fc8-b110-408d166b8e51    #元凶
created_at: 2015-12-21 06:10:24
updated_at: 2015-12-21 06:10:24
deleted_at: NULL
deleted: 0
meta_data: {}
    status: active
1 row in set (0.00 sec)
</code></pre>
<p>Real image: The images in the original directory /var/lib/glance/images have been mv'd to /data1/glance, but the original path content is still recorded in the database. Therefore, a problem arises: when nova When trying to start an instance, nova will go to the instance image cache path, and check whether there is the image in /var/lib/nova/_base by default, and if not, send a result api request to glance to request to download the image of the specified image to the local , glance looks for the image according to the value defined by image_locations in the database, which leads to failure!
Solution: update the metadata information of glance</p>
<pre><code>mysql&gt; update glance.image_locations set value='file:///data1/glance/37aaedc7-6fe6-4fc8-b110-408d166b8e51' where image_id='37aaedc7-6fe6-4fc8-b110-408d166b8e51'\G;             
Query OK, 1 row affected (0.05 sec)
Rows matched: 1  Changed: 1  Warnings: 0
</code></pre>
<p>Rebuild the virtual machine, the problem is solved! ! !</p>
<h1 id="use-config-drive-to-set-ip-resulting-in-ip-conflict-cannot-create-vm"><a class="header" href="#use-config-drive-to-set-ip-resulting-in-ip-conflict-cannot-create-vm">Use config drive to set ip, resulting in ip conflict cannot create vm</a></h1>
<pre><code>2017-06-22 11:58:50.819 18449 INFO nova.virt.libvirt.driver [req-4edc310f-8382-43ce-a021-8f04c55d5a51 25380a8cde624ae2998224074d4092ee 1cca7b8b0e0d47cb8d6eda06837960ed - - -] [instance: fb28867f-da0f-48f9-b8f8-33704c845117] Using config drive
...
2017-06-22 11:59:16.686 18449 TRACE nova.compute.manager [instance: fb28867f-da0f-48f9-b8f8-33704c845117] FixedIpAlreadyInUse: Fixed IP 172.16.1.4 is already in use.
2017-06-22 11:59:16.686 18449 TRACE nova.compute.manager [instance: fb28867f-da0f-48f9-b8f8-33704c845117]
2017-06-22 11:59:16.690 18449 INFO nova.compute.manager [req-4edc310f-8382-43ce-a021-8f04c55d5a51 25380a8cde624ae2998224074d4092ee 1cca7b8b0e0d47cb8d6eda06837960ed - - -] [instance: fb28867f-da0f-48f9-b8f8-33704c845117] Terminating instance
2017-06-22 11:59:16.698 18449 INFO nova.virt.libvirt.driver [-] [instance: fb28867f-da0f-48f9-b8f8-33704c845117] During wait destroy, instance disappeared.
2017-06-22 11:59:16.708 18449 INFO nova.virt.libvirt.driver [req-4edc310f-8382-43ce-a021-8f04c55d5a51 25380a8cde624ae2998224074d4092ee 1cca7b8b0e0d47cb8d6eda06837960ed - - -] [instance: fb28867f-da0f-48f9-b8f8-33704c845117] Deleting instance files /var/lib/nova/instances/fb28867f-da0f-48f9-b8f8-33704c845117_del
2017-06-22 11:59:16.708 18449 INFO nova.virt.libvirt.driver [req-4edc310f-8382-43ce-a021-8f04c55d5a51 25380a8cde624ae2998224074d4092ee 1cca7b8b0e0d47cb8d6eda06837960ed - - -] [instance: fb28867f-da0f-48f9-b8f8-33704c845117] Deletion of /var/lib/nova/instances/fb28867f-da0f-48f9-b8f8-33704c845117_del complete
</code></pre>
<h1 id="failed-to-mount-and-unmount-volume"><a class="header" href="#failed-to-mount-and-unmount-volume">Failed to mount and unmount volume</a></h1>
<pre><code>2017-05-09 14:19:13.957 12778 TRACE oslo_messaging.rpc.dispatcher VolumeDeviceNotFound: Volume device not found at [u'/dev/disk/by-path/ip-10.11.23.132:3260-iscsi-iqn.2000-09.com.fujitsu:storage-system.eternus-dx400:00C0C1P0-lun-78', u'/dev/disk/by-path/ip-10.11.23.133:3260-iscsi-iqn.2000-09.com.fujitsu:storage-system.eternus-dx400:00C1C1P0-lun-78'].
</code></pre>
<p>Checked the route of the node that can be mounted normally, and found that the route of the compute-2 node was lost.</p>
<pre><code>10.11.23.128    10.11.231.1     255.255.255.128 UG    0      0        0 enp17s0f0.607
10.11.231.0     0.0.0.0         255.255.255.0   U     0      0        0 enp17s0f0.607
</code></pre>
<p>After adding the route, it can be successfully mounted and unmounted</p>
<pre><code>[root@NFJD-TESTN-COMPUTE-2 ~]# ip a|grep 607
[root@NFJD-TESTN-COMPUTE-2 ~]# ip link add link enp17s0f0 name enp17s0f0.607 type vlan id 607
[root@NFJD-TESTN-COMPUTE-2 ~]# ip link set up enp17s0f0.607
[root@NFJD-TESTN-COMPUTE-2 ~]# ifconfig enp17s0f0 0.0.0.0
[root@NFJD-TESTN-COMPUTE-2 ~]# ifconfig enp17s0f0.607 10.11.231.26 netmask 255.255.255.0
[root@NFJD-TESTN-COMPUTE-2 ~]# route add -net 10.11.23.128/25 gw 10.11.231.1 dev enp17s0f0.607
[root@NFJD-TESTN-COMPUTE-2 ~]# ip a|grep 607
27: enp17s0f0.607@enp17s0f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1600 qdisc noqueue state UP 
    inet 10.11.231.26/24 brd 10.11.231.255 scope global enp17s0f0.607
</code></pre>
<h1 id="failed-when-vm-mount-volume"><a class="header" href="#failed-when-vm-mount-volume">Failed when vm mount volume</a></h1>
<p>The nova-compute.log error of the nova compute node is as follows:</p>
<pre><code>2017-05-09 15:18:01.952 10888 ERROR nova.virt.block_device [req-344ed875-4bc4-40bf-95e3-a0ff48df6059 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: ddcf7976-b282-45ac-a167-9865178ed629] Driver failed to attach volume 88442395-87d9-41f1-b9d8-0df4b941846a at /dev/vdc
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] Traceback (most recent call last):
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] File &quot;/usr/lib/python2.7/site-packages/nova/virt/block_device.py&quot;, line 276, in attach
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] device_type=self['device_type'], encryption=encryption)
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 1210, in attach_volume
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] self._connect_volume(connection_info, disk_info)
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 1157, in _connect_volume
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] driver.connect_volume(connection_info, disk_info)
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] File &quot;/usr/lib/python2.7/site-packages/oslo_concurrency/lockutils.py&quot;, line 445, in inner
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] return f(*args, **kwargs)
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/volume.py&quot;, line 505, in connect_volume
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] if self.use_multipath:
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/volume.py&quot;, line 505, in connect_volume
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] if self.use_multipath:
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] File &quot;/usr/lib64/python2.7/bdb.py&quot;, line 49, in trace_dispatch
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] return self.dispatch_line(frame)
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] File &quot;/usr/lib64/python2.7/bdb.py&quot;, line 68, in dispatch_line
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] if self.quitting: raise BdbQuit
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629] BdbQuit
2017-05-09 15:18:01.952 10888 TRACE nova.virt.block_device [instance: ddcf7976-b282-45ac-a167-9865178ed629]
</code></pre>
<p>Maybe someone added a breakpoint when debugging the code, and forgot to delete the breakpoint
to find it in the code. Sure enough, delete the breakpoint, restart the service, and the test can mount the volume</p>
<h1 id="a-host-doesnt-support-passthrough-error-occurs-when-creating-a-vm"><a class="header" href="#a-host-doesnt-support-passthrough-error-occurs-when-creating-a-vm">A host doesn't support passthrough error occurs when creating a vm</a></h1>
<pre><code>| fault                                | {&quot;message&quot;: &quot;unsupported configuration: host doesn't support passthrough of host PCI devices&quot;, &quot;code&quot;: 500, &quot;details&quot;: &quot;  File \&quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py\&quot;, line 1856, in _do_build_and_run_instance |
|                                      |     filter_properties) 
</code></pre>
<p>This error may be caused by the parent VT-d (or IOMMU) not being enabled.
Make sure the &quot;intel_iommu=on&quot; boot parameter has been enabled as described above.</p>
<p>Found that the /etc/default/grub file has been modified</p>
<p>Configure the /etc/default/grub file of the computing node, add intel_iommu=on to GRUB_CMDLINE_LINUX to activate the VT-d function, and restart the physical machine</p>
<pre><code>$ cat /etc/default/grub
GRUB_TIMEOUT=5
GRUB_DISTRIBUTOR=&quot;$(sed 's, release .*$,,g' /etc/system-release)&quot;
GRUB_DEFAULT=saved
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT=&quot;console&quot;
GRUB_CMDLINE_LINUX=&quot;crashkernel=auto rd.lvm.lv=bclinux/root rd.lvm.lv=bclinux/swap intel_iommu=on rhgb quiet&quot;
GRUB_DISABLE_RECOVERY=&quot;true&quot;
</code></pre>
<p>But there is no restart, no restart will not take effect</p>
<p>After restarting, the sriov virtual machine can be generated normally</p>
<p>For intel cpu and amd cpu, the grub configuration is different, please refer to the article for specific configuration: http://pve.proxmox.com/wiki/Pci_passthrough</p>
<p>update-grub</p>
<p>After editing the grub file, you need to update</p>
<pre><code>grub2-mkconfig   # fedora arch centos
update-grub            # ubuntu debian
</code></pre>
<p>Restart the computer for it to take effect</p>
<pre><code># cat /proc/cmdline
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2018-08-26 18:38</p>
<h1 id="troubleshooting-process"><a class="header" href="#troubleshooting-process">Troubleshooting process</a></h1>
<ul>
<li>First determine the resource ID of the fault, and determine the component where the fault occurred</li>
<li>Check the log of the corresponding component, search according to the fault resource ID, and find the corresponding ERROR log</li>
<li>If the ERROR log points the problem to other components, then according to the resource ID, time, req-id and other information in the ERROR log, other components continue to search for the problem until the root cause of the problem is found.</li>
<li>If no ERROR is found in the log, it may be that the network is not connected, causing the request to fail to reach the API. At this time, you need to check the connectivity with the API (if you use VIP, you need to check the connectivity with the VIP and the connectivity of the real IP separately. ).</li>
<li>If the corresponding request can be found in the API, but the conductor/scheduler/compute does not find the corresponding log, it may be that the MQ is faulty.</li>
<li>If the component has not refreshed any logs for a long time, the component process may hang or be in a dead state. You can try to restart the service, or open Debug first and then restart the service. </li>
</ul>
<h1 id="create-vm-error"><a class="header" href="#create-vm-error">create vm error</a></h1>
<pre><code>[root@EXTENV-194-18-2-16 nova]# cat nova-compute.log | grep 620cd801-8849-481a-80e0-2980b6c8dba6
2018-08-23 15:23:36.136 3558 INFO nova.compute.resource_tracker [req-f76d5408-00f8-4a67-854e-ad3da2098811 - - - - -] Instance 620cd801-8849-481a-80e0-2980b6c8dba6 has allocations against this compute host but is not found in the database.
</code></pre>
<p>Analysis: It feels that the information database of node is out of sync</p>
<p>nova show error vm, package cell error</p>
<pre><code>#### Each time a computing node is added, the control node needs to execute:
# su -s /bin/sh -c &quot;nova-manage cell_v2 discover_hosts --verbose&quot; nova
</code></pre>
<p>problem solved.</p>
<h1 id="the-neutron-service-is-good-and-the-command-line-creates-a-network-to-view-the-network-normally-but-the-dashboard-cannot-view-network-related-pages"><a class="header" href="#the-neutron-service-is-good-and-the-command-line-creates-a-network-to-view-the-network-normally-but-the-dashboard-cannot-view-network-related-pages">The neutron service is good, and the command line creates a network to view the network normally, but the dashboard cannot view network-related pages</a></h1>
<p>The dashboard network page reports an error Invalid service catalog service: network</p>
<p>Analysis: It should be that Keystone is not properly configured. As a result, no relevant Catalog information was found.</p>
<pre><code>[root@EXTENV-194-18-2-11 ~]# openstack catalog list
+-----------+-----------+-----------------------------------------------+
| Name      | Type      | Endpoints                                     |
+-----------+-----------+-----------------------------------------------+
| placement | placement | RegionOne                                     |
|           |           |   internal: http://nova-ha-vip:8778           |
|           |           | RegionOne                                     |
|           |           |   admin: http://nova-ha-vip:8778              |
|           |           | RegionOne                                     |
|           |           |   public: http://nova-ha-vip:8778             |
|           |           |                                               |
| keystone  | identity  | RegionOne                                     |
|           |           |   public: http://keystone-ha-vip:5000/v3/     |
|           |           | RegionOne                                     |
|           |           |   internal: http://keystone-ha-vip:35357/v3/  |
|           |           | RegionOne                                     |
|           |           |   admin: http://keystone-ha-vip:35357/v3/     |
|           |           |                                               |
| glance    | image     | RegionOne                                     |
|           |           |   admin: http://glance-ha-vip:9292            |
|           |           | RegionOne                                     |
|           |           |   internal: http://glance-ha-vip:9292         |
|           |           | RegionOne                                     |
|           |           |   public: http://glance-ha-vip:9292           |
|           |           |                                               |
| nova      | compute   | RegionOne                                     |
|           |           |   public: http://nova-ha-vip:8774/v2.1        |
|           |           | RegionOne                                     |
|           |           |   admin: http://nova-ha-vip:8774/v2.1         |
|           |           | RegionOne                                     |
|           |           |   internal: http://nova-ha-vip:8774/v2.1      |
|           |           |                                               |
| neutron   | network   |                                               |
| neutron   | network   | RegionOne                                     |
|           |           |   public: http://neutron-server-ha-vip:9696   |
|           |           | RegionOne                                     |
|           |           |   admin: http://neutron-server-ha-vip:9696    |
|           |           | RegionOne                                     |
|           |           |   internal: http://neutron-server-ha-vip:9696 |
|           |           |                                               |
+-----------+-----------+-----------------------------------------------+
</code></pre>
<p>So just delete the first piece of data without url, but found that there is only openstack catalog list, no openstack catalog delete command. Later, I checked the keystone configuration file keystone.conf and found the following configuration
see [catalog]</p>
<p>From the configuration file and some information, it can be seen that the catalog is the data read from mysql, and then the dirty data is found in the service table in the keystone library of mysql, and then I know to use openstack service delete to delete the 'dirty data', the problem is solved.</p>
<pre><code>MariaDB [keystone]&gt; select * from service;
+----------------------------------+-----------+---------+-------------------------------------------------------------+
| id                               | type      | enabled | extra                                                       |
+----------------------------------+-----------+---------+-------------------------------------------------------------+
| 520f6bf8564240be9678c4ef25305cad | placement |       1 | {&quot;description&quot;: &quot;OpenStack Placement&quot;, &quot;name&quot;: &quot;placement&quot;} |
| 960580852a594c078e68fe3683e35db5 | identity  |       1 | {&quot;name&quot;: &quot;keystone&quot;}                                        |
| 98ed18fcd8104732919bb5869a5a6dc2 | image     |       1 | {&quot;description&quot;: &quot;OpenStack Image&quot;, &quot;name&quot;: &quot;glance&quot;}        |
| abef1b9469d94d3ab9f27c8ed72a5a48 | compute   |       1 | {&quot;description&quot;: &quot;OpenStack Compute&quot;, &quot;name&quot;: &quot;nova&quot;}        |
| e37085e8fb2a49c0921c2d24f5e4f9b5 | network   |       1 | {&quot;description&quot;: &quot;OpenStack Networking&quot;, &quot;name&quot;: &quot;neutron&quot;}  |
| f1b661407ce04f79bc24605fa59bb74c | network   |       1 | {&quot;description&quot;: &quot;OpenStack Networking&quot;, &quot;name&quot;: &quot;neutron&quot;}  |
+----------------------------------+-----------+---------+-------------------------------------------------------------+
6 rows in set (0.00 sec)

MariaDB [keystone]&gt; select * from endpoint;
+----------------------------------+--------------------+-----------+----------------------------------+-----------------------------------+-------+---------+-----------+
| id                               | legacy_endpoint_id | interface | service_id                       | url                               | extra | enabled | region_id |
+----------------------------------+--------------------+-----------+----------------------------------+-----------------------------------+-------+---------+-----------+
| 142cb619cd2242828b0c9394d5baaea1 | NULL               | public    | f1b661407ce04f79bc24605fa59bb74c | http://neutron-server-ha-vip:9696 | {}    |       1 | RegionOne |
| 2252d3ef840b4c5aa1184ebe8d6094f1 | NULL               | public    | abef1b9469d94d3ab9f27c8ed72a5a48 | http://nova-ha-vip:8774/v2.1      | {}    |       1 | RegionOne |
| 476654c6e7dd4d22b290de451e3afda0 | NULL               | admin     | abef1b9469d94d3ab9f27c8ed72a5a48 | http://nova-ha-vip:8774/v2.1      | {}    |       1 | RegionOne |
| 562a5d5443af4dfab6760204d0adf3bf | NULL               | internal  | 520f6bf8564240be9678c4ef25305cad | http://nova-ha-vip:8778           | {}    |       1 | RegionOne |
| 58bd5f09811a4ebcb62a4b51fb7ae444 | NULL               | admin     | f1b661407ce04f79bc24605fa59bb74c | http://neutron-server-ha-vip:9696 | {}    |       1 | RegionOne |
| 600811f8ccaf42669d4d83b897af3933 | NULL               | admin     | 520f6bf8564240be9678c4ef25305cad | http://nova-ha-vip:8778           | {}    |       1 | RegionOne |
| 80683f619efb41dcbb6796ea04f16159 | NULL               | internal  | f1b661407ce04f79bc24605fa59bb74c | http://neutron-server-ha-vip:9696 | {}    |       1 | RegionOne |
| 8e0a684607294a729f87d7d8b1a639ca | NULL               | public    | 520f6bf8564240be9678c4ef25305cad | http://nova-ha-vip:8778           | {}    |       1 | RegionOne |
| 9ef0f18d891e45608ffc41985dc6afa6 | NULL               | public    | 960580852a594c078e68fe3683e35db5 | http://keystone-ha-vip:5000/v3/   | {}    |       1 | RegionOne |
| a0b10cb04a5b4ca3859aaf2ea4ca2a3b | NULL               | admin     | 98ed18fcd8104732919bb5869a5a6dc2 | http://glance-ha-vip:9292         | {}    |       1 | RegionOne |
| c53979becccc44f1813e9f50a619af7e | NULL               | internal  | 960580852a594c078e68fe3683e35db5 | http://keystone-ha-vip:35357/v3/  | {}    |       1 | RegionOne |
| dadbb8dc218245bbba8c9a34237413ec | NULL               | internal  | 98ed18fcd8104732919bb5869a5a6dc2 | http://glance-ha-vip:9292         | {}    |       1 | RegionOne |
| f4034b8c086a451caed52ac51a761fb0 | NULL               | public    | 98ed18fcd8104732919bb5869a5a6dc2 | http://glance-ha-vip:9292         | {}    |       1 | RegionOne |
| fc150884825544baaf4912f14e76f51a | NULL               | internal  | abef1b9469d94d3ab9f27c8ed72a5a48 | http://nova-ha-vip:8774/v2.1      | {}    |       1 | RegionOne |
| fc7132052063438895674fd7b840db68 | NULL               | admin     | 960580852a594c078e68fe3683e35db5 | http://keystone-ha-vip:35357/v3/  | {}    |       1 | RegionOne |
+----------------------------------+--------------------+-----------+----------------------------------+-----------------------------------+-------+---------+-----------+
15 rows in set (0.00 sec)

[root@EXTENV-194-18-2-11 ~]#  openstack service list
+----------------------------------+-----------+-----------+
| ID                               | Name      | Type      |
+----------------------------------+-----------+-----------+
| 520f6bf8564240be9678c4ef25305cad | placement | placement |
| 960580852a594c078e68fe3683e35db5 | keystone  | identity  |
| 98ed18fcd8104732919bb5869a5a6dc2 | glance    | image     |
| abef1b9469d94d3ab9f27c8ed72a5a48 | nova      | compute   |
| e37085e8fb2a49c0921c2d24f5e4f9b5 | neutron   | network   |
| f1b661407ce04f79bc24605fa59bb74c | neutron   | network   |
+----------------------------------+-----------+-----------+
[root@EXTENV-194-18-2-11 ~]#  openstack service delete e37085e8fb2a49c0921c2d24f5e4f9b5
[root@EXTENV-194-18-2-11 ~]# systemctl restart httpd.service memcached.service
</code></pre>
<h1 id="when-viewing-the-image-named-in-chinese-an-error-is-reported"><a class="header" href="#when-viewing-the-image-named-in-chinese-an-error-is-reported">When viewing the image named in Chinese, an error is reported</a></h1>
<pre><code>[root@NFJD-TESTVM-CORE-API-1 ~]# glance image-list
'ascii' codec can't encode character u'\u5982' in position 1242: ordinal not in range(128)
</code></pre>
<p>Analysis: The image name is named in Chinese.</p>
<p>Add export LC_ALL=zh_CN.UTF-8</p>
<p>to /etc/profile</p>
<p>At the same time, pay attention to whether there is export LC_ALL=C in the source file</p>
<h1 id="vm-build-failed"><a class="header" href="#vm-build-failed">vm build failed</a></h1>
<pre><code>2017-05-25 11:01:29.577 21880 TRACE nova.compute.manager [instance: 2c5a8e62-62d0-430d-8747-795350bb6939] ProcessExecutionError: Unexpected error while running command.
2017-05-25 11:01:29.577 21880 TRACE nova.compute.manager [instance: 2c5a8e62-62d0-430d-8747-795350bb6939] Command: qemu-img convert -O raw /var/lib/nova/instances/_base/f5797db00aacfbe240bbfb0f53c2da80e4be6dfc.part /var/lib/nova/instances/_base/f5797db00aacfbe240bbfb0f53c2da80e4be6dfc.converted
2017-05-25 11:01:29.577 21880 TRACE nova.compute.manager [instance: 2c5a8e62-62d0-430d-8747-795350bb6939] Exit code: 1
2017-05-25 11:01:29.577 21880 TRACE nova.compute.manager [instance: 2c5a8e62-62d0-430d-8747-795350bb6939] Stdout: u''
2017-05-25 11:01:29.577 21880 TRACE nova.compute.manager [instance: 2c5a8e62-62d0-430d-8747-795350bb6939] Stderr: u'qemu-img: error while writing sector 1569792: No space left on device\n'
2017-05-25 11:01:29.577 21880 TRACE nova.compute.manager [instance: 2c5a8e62-62d0-430d-8747-795350bb6939]
2017-05-25 11:01:29.580 21880 INFO nova.compute.manager [req-9fa74abf-bcc1-4b7e-aaef-2e17b593a356 6aa5df16b47442c58efde791abd60497 66458b9ead64409fb9d2e0f2c6d67d39 - - -] [instance: 2c5a8e62-62d0-430d-8747-795350bb6939] Terminating instance


# df -h
</code></pre>
<p>Found that the disk is running out</p>
<p>Countermeasure: clean up the disk</p>
<h1 id="evacuation-failed"><a class="header" href="#evacuation-failed">Evacuation failed</a></h1>
<p>Executed a nova evacuation command, but the evacuation failed, nova show reported this error: the state of the shared storage is wrong. That is, the nova instances directory does not have shared storage. If the ceph shared storage is not configured, the shared storage parameters are still included when the nova evacuate command is executed.</p>
<pre><code>| fault                                | {&quot;message&quot;: &quot;Invalid state of instance files on shared storage&quot;, &quot;code&quot;: 500, &quot;details&quot;: &quot;  File \&quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py\&quot;, line 354, in decorated_function |
|                                      |     return function(self, context, *args, **kwargs)                                                                                                                                            |
|                                      |   File \&quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py\&quot;, line 3031, in rebuild_instance                                                                                            |
|                                      |     _(\&quot;Invalid state of instance files on shared\&quot;                                                                                                                                            |
|                                      | &quot;, &quot;created&quot;: &quot;2017-05-17T01:25:17Z&quot;}       
</code></pre>
<h1 id="there-is-a-certain-probability-that-the-console-cannot-be-displayed"><a class="header" href="#there-is-a-certain-probability-that-the-console-cannot-be-displayed">There is a certain probability that the console cannot be displayed</a></h1>
<p>Operation steps: console-virtual machine, click the name of the virtual machine, click [console]</p>
<p>Expected result: display the console page normally</p>
<p>Actual result: There is a certain probability that the page will prompt &quot;Failed to connect to server&quot;, click to open in a new window to open the console page</p>
<p>The configuration problem of the nova control node, the hosts configuration of memcache and rabbitmq in the configuration file is incorrect</p>
<h1 id="on-the-dashboard-interface-use-the-image-centos7-flavor-2-to-create-a-virtual-machine-error-and-the-error-thrown-is-no-host-that-is-no-suitable-scheduling-node-was-found"><a class="header" href="#on-the-dashboard-interface-use-the-image-centos7-flavor-2-to-create-a-virtual-machine-error-and-the-error-thrown-is-no-host-that-is-no-suitable-scheduling-node-was-found">On the dashboard interface, use the image centos7, flavor-2 to create a virtual machine error. And the error thrown is no host, that is, no suitable scheduling node was found.</a></h1>
<p>If the size of the flavor is smaller than the image requirement, an error will be reported.</p>
<p>But once again, the above conditions are met and an error is reported.</p>
<p>It is possible that the flavor was created with illegal extra_specs</p>
<pre><code>OS-FLV-DISABLED:disabled  False
OS-FLV-EXT-DATA:ephemeral   0
disk    20
extra_specs {&quot;xxx&quot;: &quot;123&quot;, &quot;yyy&quot;: &quot;321&quot;}
</code></pre>
<p>The filtering option turns on the ComputeCapabilitiesFilter filter.</p>
<h1 id="import-error"><a class="header" href="#import-error">import error</a></h1>
<p>glance register.log error:</p>
<pre><code>2017-05-08 03:18:55.890 3185 ERROR glance.common.config [-] Unable to load glance-registry-keystone from configuration file /usr/share/glance/glance-registry-dist-paste.ini.
Got: ImportError('No module named simplegeneric',)
</code></pre>
<p>/usr/lib/python2.7/site-packages/simplegeneric.py has no read permission, who changed it</p>
<h1 id="keystone-error"><a class="header" href="#keystone-error">keystone error</a></h1>
<p>Permission denied: AH00072: make_sock: could not bind to address [::]:5000</p>
<pre><code>[root@controller0 ~]# systemctl start httpd.service
Job for httpd.service failed because the control process exited with error code. See &quot;systemctl status httpd.service&quot; and &quot;journalctl -xe&quot; for details.
[root@controller0 ~]# systemctl status httpd.service
● httpd.service - The Apache HTTP Server
Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled)
Active: failed (Result: exit-code) since Sat 2016-05-28 20:22:34 EDT; 11s ago
    Docs: man:httpd(8)
        man:apachectl(8)
Process: 4501 ExecStop=/bin/kill -WINCH ${MAINPID} (code=exited, status=1/FAILURE)
Process: 4499 ExecStart=/usr/sbin/httpd $OPTIONS -DFOREGROUND (code=exited, status=1/FAILURE)
Main PID: 4499 (code=exited, status=1/FAILURE)

May 28 20:22:34 controller0 httpd[4499]: (13)Permission denied: AH00072: make_sock: could not bind to address [::]:5000
May 28 20:22:34 controller0 httpd[4499]: (13)Permission denied: AH00072: make_sock: could not bind to address 0.0.0.0:5000
</code></pre>
<p>It may be a firewall configuration problem or selinux. If there is no problem with the firewall, check selinux</p>
<p>check selinux status:
1 [root@controller0 ~]# getenforce
2 enforcing #If it is not disabled, it means that selinux is running normally</p>
<p>SELINUX=enforcing changed to selinux=distabled</p>
<p>restart reboot</p>
<h1 id="re-scheduled-not-authorized-for-image"><a class="header" href="#re-scheduled-not-authorized-for-image">re-scheduled: Not authorized for image</a></h1>
<p>The CLI reports an error:</p>
<pre><code>| fault                                | {&quot;message&quot;: &quot;Build of instance d5739cf7-9830-47fd-9a75-e9b1cb4bb421 was re-scheduled: Not authorized for image dcd85799-92f6-4294-91ec-48670a218651.&quot;, &quot;code&quot;: 500, &quot;details&quot;: &quot;  File \&quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py\&quot;, line 2258, in _do_build_and_run_instance |
</code></pre>
<p>Logging in to the computing node also reports an error</p>
<pre><code>2017-05-18 15:01:24.867 40639 TRACE nova.compute.manager [instance: d5739cf7-9830-47fd-9a75-e9b1cb4bb421] ImageNotAuthorized: Not authorized for image dcd85799-92f6-4294-91ec-48670a218651.
2017-05-18 15:01:24.867 40639 TRACE nova.compute.manager [instance: d5739cf7-9830-47fd-9a75-e9b1cb4bb421]
</code></pre>
<p>So add</p>
<pre><code>[DEFAULT]
auth_strategy=keyston in /etc/nova/nova.conf
and the missing e
should be:
[DEFAULT]
auth_strategy=keystone
</code></pre>
<h1 id="libvirterror-unsupported-configuration-ide-controllers-are-unsupported-for-this-qemu-binary-or-machine-type"><a class="header" href="#libvirterror-unsupported-configuration-ide-controllers-are-unsupported-for-this-qemu-binary-or-machine-type">libvirtError: unsupported configuration: IDE controllers are unsupported for this QEMU binary or machine type</a></h1>
<pre><code>2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f] Traceback (most recent call last):
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]   File &quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 2483, in _build_resources
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]     yield resources
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]   File &quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 2355, in _build_and_run_instance
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]     block_device_info=block_device_info)
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 2704, in spawn
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]     block_device_info=block_device_info)
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 4758, in _create_domain_and_network
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]     power_on=power_on)
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 4689, in _create_domain
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]     LOG.error(err)
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]   File &quot;/usr/lib/python2.7/site-packages/oslo_utils/excutils.py&quot;, line 85, in __exit__
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]     six.reraise(self.type_, self.value, self.tb)
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]   File &quot;/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py&quot;, line 4679, in _create_domain
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]     domain.createWithFlags(launch_flags)
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]   File &quot;/usr/lib/python2.7/site-packages/eventlet/tpool.py&quot;, line 186, in doit
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]     result = proxy_call(self._autowrap, f, *args, **kwargs)
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]   File &quot;/usr/lib/python2.7/site-packages/eventlet/tpool.py&quot;, line 144, in proxy_call
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]     rv = execute(f, *args, **kwargs)
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]   File &quot;/usr/lib/python2.7/site-packages/eventlet/tpool.py&quot;, line 125, in execute
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]     six.reraise(c, e, tb)
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]   File &quot;/usr/lib/python2.7/site-packages/eventlet/tpool.py&quot;, line 83, in tworker
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]     rv = meth(*args, **kwargs)
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]   File &quot;/usr/lib64/python2.7/site-packages/libvirt.py&quot;, line 1065, in createWithFlags
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f]     if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed', dom=self)
2017-05-18 15:06:09.522 41033 TRACE nova.compute.manager [instance: c348b942-4553-4023-bbcb-296f3b1bf14f] libvirtError: unsupported configuration: IDE controllers are unsupported for this QEMU binary or machine type
</code></pre>
<p>Change the IDE to virtio</p>
<h1 id="qemu-kvm-cirrus-vga-not-available"><a class="header" href="#qemu-kvm-cirrus-vga-not-available">qemu-kvm: Cirrus VGA not available</a></h1>
<pre><code>| fault                                | {&quot;message&quot;: &quot;Build of instance a1feb48a-b5f5-48ab-93a7-838bb46573fb was re-scheduled: internal error: process exited while connecting to monitor: 2017-05-18T10:33:34.222333Z qemu-kvm: Cirrus VGA not available&quot;, &quot;code&quot;: 500, &quot;details&quot;: &quot;  File \&quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py\&quot;, line 2258, in _do_build_and_run_instance |
</code></pre>
<p>Machines with power architecture do not support Cirrus VGA</p>
<h1 id="the-default-route-cannot-be-deleted"><a class="header" href="#the-default-route-cannot-be-deleted">The default route cannot be deleted</a></h1>
<p>There are two services to confirm whether the status is normal. network and NetworkManager</p>
<h1 id="dashboard-access-lag"><a class="header" href="#dashboard-access-lag">dashboard access lag</a></h1>
<p>The reason for locating the dashboard card is that it should be a nova card,</p>
<p>nova freezes because nova cannot establish a connection with memcached,</p>
<p>It is further located that the default maximum number of connections for memcached is 1024, which has reached the maximum number of connections so far.</p>
<p>The solution is to edit /etc/sysconfig/memcached</p>
<p>The parameters are modified to:</p>
<pre><code>PORT=&quot;11211&quot;
USER=&quot;memcached&quot;
MAXCONN=&quot;65536&quot;
CACHESIZE=&quot;1024&quot;
OPTIONS=&quot;&quot;
</code></pre>
<p>restart memcached</p>
<h1 id="task_state-has-been-in-scheduling"><a class="header" href="#task_state-has-been-in-scheduling">task_state has been in scheduling</a></h1>
<p>Nova boot executes a certain node to generate vm, task_state is always in scheduling, vm_state is always in building, it may be that the nova-compute state of the node that is forced to be scheduled is down.</p>
<h1 id="access-denied-for-user-nova-to-database-nova_api"><a class="header" href="#access-denied-for-user-nova-to-database-nova_api">Access denied for user 'nova'@'%' to database 'nova_api'</a></h1>
<p>Initialize the database when initializing the nova_api database</p>
<pre><code>su -s /bin/sh -c &quot;nova-manage api_db sync&quot; nova
</code></pre>
<p>Error:</p>
<pre><code>Access denied for user 'nova'@'%' to database 'nova_api'
</code></pre>
<p>The root user enters the database and executes</p>
<pre><code>&gt; GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY '2267593eb27be7c414fc';
</code></pre>
<p>solve</p>
<h1 id="all-nodes-have-insufficient-disks-and-return-0-available-hosts"><a class="header" href="#all-nodes-have-insufficient-disks-and-return-0-available-hosts">All nodes have insufficient disks, and return 0 available hosts</a></h1>
<p>Go to the node df -h and find that there is a lot of disk space left. Nova judges the disk space of a node not based on ll, but based on the space occupied by vm and other data.</p>
<p>Such as:</p>
<pre><code>[root@NFJD-PSC-IBMN-SV356 10d31bfa-961d-44ea-b554-7575315a8e2e]# ll -h
total 679M
-rw-rw---- 1 root root  16K Nov 17 17:32 console.log
-rw-r--r-- 1 root root 679M Nov 18 10:07 disk
-rw-r--r-- 1 qemu qemu 422K Nov 17 17:24 disk.config
-rw-r--r-- 1 nova nova  162 Nov 17 17:24 disk.info
-rw-r--r-- 1 nova nova 3.4K Nov 17 17:24 libvirt.xml
[root@NFJD-PSC-IBMN-SV356 10d31bfa-961d-44ea-b554-7575315a8e2e]# qemu-img info disk
image: disk
file format: qcow2
virtual size: 40G (42949672960 bytes)
disk size: 678M
cluster_size: 65536
backing file: /var/lib/nova/instances/_base/af019e4c89c44506c068ada379c040848416510e
Format specific information:
    compat: 1.1
    lazy refcounts: false
</code></pre>
<p>ll The output of this file is more than 600 megabytes, but nova statistics are based on 40G. Because the nova code counts disk_over_committed. nova will store the statistical disk information in the table compute_nodes of the nova database and provide it to the scheduler.</p>
<p>The virtual size here minus the disk size is over_commit_size.</p>
<p>It can be seen that only the image in qcow2 format is overcommited here, and the over_commit_size of other files is equal to 0.</p>
<p>In the DiskFilter of the nova scheduling service, disk_allocation_ratio is used to over-allocate disk resources. It is not the same concept as overcommit here. It is the overuse seen from the perspective of the control node, but cannot be seen by the computing node. Overcommit is a calculation The node sees the result obtained after the disk qcow2 compression format, and the remaining space it finally reports is the actual result after deducting the hypothetical qcow2 image file decompression. Therefore, the remaining space actually reported is smaller than the space seen by the naked eye.</p>
<p>If the administrator specifies a computing node during deployment, the virtual machine will be forced to the computing node without going through the scheduling process, forcibly occupying the space that has been included in the oversubscription plan, which may eventually cause the disk resources reported by the computing node is a negative number. And in the future, as the actual disk space occupied by the virtual machine becomes larger and larger, the hard disk space of the computing node may eventually be insufficient.</p>
<h1 id="novnc-can-not-open-the-problem-location"><a class="header" href="#novnc-can-not-open-the-problem-location">novnc can not open the problem location</a></h1>
<p>Maybe the compute firewall has been changed</p>
<p>Add in /etc/sysconfig/iptables</p>
<pre><code>-A INPUT -p tcp --dport 5900:6100 -j ACCEPT
</code></pre>
<h1 id="qemu-ga-fails-to-start-because-it-cannot-find-the-corresponding-virtual-serial-character-device-prompting-that-the-channel-cannot-be-found"><a class="header" href="#qemu-ga-fails-to-start-because-it-cannot-find-the-corresponding-virtual-serial-character-device-prompting-that-the-channel-cannot-be-found">qemu-ga fails to start because it cannot find the corresponding virtual serial character device, prompting that the channel cannot be found</a></h1>
<p>glance image-update –property hw_qemu_guest_agent=yes $IMAGE_ID# … For other property configurations, be sure to set property hw_qemu_guest_agent=yes, otherwise libvert will not generate qemu-ga configuration items when starting the virtual machine, causing the qemu-ga inside the virtual machine to fail to find The corresponding virtual serial character device fails to start, prompting that the channel cannot be found.</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2018-04-14 05:18</p>
<p>henomenon: openstack dashboard novnc can not view, report Failed to connect to server (code: 1006) error</p>
<p>Check the log:</p>
<p>The consoleauth log information of the 3 controller nodes is as follows:</p>
<p>token expired or false</p>
<p>According to the vncproxy running process below, it should be that the token of the nova-consoleauth cache in step 10 and the check token in step 14 are wrong.</p>
<p>Features of VNC Proxy:</p>
<ul>
<li>Separate the public network from the private network</li>
<li>VNC client runs on the public network, VNCServer runs on the private network, and VNC Proxy acts as an intermediate bridge to connect the two</li>
<li>VNC Proxy authenticates VNC Client through token</li>
<li>VNC Proxy not only makes private network access more secure, but also separates the implementation of specific VNC Servers, which can support VNC Servers of different Hypervisors without affecting user experience</li>
</ul>
<p>Deployment of VNC Proxy</p>
<ul>
<li>Deploy the nova-consoleauth process on the Controller node for Token verification</li>
<li>Deploy the nova-novncproxy service on the Controller node, and the user's VNC Client will directly connect to this service</li>
<li>The Controller node generally has two network cards, connected to two networks, one is used for external access, we call it public network, or API network, the IP address of this network card is the external network IP, as shown in the figure 172.24.1.1, in addition A network card is used for communication between the various modules of openstack, called the management network, usually the intranet IP, as shown in the figure 10.10.10.2</li>
<li>Deploy nova-compute on the Compute node with the following configuration in the nova.conf file
<ul>
<li>vnc_enabled=True</li>
<li>vncserver_listen=0.0.0.0 //VNC Server listening address</li>
<li>vncserver_proxyclient_address=10.10.10.2 //nova vnc proxy accesses the vnc server through the intranet IP, so nova-compute will tell vnc proxy to use this IP to connect to me.</li>
<li>novncproxy_base_url= http://172.24.1.1:6080/vnc_auto.html //This url is the url returned to the customer, so the IP inside is the external network IP</li>
</ul>
</li>
</ul>
<p>The running process of VNC Proxy:</p>
<ol>
<li>A user tries to open the VNC Client connected to the virtual machine from the browser</li>
<li>The browser sends a request to nova-api, requesting to return the url to access vnc</li>
<li>nova-api calls the get vnc console method of nova-compute, requesting Return the information about connecting to VNC</li>
<li>nova-compute calls the get vnc console function of libvirt</li>
<li>libvirt will obtain the information of VNC Server by parsing the /etc/libvirt/qemu/instance-0000000c.xml file running on the virtual machine</li>
<li>libvirt will Host, port and other information are returned to nova-compute in json format 7. nova-compute will randomly generate a UUID as a Token 8. nova-compute synthesizes the information returned by libvirt and the information in the configuration file into connect_info and returns it to nova-api</li>
<li>nova-api will call the authorize_console function of nova-
consoleauth 10.nova-consoleauth will cache the information of instance –&gt; token, token –&gt; connect_info</li>
<li>nova-api will return the access url information in connect_info to the browser: http ://172.24.1.1:6080/vnc_auto.html?token=7efaee3f-eada-4731-a87c-e173cbd25e98&amp;title=helloworld%289169fdb2-5b74-46b1-9803-60d2926bd97c%29</li>
<li>The browser will try to open this link</li>
<li>This link will send the request to nova-novncproxy</li>
<li>nova-novncproxy calls the check_token function of nova-consoleauth 15.nova-consoleauth
verifies the token and returns the connect_info corresponding to the instance Give nova-novncproxy</li>
<li>nova-novncproxy connects to the VNC Server on the compute node through the host, port and other information in connect_info, thus starting the work of the proxy</li>
</ol>
<p>It may be that the high-availability deployment of the control node may be that the memcache is not configured or configured incorrectly.</p>
<p>The investigation found that the configuration item was wrong with a letter . After changing</p>
<p>the configuration memcached_serversitem in /etc/nova/nova.conf to novnc memcache_servers, it can be accessed. , and changed to P version
memcached_memcache_</p>
<pre><code>Option memcached_servers is deprecated in Mitaka. Operators should use oslo.cache configuration instead. Specifically enabled option under [cache] section should be set to True and the url(s) for the memcached servers should be in [cache]/memcache_servers option.

https://docs.openstack.org/oslo.cache/1.16.0/opts.html
memcache_servers
Type:   list
Default:    localhost:11211
Memcache servers in the format of “host:port”. (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-12-28 22:20</p>
<p>When deploying Openstack, the environment may be limited and the external network cannot be connected. At this time, the installation needs to be completed through the local yum source.</p>
<p>Taking the Pike version and the Centos platform as examples, it explains how to build a local yum source step by step.</p>
<h1 id="install-httpd-first"><a class="header" href="#install-httpd-first">Install httpd first</a></h1>
<pre><code>yum install httpd
</code></pre>
<p>In addition to apache httpd, nginx can also be installed</p>
<p>And open the httpd service</p>
<h1 id="configure-remote-yum-source"><a class="header" href="#configure-remote-yum-source">Configure remote yum source</a></h1>
<p>Foreign sources are often unstable, and the speed is not fast enough, while the domestic Alibaba image is relatively stable. The fourth line deletes the Ali intranet address</p>
<pre><code>rm -f /etc/yum.repos.d/*
curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
sed -i '/aliyuncs.com/d' /etc/yum.repos.d/*.repo
</code></pre>
<p>Configure the Openstack Pike source</p>
<p>vim /etc/yum.repos.d/Ali-pike.repo</p>
<pre><code>[Aliyun-pike]
name=Aliyun-pike
baseurl=https://mirrors.aliyun.com/centos/$releasever/cloud/$basearch/openstack-pike/
gpgcheck=0
enabled=1
cost=88

[Aliyun-qemu-ev]
name=Aliyun-qemu-ev
baseurl=https://mirrors.aliyun.com/centos/$releasever/virt/$basearch/kvm-common/
https://mirrors.aliyun.com/centos
gpgcheck=0
enabled=1
</code></pre>
<h1 id="synchronize-the-rpm-package-repository"><a class="header" href="#synchronize-the-rpm-package-repository">Synchronize the rpm package repository</a></h1>
<pre><code>[root@localhost ~]# yum makecache
[root@localhost ~]# mkdir /usr/local/nginx/html/pike
[root@localhost pike]# cd /usr/local/nginx/html/pike
[root@localhost pike]# yum repolist
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
repo id repo name status
Aliyun-pike/7/x86_64 Aliyun-pike 2,195
Aliyun-qemu-ev/7/x86_64 Aliyun-qemu-ev 39
base/7/x86_64 CentOS-7 - Base - mirrors.aliyun.com 9,591
epel/x86_64 Extra Packages for Enterprise Linux 7 - x86_64 12,184
extras/7/x86_64 CentOS-7 - Extras - mirrors.aliyun.com 327
updates/7/x86_64 CentOS-7 - Updates - mirrors.aliyun.com 1,573
repolist: 25,909
[root@localhost pike]# reposync --repoid=Aliyun-pike
(1/2195): GitPython-1.0.1-5.el7.noarch.rpm | 501 kB 00:00:00
(2/2195): PyQt4-devel-4.12-1.el7.x86_64.rpm | 419 kB 00:00:00
(3/2195): PyQt4-4.12-1.el7.x86_64.rpm | 2.9 MB 00:00:00
(4/2195): PyQt4-qsci-api-4.12-1.el7.x86_64.rpm | 156 kB 00:00:00
(5/2195): XStatic-Angular-common-1.5.8.0-1.el7.noarch.rpm | 455 kB 00:00:00
...
(2195/2195): zeromq-4.0.5-4.el7.x86_64.rpm | 434 kB 00:00:00
[root@localhost pike]# reposync --repoid=Aliyun-qemu-ev
(1/39): centos-release-qemu-ev-1.0-1.el7.noarch.rpm | 10 kB 00:00:00
(2/39): gperftools-2.4-7.el7.x86_64.rpm | 3.0 kB 00:00:00
(3/39): OVMF-20150414-2.gitc9e5618.el7.noarch.rpm | 1.3 MB 00:00:00
...
(39/39): qemu-kvm-tools-ev-2.9.0-16.el7_4.8.1.x86_64.rpm | 320 kB 00:00:05
</code></pre>
<h1 id="create-list"><a class="header" href="#create-list">create list</a></h1>
<p>Now the directory /usr/local/nginx/html/pike only has software packages and no repodate list, so createrepo needs to be recreated to create the list.</p>
<pre><code>[root@localhost pike]# pwd
/usr/local/nginx/html/pike
[root@localhost pike]# createrepo /usr/local/nginx/html/pike
-bash: createrepo: command not found
[root@localhost pike]# yum install createrepo
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package createrepo.noarch 0:0.9.9-28.el7 will be installed
--&gt; Processing Dependency: python-deltarpm for package: createrepo-0.9.9-28.el7.noarch
--&gt; Processing Dependency: deltarpm for package: createrepo-0.9.9-28.el7.noarch
--&gt; Running transaction check
---&gt; Package deltarpm.x86_64 0:3.6-3.el7 will be installed
---&gt; Package python-deltarpm.x86_64 0:3.6-3.el7 will be installed
--&gt; Finished Dependency Resolution
Dependencies Resolved
==============================================================================================================================================================================================================================================================================
Package Arch Version Repository Size
==============================================================================================================================================================================================================================================================================
Installing:
createrepo noarch 0.9.9-28.el7 base 94 k
Installing for dependencies:
deltarpm x86_64 3.6-3.el7 base 82 k
python-deltarpm x86_64 3.6-3.el7 base 31 k
Transaction Summary
==============================================================================================================================================================================================================================================================================
Install 1 Package (+2 Dependent packages)
Total download size: 207 k
Installed size: 558 k
Is this ok [y/d/N]: y
Downloading packages:
(1/3): deltarpm-3.6-3.el7.x86_64.rpm | 82 kB 00:00:00
(2/3): createrepo-0.9.9-28.el7.noarch.rpm | 94 kB 00:00:00
(3/3): python-deltarpm-3.6-3.el7.x86_64.rpm | 31 kB 00:00:00
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total 446 kB/s | 207 kB 00:00:00
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
Installing : deltarpm-3.6-3.el7.x86_64 1/3
Installing : python-deltarpm-3.6-3.el7.x86_64 2/3
Installing : createrepo-0.9.9-28.el7.noarch 3/3
Verifying : createrepo-0.9.9-28.el7.noarch 1/3
Verifying : deltarpm-3.6-3.el7.x86_64 2/3
Verifying : python-deltarpm-3.6-3.el7.x86_64 3/3
Installed:
createrepo.noarch 0:0.9.9-28.el7
Dependency Installed:
deltarpm.x86_64 0:3.6-3.el7 python-deltarpm.x86_64 0:3.6-3.el7
Complete!
[root@localhost pike]# createrepo /usr/local/nginx/html/pike
Spawning worker 0 with 559 pkgs
Spawning worker 1 with 559 pkgs
Spawning worker 2 with 558 pkgs
Spawning worker 3 with 558 pkgs
Workers Finished
Saving Primary metadata
Saving file lists metadata
Saving other metadata
Generating sqlite DBs
Sqlite DBs complete
</code></pre>
<h1 id="configure-local-yum-source"><a class="header" href="#configure-local-yum-source">Configure local yum source</a></h1>
<p>For example, control node yum source configuration</p>
<p>vim /etc/yum.repos.d/openstack.repo</p>
<pre><code>[openstack]
name=openstack
baseurl=http://192.168.206.146/pike
enabled=1
gpgcheck=0
</code></pre>
<p>192.168.206.146 is the ip address of the previously synchronized local yum source</p>
<p>yum makecache</p>
<p>Same as other nodes.</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-07-20 18:30</p>
<p>Since our operating system and CPU features adopt the NUMA architecture, we can optimize the KVM CPU by adjusting the NUMA relationship corresponding to KVM. Here, we generally do related operations through the method of CPU binding.</p>
<p>Virtual machine vCPU core binding affinity: Support the affinity policy between virtual machine vCPU and physical CPU core, VCPU and physical CPU are bound one by one, different VMs run on different physical CPUs, that is, physical CPUs are not shared between VMs , to avoid performance degradation and uncertainty caused by over-allocation.</p>
<p>For example, if we want to see the vCPU corresponding to the physical CPU in this Win7-ent virtual machine, we can run: <code># virsh vcpuinfo Win7-ent</code> can view.</p>
<p><img src="openstack/2023-01-17-14-39-27.png" alt="" /></p>
<p>This virtual machine has 2 vCPU dual-core, and then they all run on the CPU8 of the physical machine, and the usage time is 2964.6s. The last one is the affinity of the CPU. This yyyyy represents the logical core inside the physical CPU used, and a y represents one of the CPU logical cores. All are y, which means that the 24 CPU cores of this physical machine can be scheduled and used.</p>
<p>You can enter vrish, and then run emulatorpin Win7-ent. Through this command, we can get the cores that this virtual machine can use in more detail:</p>
<p><img src="openstack/2023-01-17-14-39-39.png" alt="" /></p>
<p>We can see that the current CPU of this virtual machine 0-23 can be scheduled for use</p>
<p>Then the above is to check the virtual machine CPU NUMA scheduling information. If we want to bind the virtual machine to a fixed CPU, we need to do the following: # virsh emulatorpin Win7-ent 18-23 –live Through this command, we put This win7 virtual machine vCPU is bound to the cores between the 6 CPUs of 18-23.</p>
<p>We use the command to check emulatorpin Win7-ent</p>
<p><img src="openstack/2023-01-17-14-39-51.png" alt="" /></p>
<p>You can also use virsh dumpxml Win7-ent to check and confirm:</p>
<p><img src="openstack/2023-01-17-14-40-02.png" alt="" /></p>
<p>I have two vCPUs in a virtual machine, such as this win7, which is dual-core. I want vCPU1 and vCPU2 in it to be bound to different physical CPUs, is that okay? How to do it? This is also possible, we can bind the relevant vCPUs separately through the following method</p>
<pre><code># virsh vcpupin Win7-ent 0 22

# virsh vcpupin Win7-ent 1 23

# virsh dumpxml Win7-ent
</code></pre>
<p><img src="openstack/2023-01-17-14-40-17.png" alt="" /></p>
<pre><code># virsh vcpuinfo Win7-ent
</code></pre>
<p><img src="openstack/2023-01-17-14-40-30.png" alt="" /></p>
<p>It should be noted here that if you reboot the virtual machine, the binding configuration will still take effect, but if you shut it down, the effect of CPU binding will be invalid. If we want to make the VM shut down and then get up to take effect, we must write the parameters into the XML of the virtual machine, and then save it, so that it will not be invalid even after shutting down.</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-07-04 10:20</p>
<h1 id="the-resize-operation-on-the-cloud-host-is-not-successful-1"><a class="header" href="#the-resize-operation-on-the-cloud-host-is-not-successful-1">The resize operation on the cloud host is not successful</a></h1>
<p>Resize a vm, that is, change from a small flavor to a large flavor, without success</p>
<h1 id="check-the-nova-computelog-of-the-node-where-the-cloud-host-is-located"><a class="header" href="#check-the-nova-computelog-of-the-node-where-the-cloud-host-is-located">Check the nova-compute.log of the node where the cloud host is located</a></h1>
<pre><code>2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher File &quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 6959, in _error_out_instance_on_exception
2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher raise error.inner_exception
2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher ResizeError: Resize error: not able to execute ssh command: Unexpected error while running command.
2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher Command: ssh 172.16.231.26 -p 22 mkdir -p /var/lib/nova/instances/54091e90-55f5-4f4d-8f66-31fbc787584f
2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher Exit code: 255
2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher Stdout: u''
2017-07-03 17:50:08.573 24296 TRACE oslo_messaging.rpc.dispatcher Stderr: u'@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\n@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @\r\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\r\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\r\nIt is also possible that a host key has just been changed.\r\nThe fingerprint for the ECDSA key sent by the remote host is\nc8:0b:98:9d:8d:d8:14:89:e6:fe:97:66:22:4e:57:13.\r\nPlease contact your system administrator.\r\nAdd correct host key in /var/lib/nova/.ssh/known_hosts to get rid of this message.\r\nOffending ECDSA key in /var/lib/nova/.ssh/known_hosts:34\r\nPassword authentication is disabled to avoid man-in-the-middle attacks.\r\nKeyboard-interactive authentication is disabled to avoid man-in-the-middle attacks.\r\nPermission denied (publickey,gssapi-keyex,gssapi-with-mic,password).\r\n'
</code></pre>
<h1 id="analyze-the-reasons"><a class="header" href="#analyze-the-reasons">Analyze the reasons</a></h1>
<p>It may be that the host key of the nova user of the computing node has changed</p>
<h1 id="countermeasures"><a class="header" href="#countermeasures">Countermeasures</a></h1>
<p>For all computing nodes, delete /var/lib/nova/.ssh/known_hosts rows related to computing nodes, log in to each other with nova user ssh again, add a host key and ensure that ssh between nova user computing nodes is mutual trust</p>
<h1 id="test-ok"><a class="header" href="#test-ok">test ok</a></h1>
<pre><code>[root@NFJD-TESTVM-CORE-API-1 ~]# nova show e1b51267-2fad-48da-a749-bb36b729bf0c
+--------------------------------------+-------------------------------------------------------------------+
| Property                             | Value                                                             |
+--------------------------------------+-------------------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                            |
| OS-EXT-AZ:availability_zone          | nova                                                              |
| OS-EXT-SRV-ATTR:host                 | NFJD-TESTN-COMPUTE-2                                              |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | NFJD-TESTN-COMPUTE-2                                              |
| OS-EXT-SRV-ATTR:instance_name        | instance-0000685c                                                 |
| OS-EXT-STS:power_state               | 1                                                                 |
| OS-EXT-STS:task_state                | -                                                                 |
| OS-EXT-STS:vm_state                  | active                                                            |
| OS-SRV-USG:launched_at               | 2017-07-04T01:13:23.000000                                        |
| OS-SRV-USG:terminated_at             | -                                                                 |
| accessIPv4                           |                                                                   |
| accessIPv6                           |                                                                   |
| config_drive                         | True                                                              |
| created                              | 2017-07-04T01:12:23Z                                              |
| flavor                               | m1.medium (3)                                                     |
| hostId                               | d4b42eac18963a6b9c295b06afa49642ce9e51d12d9aef2d1a16e930          |
| id                                   | e1b51267-2fad-48da-a749-bb36b729bf0c                              |
| image                                | CentOS-7.1-x86_64-20161018 (8aaf1759-9fb4-4ba9-8cff-1743eb824c5f) |
| key_name                             | -                                                                 |
| metadata                             | {}                                                                |
| name                                 | test                                                              |
| net001 network                       | 192.168.2.127                                                     |
| os-extended-volumes:volumes_attached | []                                                                |
| progress                             | 0                                                                 |
| security_groups                      | default                                                           |
| status                               | ACTIVE                                                            |
| tenant_id                            | 6c149dcd3cf64171b8dd972dd03bbac0                                  |
| updated                              | 2017-07-04T01:12:19Z                                              |
| user_id                              | 62f52135115f4898bd0d82c1f0cd632b                                  |
+--------------------------------------+-------------------------------------------------------------------+
[root@NFJD-TESTVM-CORE-API-1 ~]# nova resize e1b51267-2fad-48da-a749-bb36b729bf0c 4
[root@NFJD-TESTVM-CORE-API-1 ~]# nova show e1b51267-2fad-48da-a749-bb36b729bf0c
+--------------------------------------+-------------------------------------------------------------------+
| Property                             | Value                                                             |
+--------------------------------------+-------------------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                            |
| OS-EXT-AZ:availability_zone          | nova                                                              |
| OS-EXT-SRV-ATTR:host                 | NFJD-TESTN-COMPUTE-2                                              |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | NFJD-TESTN-COMPUTE-2                                              |
| OS-EXT-SRV-ATTR:instance_name        | instance-0000685c                                                 |
| OS-EXT-STS:power_state               | 1                                                                 |
| OS-EXT-STS:task_state                | -                                                                 |
| OS-EXT-STS:vm_state                  | active                                                            |
| OS-SRV-USG:launched_at               | 2017-07-04T02:35:50.000000                                        |
| OS-SRV-USG:terminated_at             | -                                                                 |
| accessIPv4                           |                                                                   |
| accessIPv6                           |                                                                   |
| config_drive                         | True                                                              |
| created                              | 2017-07-04T02:34:42Z                                              |
| flavor                               | m1.large (4)                                                     |
| hostId                               | d4b42eac18963a6b9c295b06afa49642ce9e51d12d9aef2d1a16e930          |
| id                                   | e1b51267-2fad-48da-a749-bb36b729bf0c                              |
| image                                | CentOS-7.1-x86_64-20161018 (8aaf1759-9fb4-4ba9-8cff-1743eb824c5f) |
| key_name                             | -                                                                 |
| metadata                             | {}                                                                |
| name                                 | test                                                              |
| net001 network                       | 192.168.2.127                                                     |
| os-extended-volumes:volumes_attached | []                                                                |
| progress                             | 0                                                                 |
| security_groups                      | default                                                           |
| status                               | ACTIVE                                                            |
| tenant_id                            | 6c149dcd3cf64171b8dd972dd03bbac0                                  |
| updated                              | 2017-07-04T02:34:19Z                                              |
| user_id                              | 62f52135115f4898bd0d82c1f0cd632b                                  |
+--------------------------------------+-------------------------------------------------------------------+
</code></pre>
<p>You can see the flavor from m1.medium -&gt; m1.large</p>
<h1 id="analyze-the-resize-process-through-logs"><a class="header" href="#analyze-the-resize-process-through-logs">Analyze the resize process through logs</a></h1>
<h2 id="source-node"><a class="header" href="#source-node">source node</a></h2>
<pre><code>2017-07-04 10:35:40.999 9791 DEBUG oslo_concurrency.processutils [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:223
</code></pre>
<p>Read cloud host disk information</p>
<pre><code>2017-07-04 10:35:41.430 9791 DEBUG oslo_concurrency.processutils [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Running cmd (subprocess): ssh 172.16.231.26 -p 22 mkdir -p /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:223
</code></pre>
<p>Create folder with instance id from source node ssh to target node</p>
<pre><code>2017-07-04 10:35:41.524 9791 DEBUG nova.virt.libvirt.driver [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Shutting down instance from state 1 _clean_shutdown /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:2517
2017-07-04 10:35:43.572 9791 INFO nova.virt.libvirt.driver [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Instance shutdown successfully after 2 seconds.
2017-07-04 10:35:43.578 9791 INFO nova.virt.libvirt.driver [-] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Instance destroyed successfully.
</code></pre>
<p>shutdown vm</p>
<pre><code>2017-07-04 10:35:43.578 9791 DEBUG oslo_concurrency.processutils [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Running cmd (subprocess): mv /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca_resize execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:223
</code></pre>
<p>The disk folder of vm is moved from d63958dc-3510-4291-b219-93183c49d2ca to d63958dc-3510-4291-b219-93183c49d2ca_resize</p>
<pre><code>2017-07-04 10:35:49.547 9791 DEBUG oslo_concurrency.processutils [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] CMD &quot;qemu-img convert -f qcow2 -O qcow2 /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca_resize/disk /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca_resize/disk_rbase&quot; returned: 0 in 5.957s execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:254
</code></pre>
<p>Generate the disk file into a new file disk_rbase in qcow2 format</p>
<pre><code>2017-07-04 10:35:49.682 9791 DEBUG oslo_concurrency.processutils [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] CMD &quot;rsync --sparse --compress --dry-run /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca_resize/disk_rbase 172.16.231.26:/var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk&quot; returned: 0 in 0.133s execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:254
</code></pre>
<p>Copy the disk file to the new node via rsync</p>
<pre><code>2017-07-04 10:35:58.295 9791 DEBUG nova.virt.driver [-] Emitting event &lt;LifecycleEvent: 1499135743.29, d63958dc-3510-4291-b219-93183c49d2ca =&gt; Stopped&gt; emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1309
2017-07-04 10:35:58.297 9791 INFO nova.compute.manager [-] [instance: d63958dc-3510-4291-b219-93183c49d2ca] VM Stopped (Lifecycle Event)
2017-07-04 10:35:58.367 9791 DEBUG nova.compute.manager [req-bb3cd615-5752-4574-86a5-efea318d4cd3 - - - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Synchronizing instance power state after lifecycle event &quot;Stopped&quot;; current vm_state: active, current task_state: resize_migrating, current DB power_state: 1, VM power_state: 4 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1274
</code></pre>
<p>The life cycle of vm is updated to STOP shutdown</p>
<pre><code>2017-07-04 10:38:27.357 9791 DEBUG oslo_concurrency.processutils [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] CMD &quot;rsync --sparse --compress /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca_resize/disk_rbase 172.16.231.26:/var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk&quot; returned: 0 in 157.673s execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:254
</code></pre>
<p>The previous &quot;copy the disk file to the new node via rsync&quot; command ends, and the command returns a status code of 0, which is normal. It can be seen that this copy is very time-consuming, from the time the command is issued to the end, it takes nearly three minutes</p>
<pre><code>2017-07-04 10:38:27.360 9791 DEBUG oslo_concurrency.processutils [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Running cmd (subprocess): rm -f /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca_resize/disk_rbase execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:223
</code></pre>
<p>Delete the disk_rbase file</p>
<pre><code>2017-07-04 10:38:27.887 9791 DEBUG oslo_concurrency.processutils [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Running cmd (subprocess): rsync --sparse --compress --dry-run /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca_resize/disk.config 172.16.231.26:/var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk.config execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:223
</code></pre>
<p>Copy the disk.conf file to the new node via rsync</p>
<h2 id="new-node"><a class="header" href="#new-node">new node</a></h2>
<pre><code>2017-07-04 10:36:41.924 7940 DEBUG nova.compute.manager [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Stashing vm_state: active _prep_resize /usr/lib/python2.7/site-packages/nova/compute/manager.py:4197
2017-07-04 10:36:42.083 7940 DEBUG oslo_concurrency.lockutils [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Lock &quot;compute_resources&quot; acquired by &quot;resize_claim&quot; :: waited 0.000s inner /usr/lib/python2.7/site-packages/oslo_concurrency/lockutils.py:444
2017-07-04 10:36:42.084 7940 DEBUG nova.compute.resource_tracker [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Memory overhead for 8192 MB instance; 0 MB resize_claim /usr/lib/python2.7/site-packages/nova/compute/resource_tracker.py:181
2017-07-04 10:36:42.087 7940 INFO nova.compute.claims [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Attempting claim: memory 8192 MB, disk 80 GB
2017-07-04 10:36:42.087 7940 INFO nova.compute.claims [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Total memory: 257763 MB, used: 334848.00 MB
2017-07-04 10:36:42.088 7940 INFO nova.compute.claims [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] memory limit: 386644.50 MB, free: 51796.50 MB
2017-07-04 10:36:42.088 7940 INFO nova.compute.claims [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Total disk: 199 GB, used: 1960.00 GB
2017-07-04 10:36:42.089 7940 INFO nova.compute.claims [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] disk limit: 3184.00 GB, free: 1224.00 GB
2017-07-04 10:36:42.135 7940 DEBUG nova.compute.resources.vcpu [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Total CPUs: 32 VCPUs, used: 172.00 VCPUs test /usr/lib/python2.7/site-packages/nova/compute/resources/vcpu.py:52
2017-07-04 10:36:42.136 7940 DEBUG nova.compute.resources.vcpu [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] CPUs limit: 512.00 VCPUs, free: 340.00 VCPUs test /usr/lib/python2.7/site-packages/nova/compute/resources/vcpu.py:63
2017-07-04 10:36:42.136 7940 INFO nova.compute.claims [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Claim successful
</code></pre>
<p>Apply for cpu, mem, disk resources for the vm about to be resized</p>
<pre><code>2017-07-04 10:39:31.543 7940 DEBUG keystoneclient.session [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] REQ: curl -g -i -X GET http://172.16.231.21:9696/v2.0/ports.json?tenant_id=6c149dcd3cf64171b8dd972dd03bbac0&amp;device_id=d63958dc-3510-4291-b219-93183c49d2ca -H &quot;User-Agent: python-neutronclient&quot; -H &quot;Accept: application/json&quot; -H &quot;X-Auth-Token: {SHA1}a4553ab155061f8207794e3a5fad6865cd9af651&quot; _http_log_request /usr/lib/python2.7/site-packages/keystoneclient/session.py:195
2017-07-04 10:39:31.833 7940 DEBUG keystoneclient.session [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] RESP: [200] date: Tue, 04 Jul 2017 02:37:04 GMT connection: keep-alive content-type: application/json; charset=UTF-8 content-length: 701 x-openstack-request-id: req-a75dc999-a4b4-4d94-b544-7dfea42becf0
RESP BODY: {&quot;ports&quot;: [{&quot;status&quot;: &quot;ACTIVE&quot;, &quot;binding:host_id&quot;: &quot;NFJD-TESTN-COMPUTE-3&quot;, &quot;allowed_address_pairs&quot;: [], &quot;extra_dhcp_opts&quot;: [], &quot;device_owner&quot;: &quot;compute:nova&quot;, &quot;fixed_ips&quot;: [{&quot;subnet_id&quot;: &quot;8d375886-7bb3-4d9f-a049-ecf2eb25f62a&quot;, &quot;ip_address&quot;: &quot;192.168.2.128&quot;}], &quot;id&quot;: &quot;327e38f0-d108-4240-a053-5c4c4878f88c&quot;, &quot;security_groups&quot;: [&quot;68a68b74-d424-4a15-8830-c7d2b89ebd56&quot;], &quot;device_id&quot;: &quot;d63958dc-3510-4291-b219-93183c49d2ca&quot;, &quot;name&quot;: &quot;&quot;, &quot;admin_state_up&quot;: true, &quot;network_id&quot;: &quot;ee98ec6a-e1ed-43eb-9d0a-26080d277484&quot;, &quot;tenant_id&quot;: &quot;6c149dcd3cf64171b8dd972dd03bbac0&quot;, &quot;binding:vif_details&quot;: {&quot;port_filter&quot;: false}, &quot;binding:vnic_type&quot;: &quot;normal&quot;, &quot;binding:vif_type&quot;: &quot;ovs&quot;, &quot;mac_address&quot;: &quot;fa:16:3e:40:33:dd&quot;}]}
_http_log_response /usr/lib/python2.7/site-packages/keystoneclient/session.py:224
</code></pre>
<p>Get the port information of the vm</p>
<pre><code>2017-07-04 10:39:31.834 7940 DEBUG keystoneclient.session [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] REQ: curl -g -i -X PUT http://172.16.231.21:9696/v2.0/ports/327e38f0-d108-4240-a053-5c4c4878f88c.json -H &quot;User-Agent: python-neutronclient&quot; -H &quot;Content-Type: application/json&quot; -H &quot;Accept: application/json&quot; -H &quot;X-Auth-Token: {SHA1}a4553ab155061f8207794e3a5fad6865cd9af651&quot; -d '{&quot;port&quot;: {&quot;binding:host_id&quot;: &quot;NFJD-TESTN-COMPUTE-2&quot;}}' _http_log_request /usr/lib/python2.7/site-packages/keystoneclient/session.py:195
2017-07-04 10:39:31.928 7940 DEBUG keystoneclient.session [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] RESP: [200] date: Tue, 04 Jul 2017 02:37:04 GMT connection: keep-alive content-type: application/json; charset=UTF-8 content-length: 698 x-openstack-request-id: req-4c2d569d-b589-4b33-b17a-2fe9ecec50c8
RESP BODY: {&quot;port&quot;: {&quot;status&quot;: &quot;ACTIVE&quot;, &quot;binding:host_id&quot;: &quot;NFJD-TESTN-COMPUTE-2&quot;, &quot;allowed_address_pairs&quot;: [], &quot;extra_dhcp_opts&quot;: [], &quot;device_owner&quot;: &quot;compute:nova&quot;, &quot;fixed_ips&quot;: [{&quot;subnet_id&quot;: &quot;8d375886-7bb3-4d9f-a049-ecf2eb25f62a&quot;, &quot;ip_address&quot;: &quot;192.168.2.128&quot;}], &quot;id&quot;: &quot;327e38f0-d108-4240-a053-5c4c4878f88c&quot;, &quot;security_groups&quot;: [&quot;68a68b74-d424-4a15-8830-c7d2b89ebd56&quot;], &quot;device_id&quot;: &quot;d63958dc-3510-4291-b219-93183c49d2ca&quot;, &quot;name&quot;: &quot;&quot;, &quot;admin_state_up&quot;: true, &quot;network_id&quot;: &quot;ee98ec6a-e1ed-43eb-9d0a-26080d277484&quot;, &quot;tenant_id&quot;: &quot;6c149dcd3cf64171b8dd972dd03bbac0&quot;, &quot;binding:vif_details&quot;: {&quot;port_filter&quot;: false}, &quot;binding:vnic_type&quot;: &quot;normal&quot;, &quot;binding:vif_type&quot;: &quot;ovs&quot;, &quot;mac_address&quot;: &quot;fa:16:3e:40:33:dd&quot;}}
_http_log_response /usr/lib/python2.7/site-packages/keystoneclient/session.py:224
</code></pre>
<p>Modify the port information of vm, and change the binding host information from NFJD-TESTN-COMPUTE-2 to NFJD-TESTN-COMPUTE-3</p>
<p>REQ body information -d '{“port”: {“binding:host_id”: “NFJD-TESTN -COMPUTE-2”}}’</p>
<p>RESP body changed from “binding:host_id”: “NFJD-TESTN-COMPUTE-3” to “binding:host_id”: “NFJD-TESTN-COMPUTE-2”,</p>
<pre><code>2017-07-04 10:39:32.654 7940 DEBUG nova.virt.libvirt.driver [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Starting finish_migration finish_migration /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:6989
</code></pre>
<p>Start the finish_migration method</p>
<pre><code>2017-07-04 10:39:32.655 7940 DEBUG nova.virt.disk.api [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Checking if we can resize image /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk. size=85899345920 can_resize_image /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:213
2017-07-04 10:39:32.655 7940 DEBUG oslo_concurrency.processutils [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:223
</code></pre>
<p>Check whether the disk file can be resized by command &quot;qemu-img info&quot;</p>
<pre><code>2017-07-04 10:39:32.722 7940 DEBUG nova.virt.disk.api [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Checking if we can extend filesystem inside /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk. CoW=True is_image_extendable /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:227
2017-07-04 10:39:32.723 7940 DEBUG nova.virt.disk.vfs.api [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Instance for image imgfile=/var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk imgfmt=qcow2 partition=None instance_for_image /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/api.py:46
2017-07-04 10:39:32.723 7940 DEBUG nova.virt.disk.vfs.api [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Using primary VFSGuestFS instance_for_image /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/api.py:50
2017-07-04 10:39:32.724 7940 DEBUG nova.virt.disk.vfs.guestfs [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Setting up appliance for /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk qcow2 setup /usr/lib/python2.7/site-packages/nova/virt/disk/vfs/guestfs.py:169
</code></pre>
<p>Check if you can extend filesystem</p>
<pre><code>2017-07-04 10:39:34.190 7940 DEBUG nova.network.neutronv2.api [req-5539f967-c22d-42c0-8f82-afa2017bb990 95c6c2eaccd8432481933be931a54690 98c26c5e5374423bb2c2e61a96fe6470 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] _get_instance_nw_info() _get_instance_nw_info /usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py:773
2017-07-04 10:39:34.213 7940 DEBUG keystoneclient.auth.identity.v2 [req-5539f967-c22d-42c0-8f82-afa2017bb990 95c6c2eaccd8432481933be931a54690 98c26c5e5374423bb2c2e61a96fe6470 - - -] Making authentication request to http://172.16.231.115:35357/v2.0/tokens get_auth_ref /usr/lib/python2.7/site-packages/keystoneclient/auth/identity/v2.py:76
2017-07-04 10:39:34.376 7940 DEBUG keystoneclient.session [req-5539f967-c22d-42c0-8f82-afa2017bb990 95c6c2eaccd8432481933be931a54690 98c26c5e5374423bb2c2e61a96fe6470 - - -] REQ: curl -g -i -X GET http://172.16.231.21:9696/v2.0/ports.json?tenant_id=6c149dcd3cf64171b8dd972dd03bbac0&amp;device_id=d63958dc-3510-4291-b219-93183c49d2ca -H &quot;User-Agent: python-neutronclient&quot; -H &quot;Accept: application/json&quot; -H &quot;X-Auth-Token: {SHA1}0b2a12732f6bd659a206dfb020858f050cdc5fd6&quot; _http_log_request /usr/lib/python2.7/site-packages/keystoneclient/session.py:195
2017-07-04 10:39:34.638 7940 DEBUG keystoneclient.session [req-5539f967-c22d-42c0-8f82-afa2017bb990 95c6c2eaccd8432481933be931a54690 98c26c5e5374423bb2c2e61a96fe6470 - - -] RESP: [200] date: Tue, 04 Jul 2017 02:37:07 GMT connection: keep-alive content-type: application/json; charset=UTF-8 content-length: 701 x-openstack-request-id: req-a8f53ba8-9851-40c3-8ac2-0fc8a852d833
RESP BODY: {&quot;ports&quot;: [{&quot;status&quot;: &quot;ACTIVE&quot;, &quot;binding:host_id&quot;: &quot;NFJD-TESTN-COMPUTE-2&quot;, &quot;allowed_address_pairs&quot;: [], &quot;extra_dhcp_opts&quot;: [], &quot;device_owner&quot;: &quot;compute:nova&quot;, &quot;fixed_ips&quot;: [{&quot;subnet_id&quot;: &quot;8d375886-7bb3-4d9f-a049-ecf2eb25f62a&quot;, &quot;ip_address&quot;: &quot;192.168.2.128&quot;}], &quot;id&quot;: &quot;327e38f0-d108-4240-a053-5c4c4878f88c&quot;, &quot;security_groups&quot;: [&quot;68a68b74-d424-4a15-8830-c7d2b89ebd56&quot;], &quot;device_id&quot;: &quot;d63958dc-3510-4291-b219-93183c49d2ca&quot;, &quot;name&quot;: &quot;&quot;, &quot;admin_state_up&quot;: true, &quot;network_id&quot;: &quot;ee98ec6a-e1ed-43eb-9d0a-26080d277484&quot;, &quot;tenant_id&quot;: &quot;6c149dcd3cf64171b8dd972dd03bbac0&quot;, &quot;binding:vif_details&quot;: {&quot;port_filter&quot;: false}, &quot;binding:vnic_type&quot;: &quot;normal&quot;, &quot;binding:vif_type&quot;: &quot;ovs&quot;, &quot;mac_address&quot;: &quot;fa:16:3e:40:33:dd&quot;}]}
_http_log_response /usr/lib/python2.7/site-packages/keystoneclient/session.py:224
2017-07-04 10:39:34.640 7940 DEBUG keystoneclient.session [req-5539f967-c22d-42c0-8f82-afa2017bb990 95c6c2eaccd8432481933be931a54690 98c26c5e5374423bb2c2e61a96fe6470 - - -] REQ: curl -g -i -X GET http://172.16.231.21:9696/v2.0/networks.json?id=ee98ec6a-e1ed-43eb-9d0a-26080d277484 -H &quot;User-Agent: python-neutronclient&quot; -H &quot;Accept: application/json&quot; -H &quot;X-Auth-Token: {SHA1}aafe088a1f0bdaf55a3374d2e8437a7fac0d3a6e&quot; _http_log_request /usr/lib/python2.7/site-packages/keystoneclient/session.py:195
2017-07-04 10:39:34.891 7940 DEBUG keystoneclient.session [req-5539f967-c22d-42c0-8f82-afa2017bb990 95c6c2eaccd8432481933be931a54690 98c26c5e5374423bb2c2e61a96fe6470 - - -] RESP: [200] date: Tue, 04 Jul 2017 02:37:07 GMT connection: keep-alive content-type: application/json; charset=UTF-8 content-length: 269 x-openstack-request-id: req-0d9ef411-e617-443b-821d-730c66cd9507
RESP BODY: {&quot;networks&quot;: [{&quot;status&quot;: &quot;ACTIVE&quot;, &quot;subnets&quot;: [&quot;8d375886-7bb3-4d9f-a049-ecf2eb25f62a&quot;], &quot;name&quot;: &quot;net001&quot;, &quot;admin_state_up&quot;: true, &quot;router:external&quot;: false, &quot;tenant_id&quot;: &quot;23f23d0bed4f4f408ff7d21492254e1d&quot;, &quot;shared&quot;: false, &quot;id&quot;: &quot;ee98ec6a-e1ed-43eb-9d0a-26080d277484&quot;}]}
_http_log_response /usr/lib/python2.7/site-packages/keystoneclient/session.py:224
2017-07-04 10:39:34.893 7940 DEBUG keystoneclient.session [req-5539f967-c22d-42c0-8f82-afa2017bb990 95c6c2eaccd8432481933be931a54690 98c26c5e5374423bb2c2e61a96fe6470 - - -] REQ: curl -g -i -X GET http://172.16.231.21:9696/v2.0/floatingips.json?fixed_ip_address=192.168.2.128&amp;port_id=327e38f0-d108-4240-a053-5c4c4878f88c -H &quot;User-Agent: python-neutronclient&quot; -H &quot;Accept: application/json&quot; -H &quot;X-Auth-Token: {SHA1}0b2a12732f6bd659a206dfb020858f050cdc5fd6&quot; _http_log_request /usr/lib/python2.7/site-packages/keystoneclient/session.py:195
2017-07-04 10:39:34.901 7940 DEBUG keystoneclient.session [req-5539f967-c22d-42c0-8f82-afa2017bb990 95c6c2eaccd8432481933be931a54690 98c26c5e5374423bb2c2e61a96fe6470 - - -] RESP: [200] date: Tue, 04 Jul 2017 02:37:07 GMT connection: keep-alive content-type: application/json; charset=UTF-8 content-length: 19 x-openstack-request-id: req-d8f3d38b-b1a7-4ae6-8dd7-0deae746add4
RESP BODY: {&quot;floatingips&quot;: []}
_http_log_response /usr/lib/python2.7/site-packages/keystoneclient/session.py:224
2017-07-04 10:39:34.902 7940 DEBUG keystoneclient.session [req-5539f967-c22d-42c0-8f82-afa2017bb990 95c6c2eaccd8432481933be931a54690 98c26c5e5374423bb2c2e61a96fe6470 - - -] REQ: curl -g -i -X GET http://172.16.231.21:9696/v2.0/subnets.json?id=8d375886-7bb3-4d9f-a049-ecf2eb25f62a -H &quot;User-Agent: python-neutronclient&quot; -H &quot;Accept: application/json&quot; -H &quot;X-Auth-Token: {SHA1}aafe088a1f0bdaf55a3374d2e8437a7fac0d3a6e&quot; _http_log_request /usr/lib/python2.7/site-packages/keystoneclient/session.py:195
2017-07-04 10:39:34.917 7940 DEBUG keystoneclient.session [req-5539f967-c22d-42c0-8f82-afa2017bb990 95c6c2eaccd8432481933be931a54690 98c26c5e5374423bb2c2e61a96fe6470 - - -] RESP: [200] date: Tue, 04 Jul 2017 02:37:07 GMT connection: keep-alive content-type: application/json; charset=UTF-8 content-length: 492 x-openstack-request-id: req-fc2f8f43-b004-4cb1-b2c6-d35ecdf43e14
RESP BODY: {&quot;subnets&quot;: [{&quot;vsd_managed&quot;: false, &quot;name&quot;: &quot;net001&quot;, &quot;enable_dhcp&quot;: true, &quot;network_id&quot;: &quot;ee98ec6a-e1ed-43eb-9d0a-26080d277484&quot;, &quot;tenant_id&quot;: &quot;23f23d0bed4f4f408ff7d21492254e1d&quot;, &quot;dns_nameservers&quot;: [&quot;8.8.8.8&quot;], &quot;gateway_ip&quot;: &quot;192.168.2.1&quot;, &quot;ipv6_ra_mode&quot;: null, &quot;allocation_pools&quot;: [{&quot;start&quot;: &quot;192.168.2.2&quot;, &quot;end&quot;: &quot;192.168.2.248&quot;}], &quot;host_routes&quot;: [], &quot;ip_version&quot;: 4, &quot;ipv6_address_mode&quot;: null, &quot;cidr&quot;: &quot;192.168.2.0/24&quot;, &quot;id&quot;: &quot;8d375886-7bb3-4d9f-a049-ecf2eb25f62a&quot;, &quot;subnetpool_id&quot;: null}]}
_http_log_response /usr/lib/python2.7/site-packages/keystoneclient/session.py:224
2017-07-04 10:39:34.918 7940 DEBUG keystoneclient.session [req-5539f967-c22d-42c0-8f82-afa2017bb990 95c6c2eaccd8432481933be931a54690 98c26c5e5374423bb2c2e61a96fe6470 - - -] REQ: curl -g -i -X GET http://172.16.231.21:9696/v2.0/ports.json?network_id=ee98ec6a-e1ed-43eb-9d0a-26080d277484&amp;device_owner=network%3Adhcp -H &quot;User-Agent: python-neutronclient&quot; -H &quot;Accept: application/json&quot; -H &quot;X-Auth-Token: {SHA1}aafe088a1f0bdaf55a3374d2e8437a7fac0d3a6e&quot; _http_log_request /usr/lib/python2.7/site-packages/keystoneclient/session.py:195
2017-07-04 10:39:34.931 7940 DEBUG keystoneclient.session [req-5539f967-c22d-42c0-8f82-afa2017bb990 95c6c2eaccd8432481933be931a54690 98c26c5e5374423bb2c2e61a96fe6470 - - -] RESP: [200] date: Tue, 04 Jul 2017 02:37:07 GMT connection: keep-alive content-type: application/json; charset=UTF-8 content-length: 13 x-openstack-request-id: req-e39c8301-a788-4ca4-9bed-53c549acd0b1
RESP BODY: {&quot;ports&quot;: []}
_http_log_response /usr/lib/python2.7/site-packages/keystoneclient/session.py:224
2017-07-04 10:39:34.932 7940 DEBUG nova.network.base_api [req-5539f967-c22d-42c0-8f82-afa2017bb990 95c6c2eaccd8432481933be931a54690 98c26c5e5374423bb2c2e61a96fe6470 - - -] Updating cache with info: [VIF({'profile': None, 'ovs_interfaceid': u'327e38f0-d108-4240-a053-5c4c4878f88c', 'preserve_on_delete': False, 'network': Network({'bridge': 'alubr0', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'192.168.2.128'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [], 'cidr': u'192.168.2.0/24', 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.2.1'})})], 'meta': {'injected': False, 'tenant_id': u'23f23d0bed4f4f408ff7d21492254e1d'}, 'id': u'ee98ec6a-e1ed-43eb-9d0a-26080d277484', 'label': u'net001'}), 'devname': u'tap327e38f0-d1', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': False}, 'address': u'fa:16:3e:40:33:dd', 'active': True, 'type': u'ovs', 'id': u'327e38f0-d108-4240-a053-5c4c4878f88c', 'qbg_params': None})] update_instance_cache_with_nw_info /usr/lib/python2.7/site-packages/nova/network/base_api.py:42
</code></pre>
<p>Call the neutron API to get the network information of the vm and update the cache</p>
<pre><code>2017-07-04 10:39:35.163 7940 DEBUG nova.virt.disk.api [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Checking if we can resize image /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk. size=85899345920 can_resize_image /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:213
2017-07-04 10:39:35.165 7940 DEBUG oslo_concurrency.processutils [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Running cmd (subprocess): env LC_ALL=C LANG=C qemu-img info /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:223
</code></pre>
<p>Get the information of the image file to judge whether it can be resized</p>
<pre><code>2017-07-04 10:39:35.230 7940 DEBUG oslo_concurrency.processutils [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Running cmd (subprocess): qemu-img resize /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk 85899345920 execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:223
</code></pre>
<p>Resize image file by &quot;qemu-img resize&quot; command</p>
<pre><code>2017-07-04 10:39:35.297 7940 DEBUG nova.virt.disk.api [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Checking if we can extend filesystem inside /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk. CoW=True is_image_extendable /usr/lib/python2.7/site-packages/nova/virt/disk/api.py:227
</code></pre>
<p>Check whether the image file extends filesystem. Check result: CoW=True is_image_extendable</p>
<pre><code>2017-07-04 10:39:37.728 7940 INFO nova.virt.libvirt.driver [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Creating image
2017-07-04 10:39:37.729 7940 DEBUG oslo_concurrency.processutils [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf chown 162 /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk.config execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:223
2017-07-04 10:39:37.804 7940 INFO nova.virt.libvirt.driver [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Using config drive
2017-07-04 10:39:37.901 7940 INFO nova.virt.libvirt.driver [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Creating config drive at /var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk.config
</code></pre>
<p>Generate an image file, use nova-rootwrap, use root privileges to modify the disk.config file permissions, disk.config configuration uses config drive</p>
<pre><code>2017-07-04 10:39:37.924 7940 DEBUG nova.virt.libvirt.driver [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Start _get_guest_xml network_info=[VIF({'profile': None, 'ovs_interfaceid': u'327e38f0-d108-4240-a053-5c4c4878f88c', 'preserve_on_delete': False, 'network': Network({'bridge': 'alubr0', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:40:33:dd', 'floating_ips': [], 'label': u'net001', 'meta': {}, 'address': u'192.168.2.128', 'type': 'fixed'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [], 'cidr': u'192.168.2.0/24', 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.2.1'})})], 'meta': {'injected': False, 'tenant_id': u'23f23d0bed4f4f408ff7d21492254e1d'}, 'id': u'ee98ec6a-e1ed-43eb-9d0a-26080d277484', 'label': u'net001'}), 'devname': u'tap327e38f0-d1', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': False}, 'address': u'fa:16:3e:40:33:dd', 'active': True, 'type': u'ovs', 'id': u'327e38f0-d108-4240-a053-5c4c4878f88c', 'qbg_params': None})] disk_info={'disk_bus': 'virtio', 'cdrom_bus': 'ide', 'mapping': {'disk.config': {'bus': 'ide', 'type': 'cdrom', 'dev': 'hdd'}, 'disk': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': u'vda'}, 'root': {'bus': 'virtio', 'boot_index': '1', 'type': 'disk', 'dev': u'vda'}}} image_meta={u'min_disk': 40, u'container_format': u'bare', u'min_ram': 0, u'disk_format': u'qcow2', u'properties': {u'image.os_type': u'linux', u'hw_qemu_guest_agent': u'yes', u'base_image_ref': u'8aaf1759-9fb4-4ba9-8cff-1743eb824c5f', u'os_distro': u'centos', u'hw_ovirt_guest_agent': u'yes'}} rescue=None block_device_info={'block_device_mapping': [], 'swap': None, 'ephemerals': [], 'root_device_name': u'/dev/vda'} _get_guest_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4547
2017-07-04 10:39:38.140 7940 DEBUG nova.virt.libvirt.vif [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] vif_type=ovs instance=Instance(access_ip_v4=None,access_ip_v6=None,architecture=None,auto_disk_config=False,availability_zone='nova',cell_name=None,cleaned=False,config_drive='True',created_at=2017-07-04T02:34:42Z,default_ephemeral_device=None,default_swap_device=None,deleted=False,deleted_at=None,disable_terminate=False,display_description='test',display_name='test',ephemeral_gb=0,ephemeral_key_uuid=None,fault=&lt;?&gt;,flavor=Flavor(9),host='NFJD-TESTN-COMPUTE-2',hostname='test',id=26717,image_ref='8aaf1759-9fb4-4ba9-8cff-1743eb824c5f',info_cache=InstanceInfoCache,instance_type_id=9,kernel_id='',key_data=None,key_name=None,launch_index=0,launched_at=2017-07-04T02:35:15Z,launched_on='NFJD-TESTN-COMPUTE-3',locked=False,locked_by=None,memory_mb=8192,metadata={},new_flavor=Flavor(9),node='NFJD-TESTN-COMPUTE-2',numa_topology=None,old_flavor=Flavor(3),os_type=None,pci_devices=PciDeviceList,pci_requests=&lt;?&gt;,power_state=1,progress=0,project_id='6c149dcd3cf64171b8dd972dd03bbac0',ramdisk_id='',reservation_id='r-7uedfsof',root_device_name='/dev/vda',root_gb=80,scheduled_at=None,security_groups=SecurityGroupList,shutdown_terminate=False,system_metadata={image_base_image_ref='8aaf1759-9fb4-4ba9-8cff-1743eb824c5f',image_container_format='bare',image_disk_format='qcow2',image_hw_ovirt_guest_agent='yes',image_hw_qemu_guest_agent='yes',image_image.os_type='linux',image_min_disk='40',image_min_ram='0',image_os_distro='centos',old_vm_state='active'},tags=&lt;?&gt;,task_state='resize_finish',terminated_at=None,updated_at=2017-07-04T02:38:01Z,user_data=None,user_id='62f52135115f4898bd0d82c1f0cd632b',uuid=d63958dc-3510-4291-b219-93183c49d2ca,vcpu_model=VirtCPUModel,vcpus=4,vm_mode=None,vm_state='active') vif=VIF({'profile': None, 'ovs_interfaceid': u'327e38f0-d108-4240-a053-5c4c4878f88c', 'preserve_on_delete': False, 'network': Network({'bridge': 'alubr0', 'subnets': [Subnet({'ips': [FixedIP({'version': 4, 'vif_mac': u'fa:16:3e:40:33:dd', 'floating_ips': [], 'label': u'net001', 'meta': {}, 'address': u'192.168.2.128', 'type': 'fixed'})], 'version': 4, 'meta': {}, 'dns': [IP({'meta': {}, 'version': 4, 'type': 'dns', 'address': u'8.8.8.8'})], 'routes': [], 'cidr': u'192.168.2.0/24', 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'192.168.2.1'})})], 'meta': {'injected': False, 'tenant_id': u'23f23d0bed4f4f408ff7d21492254e1d'}, 'id': u'ee98ec6a-e1ed-43eb-9d0a-26080d277484', 'label': u'net001'}), 'devname': u'tap327e38f0-d1', 'vnic_type': u'normal', 'qbh_params': None, 'meta': {}, 'details': {u'port_filter': False}, 'address': u'fa:16:3e:40:33:dd', 'active': True, 'type': u'ovs', 'id': u'327e38f0-d108-4240-a053-5c4c4878f88c', 'qbg_params': None}) virt_typekvm get_config /usr/lib/python2.7/site-packages/nova/virt/libvirt/vif.py:364
2017-07-04 10:39:38.144 7940 DEBUG nova.virt.libvirt.driver [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Qemu guest agent is enabled through image metadata _set_qemu_guest_agent /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4145
2017-07-04 10:39:38.144 7940 DEBUG nova.virt.libvirt.driver [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Ovirt guest agent is enabled through image metadata _set_ovirt_guest_agent /usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py:4161
2017-07-04 10:39:38.147 7940 DEBUG nova.virt.libvirt.config [req-ced45485-770b-4751-a339-58f070c6342b 62f52135115f4898bd0d82c1f0cd632b 6c149dcd3cf64171b8dd972dd03bbac0 - - -] Generated XML ('&lt;domain type=&quot;kvm&quot;&gt;\n  &lt;uuid&gt;d63958dc-3510-4291-b219-93183c49d2ca&lt;/uuid&gt;\n  &lt;name&gt;instance-0000685d&lt;/name&gt;\n  &lt;memory&gt;8388608&lt;/memory&gt;\n  &lt;vcpu&gt;4&lt;/vcpu&gt;\n  &lt;metadata&gt;\n    &lt;nova:instance xmlns:nova=&quot;http://openstack.org/xmlns/libvirt/nova/1.0&quot;&gt;\n      &lt;nova:package version=&quot;2015.1.5-1.el7.centos&quot;/&gt;\n      &lt;nova:name&gt;test&lt;/nova:name&gt;\n      &lt;nova:creationTime&gt;2017-07-04 02:39:38&lt;/nova:creationTime&gt;\n      &lt;nova:flavor name=&quot;m1.large&quot;&gt;\n        &lt;nova:memory&gt;8192&lt;/nova:memory&gt;\n        &lt;nova:disk&gt;80&lt;/nova:disk&gt;\n        &lt;nova:swap&gt;0&lt;/nova:swap&gt;\n        &lt;nova:ephemeral&gt;0&lt;/nova:ephemeral&gt;\n        &lt;nova:vcpus&gt;4&lt;/nova:vcpus&gt;\n      &lt;/nova:flavor&gt;\n      &lt;nova:owner&gt;\n        &lt;nova:user uuid=&quot;62f52135115f4898bd0d82c1f0cd632b&quot;&gt;admin&lt;/nova:user&gt;\n        &lt;nova:project uuid=&quot;6c149dcd3cf64171b8dd972dd03bbac0&quot;&gt;admin&lt;/nova:project&gt;\n      &lt;/nova:owner&gt;\n      &lt;nova:root type=&quot;image&quot; uuid=&quot;8aaf1759-9fb4-4ba9-8cff-1743eb824c5f&quot;/&gt;\n    &lt;/nova:instance&gt;\n  &lt;/metadata&gt;\n  &lt;sysinfo type=&quot;smbios&quot;&gt;\n    &lt;system&gt;\n      &lt;entry name=&quot;manufacturer&quot;&gt;Fedora Project&lt;/entry&gt;\n      &lt;entry name=&quot;product&quot;&gt;OpenStack Nova&lt;/entry&gt;\n      &lt;entry name=&quot;version&quot;&gt;2015.1.5-1.el7.centos&lt;/entry&gt;\n      &lt;entry name=&quot;serial&quot;&gt;604c99de-78ac-4609-b0b9-f6fb62e705c2&lt;/entry&gt;\n      &lt;entry name=&quot;uuid&quot;&gt;d63958dc-3510-4291-b219-93183c49d2ca&lt;/entry&gt;\n    &lt;/system&gt;\n  &lt;/sysinfo&gt;\n  &lt;os&gt;\n    &lt;type&gt;hvm&lt;/type&gt;\n    &lt;boot dev=&quot;hd&quot;/&gt;\n    &lt;smbios mode=&quot;sysinfo&quot;/&gt;\n  &lt;/os&gt;\n  &lt;features&gt;\n    &lt;acpi/&gt;\n    &lt;apic/&gt;\n  &lt;/features&gt;\n  &lt;cputune&gt;\n    &lt;shares&gt;4096&lt;/shares&gt;\n  &lt;/cputune&gt;\n  &lt;clock offset=&quot;utc&quot;&gt;\n    &lt;timer name=&quot;pit&quot; tickpolicy=&quot;delay&quot;/&gt;\n    &lt;timer name=&quot;rtc&quot; tickpolicy=&quot;catchup&quot;/&gt;\n    &lt;timer name=&quot;hpet&quot; present=&quot;no&quot;/&gt;\n  &lt;/clock&gt;\n  &lt;cpu mode=&quot;host-model&quot; match=&quot;exact&quot;&gt;\n    &lt;topology sockets=&quot;4&quot; cores=&quot;1&quot; threads=&quot;1&quot;/&gt;\n  &lt;/cpu&gt;\n  &lt;devices&gt;\n    &lt;disk type=&quot;file&quot; device=&quot;disk&quot;&gt;\n      &lt;driver name=&quot;qemu&quot; type=&quot;qcow2&quot; cache=&quot;directsync&quot;/&gt;\n      &lt;source file=&quot;/var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk&quot;/&gt;\n      &lt;target bus=&quot;virtio&quot; dev=&quot;vda&quot;/&gt;\n    &lt;/disk&gt;\n    &lt;disk type=&quot;file&quot; device=&quot;cdrom&quot;&gt;\n      &lt;driver name=&quot;qemu&quot; type=&quot;raw&quot; cache=&quot;directsync&quot;/&gt;\n      &lt;source file=&quot;/var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/disk.config&quot;/&gt;\n      &lt;target bus=&quot;ide&quot; dev=&quot;hdd&quot;/&gt;\n    &lt;/disk&gt;\n    &lt;interface type=&quot;bridge&quot;&gt;\n      &lt;mac address=&quot;fa:16:3e:40:33:dd&quot;/&gt;\n      &lt;model type=&quot;virtio&quot;/&gt;\n      &lt;source bridge=&quot;alubr0&quot;/&gt;\n      &lt;target dev=&quot;tap327e38f0-d1&quot;/&gt;\n      &lt;virtualport type=&quot;openvswitch&quot;&gt;\n        &lt;parameters interfaceid=&quot;327e38f0-d108-4240-a053-5c4c4878f88c&quot;/&gt;\n      &lt;/virtualport&gt;\n    &lt;/interface&gt;\n    &lt;serial type=&quot;file&quot;&gt;\n      &lt;source path=&quot;/var/lib/nova/instances/d63958dc-3510-4291-b219-93183c49d2ca/console.log&quot;/&gt;\n    &lt;/serial&gt;\n    &lt;serial type=&quot;pty&quot;/&gt;\n    &lt;input type=&quot;tablet&quot; bus=&quot;usb&quot;/&gt;\n    &lt;graphics type=&quot;vnc&quot; autoport=&quot;yes&quot; keymap=&quot;en-us&quot; listen=&quot;0.0.0.0&quot;/&gt;\n    &lt;video&gt;\n      &lt;model type=&quot;cirrus&quot;/&gt;\n    &lt;/video&gt;\n    &lt;channel type=&quot;unix&quot;&gt;\n      &lt;source mode=&quot;bind&quot; path=&quot;/var/lib/libvirt/qemu/org.qemu.guest_agent.0.instance-0000685d.sock&quot;/&gt;\n      &lt;target type=&quot;virtio&quot; name=&quot;org.qemu.guest_agent.0&quot;/&gt;\n    &lt;/channel&gt;\n    &lt;channel type=&quot;unix&quot;&gt;\n      &lt;source mode=&quot;bind&quot; path=&quot;/var/lib/libvirt/qemu/com.redhat.rhevm.vdsm.instance-0000685d.sock&quot;/&gt;\n      &lt;target type=&quot;virtio&quot; name=&quot;com.redhat.rhevm.vdsm&quot;/&gt;\n    &lt;/channel&gt;\n    &lt;channel type=&quot;unix&quot;&gt;\n      &lt;source mode=&quot;bind&quot; path=&quot;/var/lib/libvirt/qemu/com.redhat.rhevm.vdsm.0.instance-0000685d.sock&quot;/&gt;\n      &lt;target type=&quot;virtio&quot; name=&quot;com.redhat.rhevm.vdsm.0&quot;/&gt;\n    &lt;/channel&gt;\n    &lt;memballoon model=&quot;virtio&quot;&gt;\n      &lt;stats period=&quot;10&quot;/&gt;\n    &lt;/memballoon&gt;\n  &lt;/devices&gt;\n&lt;/domain&gt;\n',)  to_xml /usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py:82
</code></pre>
<p>Generate the xml file required for vm boot</p>
<pre><code>2017-07-04 10:39:39.224 7940 DEBUG nova.virt.driver [req-13fbc44e-ff27-43a1-bedc-6c40889619ce - - - - -] Emitting event &lt;LifecycleEvent: 1499135979.22, d63958dc-3510-4291-b219-93183c49d2ca =&gt; Resumed&gt; emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1309
2017-07-04 10:39:39.226 7940 INFO nova.compute.manager [req-13fbc44e-ff27-43a1-bedc-6c40889619ce - - - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] VM Resumed (Lifecycle Event)
2017-07-04 10:39:39.230 7940 INFO nova.virt.libvirt.driver [-] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Instance running successfully.
2017-07-04 10:39:39.297 7940 DEBUG nova.compute.manager [req-13fbc44e-ff27-43a1-bedc-6c40889619ce - - - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Synchronizing instance power state after lifecycle event &quot;Resumed&quot;; current vm_state: active, current task_state: resize_finish, current DB power_state: 1, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1274
2017-07-04 10:39:39.361 7940 INFO nova.compute.manager [req-13fbc44e-ff27-43a1-bedc-6c40889619ce - - - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] During sync_power_state the instance has a pending task (resize_finish). Skip.
2017-07-04 10:39:39.362 7940 DEBUG nova.virt.driver [req-13fbc44e-ff27-43a1-bedc-6c40889619ce - - - - -] Emitting event &lt;LifecycleEvent: 1499135979.23, d63958dc-3510-4291-b219-93183c49d2ca =&gt; Started&gt; emit_event /usr/lib/python2.7/site-packages/nova/virt/driver.py:1309
2017-07-04 10:39:39.363 7940 INFO nova.compute.manager [req-13fbc44e-ff27-43a1-bedc-6c40889619ce - - - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] VM Started (Lifecycle Event)
2017-07-04 10:39:39.425 7940 DEBUG nova.compute.manager [req-13fbc44e-ff27-43a1-bedc-6c40889619ce - - - - -] [instance: d63958dc-3510-4291-b219-93183c49d2ca] Synchronizing instance power state after lifecycle event &quot;Started&quot;; current vm_state: resized, current task_state: None, current DB power_state: 1, VM power_state: 1 handle_lifecycle_event /usr/lib/python2.7/site-packages/nova/compute/manager.py:1274
</code></pre>
<p>Start the vm and update the life cycle of the vm</p>
<h1 id="if-the-vm-before-resize-is-shut-down-the-resize-operation-will-not-start-automatically-after-resize"><a class="header" href="#if-the-vm-before-resize-is-shut-down-the-resize-operation-will-not-start-automatically-after-resize">If the vm before resize is shut down, the resize operation will not start automatically after resize</a></h1>
<p>nova/compute/manager.py</p>
<pre><code class="language-ptyhon">...
        # NOTE(mriedem): If the original vm_state was STOPPED, we don't
        # automatically power on the instance after it's migrated
        power_on = old_vm_state != vm_states.STOPPED

        try: 
            self.driver.finish_migration(context, migration, instance,
                                         disk_info,
                                         network_info,
                                         image_meta, resize_instance,
                                         block_device_info, power_on)
...
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-30 05:16</p>
<h1 id="create-order-a-cloud-disk"><a class="header" href="#create-order-a-cloud-disk">Create (order) a cloud disk</a></h1>
<pre><code>[root@NFJD-PSC-BCEC-SV3 deployer]# cinder create 500 --volume-type fujitsu-ipsan --name 0630
+---------------------------------------+--------------------------------------+
|                Property               |                Value                 |
+---------------------------------------+--------------------------------------+
|              attachments              |                  []                  |
|           availability_zone           |                 nova                 |
|                bootable               |                false                 |
|          consistencygroup_id          |                 None                 |
|               created_at              |      2017-06-30T08:23:05.000000      |
|              description              |                 None                 |
|               encrypted               |                False                 |
|                   id                  | 08d3018c-b1cd-4652-87ed-45a48a14b6f3 |
|                metadata               |                  {}                  |
|              multiattach              |                False                 |
|                  name                 |                 0630                 |
|      os-vol-tenant-attr:tenant_id     |   597854e23bfe46abb6178f786af12391   |
|   os-volume-replication:driver_data   |                 None                 |
| os-volume-replication:extended_status |                 None                 |
|           replication_status          |               disabled               |
|                  size                 |                 500                  |
|              snapshot_id              |                 None                 |
|              source_volid             |                 None                 |
|                 status                |               creating               |
|                user_id                |   622684c00210427091c7493f36aaa6cf   |
|              volume_type              |            fujitsu-ipsan             |
+---------------------------------------+--------------------------------------+
</code></pre>
<h1 id="mount-cloud-disk"><a class="header" href="#mount-cloud-disk">Mount cloud disk</a></h1>
<p>Existing cloud host id 27b31829-326f-4029-a537-bb327303a32c</p>
<p>Mount the cloud disk to the cloud host</p>
<pre><code>[root@NFJD-PSC-BCEC-SV3 deployer]# nova volume-attach 27b31829-326f-4029-a537-bb327303a32c 08d3018c-b1cd-4652-87ed-45a48a14b6f3
+----------+--------------------------------------+
| Property | Value                                |
+----------+--------------------------------------+
| device   | /dev/vdb                             |
| id       | 08d3018c-b1cd-4652-87ed-45a48a14b6f3 |
| serverId | 27b31829-326f-4029-a537-bb327303a32c |
| volumeId | 08d3018c-b1cd-4652-87ed-45a48a14b6f3 |
+----------+--------------------------------------+
</code></pre>
<p>Comparing the execution results of the fdisk -l command before and after mounting, it is found that after the cloud disk is mounted, there is an extra vdb</p>
<p><img src="openstack/2023-01-17-13-45-58.png" alt="" /></p>
<h1 id="artition"><a class="header" href="#artition">artition</a></h1>
<p>Run fdisk /dev/vdb to partition the data disk. According to the prompt, enter n, p, 1 in sequence, press Enter twice, wq, and the partition starts.</p>
<p><img src="openstack/2023-01-17-13-46-13.png" alt="" /></p>
<p>Run the fdisk -l command to view the new partition. The new partition vdb1 has been created. Like /dev/xvdb in the example below.</p>
<p><img src="openstack/2023-01-17-13-46-26.png" alt="" /></p>
<h1 id="format"><a class="header" href="#format">format</a></h1>
<p>Run mkfs.ext3 /dev/xvdb1 to format the new partition. The time required for formatting depends on the size of the data disk. You can also decide to choose other file formats, such as ext4, etc.</p>
<p><img src="openstack/2023-01-17-13-46-39.png" alt="" /></p>
<p>Run echo /dev/xvdb1 /mnt ext3 defaults 0 0 &gt;&gt; /etc/fstab to write new partition information. Once complete, you can use the cat /etc/fstab command to view.</p>
<p><img src="openstack/2023-01-17-13-46-51.png" alt="" /></p>
<p>Through the command mount -a, after modifying /etc/fstab, it will take effect without restarting. Then execute df -h to view the partitions. If the data disk information appears, it means that the mount is successful and the new partition can be used.</p>
<p><img src="openstack/2023-01-17-13-47-02.png" alt="" /></p>
<h1 id="linux-uses-the-dd-command-to-quickly-generate-large-files"><a class="header" href="#linux-uses-the-dd-command-to-quickly-generate-large-files">Linux uses the dd command to quickly generate large files</a></h1>
<p><img src="openstack/2023-01-17-13-47-16.png" alt="" /></p>
<p>The dd command can easily create a file of a specified size, such as</p>
<pre><code>dd if=/dev/zero of=test bs=1M count=1000
</code></pre>
<p>A 1000M test file will be generated, and the content of the file is all 0 (because it is read from /dev/zero, /dev/zero is the source of 0)</p>
<p>But this is for actually writing to the hard disk. The speed of file generation depends on the read and write speed of the hard disk. If you want to generate very large files, the speed is very slow.</p>
<p>In a certain scenario, we just want the file system to think that there is a very large file here, but we don’t actually write it to the hard disk</p>
<p>then you can</p>
<pre><code>dd if=/dev/zero of=test bs=1M count=0 seek=100000
</code></pre>
<p>The displayed size of the file created at this time in the file system is 100000MB, but it does not actually occupy a block, so the creation speed is comparable to the memory speed</p>
<p>The function of seek is to skip the part of the specified size in the output file, which achieves the purpose of creating a large file, but not actually writing</p>
<blockquote>
<p>Of course, because you don’t actually write to the hard disk, you can create 100G of such files on a hard disk with a capacity of only 10G.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-28 18:24</p>
<h1 id="phenomenon"><a class="header" href="#phenomenon">Phenomenon</a></h1>
<p>Execute the nova evacuate operation, but there is a problem during the rebuild. After a certain step, the error &quot;rebuild virtual machine has been deleted&quot; is reported.</p>
<h1 id="reason"><a class="header" href="#reason">reason</a></h1>
<p>When an instance is evacuated from one node to another, an InstanceNotFound exception is expected to occur during the execution of the check_instance_exists method during the rebuild process.</p>
<p>The check_instance_exists method is used to ensure that the instance does not exist on the new node, and if it exists, an exception will be thrown.</p>
<p>The check_instance_exists in the actual code reported this exception when it should not report the InstanceNotFound exception, and the exception of the method was not added to capture the InstanceNotFound exception.</p>
<h1 id="community-fixes"><a class="header" href="#community-fixes">community fixes</a></h1>
<p>nova/virt/libvirt/driver.py</p>
<pre><code class="language-python">def instance_exists(self, instance):
    &quot;&quot;&quot;Efficient override of base instance_exists method.&quot;&quot;&quot;
    try:
        self._host.get_guest(instance)
        return True
    except exception.InternalError:
        return False
</code></pre>
<p>change to:</p>
<pre><code class="language-python">def instance_exists(self, instance):
    &quot;&quot;&quot;Efficient override of base instance_exists method.&quot;&quot;&quot;
    try:
        self._host.get_guest(instance)
        return True
    except (exception.InternalError, exception.InstanceNotFound):
        return False
</code></pre>
<h1 id="unit-test"><a class="header" href="#unit-test">unit test</a></h1>
<pre><code class="language-python">@mock.patch.object(host.Host, 'get_guest')
def test_instance_exists(self, mock_get_guest):
    drvr = libvirt_driver.LibvirtDriver(fake.FakeVirtAPI(), False)
    self.assertTrue(drvr.instance_exists(None))

    mock_get_guest.side_effect = exception.InstanceNotFound
    self.assertFalse(drvr.instance_exists(None))

    mock_get_guest.side_effect = exception.InternalError
    self.assertFalse(drvr.instance_exists(None))
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-22 09:16</p>
<h1 id="nova-evacuate"><a class="header" href="#nova-evacuate">nova evacuate</a></h1>
<p>Various implementation schemes basically call nova's evacuate interface without exception.</p>
<p>nova evacuate is very similar to hot migration. They all want to transfer instances from one node to another. The difference is mainly that thermal migration is performed under normal conditions, while evacuation is performed under abnormal conditions. A vivid example is that thermal migration occurs before an earthquake to escape from a building, and evacuation occurs after an earthquake occurs by escaping from a destroyed building.</p>
<p>But strictly speaking, the above analogy is not rigorous enough. To be precise, it is not escaped from the destroyed building, but resurrected.</p>
<p>So the translation of nova evacuate into resurrection is more accurate than evacuation.</p>
<h1 id="masakari"><a class="header" href="#masakari">masakari</a></h1>
<p>NTT NTT's open source project masakari has become an independent project of Openstack, which is dedicated to compute node ha.</p>
<p>The Japanese word for masakari is まさかり, meaning an axe. To be precise, it's an axe. It is the ax used by Zhou Xingchi's movie Ax Gang. It is not known whether the initiators of the project, several Japanese, were affected by this.</p>
<p>The name masakari is to find the location of the fault, use the ax to help the highest stunt, and throw the ax accurately to isolate and cut off the fault.</p>
<h1 id="detect-3-type-of-vm-down"><a class="header" href="#detect-3-type-of-vm-down">detect 3 type of vm down</a></h1>
<ul>
<li>unexpected vm down (monitoring libvirt's events)</li>
<li>vm manager down (monitoring manager process)</li>
<li>host down (using pacemaker)</li>
</ul>
<h1 id="new-features-that-masakari-is-working-on"><a class="header" href="#new-features-that-masakari-is-working-on">New features that masakari is working on</a></h1>
<ol>
<li>
<p>Now it only supports evacuation of vm in active, stopped, and error states.
Later, it can be added to evacuate vm in more states, such as shelved, rescued, paused and suspended</p>
</li>
<li>
<p>Now the process after the fault is detected is fixed, and the only way to change it is to change the code.
In the future, users can customize the process by writing yaml files.</p>
</li>
</ol>
<h1 id="compute-node-ha-other-technologies-used"><a class="header" href="#compute-node-ha-other-technologies-used">compute node ha other technologies used</a></h1>
<ul>
<li>consul</li>
<li>raft</li>
<li>gossip</li>
</ul>
<h1 id="compute-node-ha-other-related-open-source-projects"><a class="header" href="#compute-node-ha-other-related-open-source-projects">compute node ha other related open source projects</a></h1>
<ul>
<li>Openstack Congress (Policy as a Service)</li>
<li>pacemaker-remote (host monitor solution)</li>
<li>mistral-evacuate (workflow)</li>
</ul>
<h1 id="appendix-1-gossip-protocol"><a class="header" href="#appendix-1-gossip-protocol">Appendix 1: Gossip protocol</a></h1>
<p>A Gossip protocol runs between all Consul Agents (both server and normal). Both server nodes and ordinary Agents will join this Gossip cluster to send and receive Gossip messages. Every once in a while, each node will randomly select several nodes to send Gossip messages, and other nodes will randomly select other nodes to relay and send messages again. After such a period of time, the entire cluster can receive this message.</p>
<p>At first glance, it seems that the efficiency of this sending method is very low, but there have been papers in mathematics that have demonstrated its feasibility, and the Gossip protocol is already a relatively mature protocol in the P2P network. You can check the introduction of Gossip, which contains a simulator that can tell you the time and bandwidth required for messages to propagate in the cluster. The biggest advantage of the Gossip protocol is that even if the number of cluster nodes increases, the load on each node will not increase much and is almost constant. This allows Consul-managed clusters to scale out to thousands of nodes.</p>
<h1 id="appendix-2-path-algorithms"><a class="header" href="#appendix-2-path-algorithms">Appendix 2: Path Algorithms</a></h1>
<p>Find the optimal path for evacuate.</p>
<p>Not only for compute node HA, but also for load optimization and balancing</p>
<p>Try to design the migration path algorithm to optimize the performance of the node where the VM is located to maximize the return on hardware investment.</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-15 06:47</p>
<h1 id="update-package"><a class="header" href="#update-package">update package</a></h1>
<pre><code>$ sudo apt-get update
$ sudo apt-get upgrade
$ sudo reboot
</code></pre>
<h1 id="install-influxdb"><a class="header" href="#install-influxdb">install influxdb</a></h1>
<pre><code>hanwei@ubuntu-lab:~$ wget https://dl.influxdata.com/influxdb/releases/influxdb_1.2.4_amd64.deb
--2017-06-14 16:37:09--  https://dl.influxdata.com/influxdb/releases/influxdb_1.2.4_amd64.deb
Resolving dl.influxdata.com (dl.influxdata.com)... 52.84.167.167, 52.84.167.39, 52.84.167.178, ...
Connecting to dl.influxdata.com (dl.influxdata.com)|52.84.167.167|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 17305080 (17M) [application/x-debian-package]
Saving to: ‘influxdb_1.2.4_amd64.deb’

influxdb_1.2.4_amd64.deb         100%[========================================================&gt;]  16.50M   875KB/s    in 19s     

2017-06-14 16:37:29 (874 KB/s) - ‘influxdb_1.2.4_amd64.deb’ saved [17305080/17305080]

hanwei@ubuntu-lab:~$ sudo dpkg -i influxdb_1.2.4_amd64.deb
[sudo] password for hanwei: 
Selecting previously unselected package influxdb.
(Reading database ... 63342 files and directories currently installed.)
Preparing to unpack influxdb_1.2.4_amd64.deb ...
Unpacking influxdb (1.2.4-1) ...
Setting up influxdb (1.2.4-1) ...
Created symlink from /etc/systemd/system/influxd.service to /lib/systemd/system/influxdb.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/influxdb.service to /lib/systemd/system/influxdb.service.
Processing triggers for man-db (2.7.5-1) ...
hanwei@ubuntu-lab:~$ service influxdb start
==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ===
Authentication is required to start 'influxdb.service'.
Authenticating as: hanwei,,, (hanwei)
Password: 
==== AUTHENTICATION COMPLETE ===
hanwei@ubuntu-lab:~$ service influxdb status
● influxdb.service - InfluxDB is an open-source, distributed, time series database
Loaded: loaded (/lib/systemd/system/influxdb.service; enabled; vendor preset: enabled)
Active: active (running) since Wed 2017-06-14 16:50:05 CST; 4s ago
    Docs: https://docs.influxdata.com/influxdb/
Main PID: 2156 (influxd)
    Tasks: 7
Memory: 4.9M
    CPU: 49ms
CGroup: /system.slice/influxdb.service
        └─2156 /usr/bin/influxd -config /etc/influxdb/influxdb.conf 
</code></pre>
<h1 id="modify-the-influxdb-configuration-file"><a class="header" href="#modify-the-influxdb-configuration-file">Modify the influxdb configuration file</a></h1>
<pre><code>hanwei@ubuntu-lab:~$ vim /etc/influxdb/influxdb.conf
[admin]
# Determines whether the admin service is enabled.
# enabled = false

# The default bind address used by the admin service.
# bind-address = &quot;:8083&quot;

# Whether the admin service should use HTTPS.
# https-enabled = false
</code></pre>
<p>change to: </p>
<pre><code>enabled = true
bind-address = &quot;:8083&quot;
</code></pre>
<h1 id="create-database-collectd-in-infuxdb"><a class="header" href="#create-database-collectd-in-infuxdb">Create database Collectd in infuxdb</a></h1>
<p>Two ways CLI or web ui</p>
<p>http://192.168.206.144:8086/query?q=CREATE+DATABASE+%22collectd%22&amp;db=collectd</p>
<p>or use web ui</p>
<p><img src="openstack/2023-01-17-11-27-16.png" alt="" /></p>
<h1 id="install-collectd"><a class="header" href="#install-collectd">Install collectd</a></h1>
<pre><code>$ sudo apt-get install collectd
</code></pre>
<p>Configure Collectd as the client, and send the collected data directly to InfluxDB</p>
<pre><code>$ sudo vi /etc/collectd/collectd.conf
...
LoadPlugin network
...
&lt;Plugin network&gt;
        Server &quot;192.168.2.183&quot; &quot;25826&quot;
&lt;/Plugin&gt;
...
</code></pre>
<p>Restart collectd:</p>
<pre><code>$ sudo /etc/init.d/collectd restart
</code></pre>
<h1 id="configure-the-collectd-plugin-that-comes-with-influxdb"><a class="header" href="#configure-the-collectd-plugin-that-comes-with-influxdb">Configure the Collectd plugin that comes with InfluxDB</a></h1>
<p>The Collectd plug-in that comes with InfluxDB is disabled by default. It needs to be manually configured to enable enabled = true, and fill in the line database = &quot;collectd&quot;. The &quot;collectd&quot; here is the database we created above. Remember to restart InfluxDB after changing the configuration</p>
<pre><code>$ sudo vim /etc/influxdb/shared/influxdb.conf
...
# Configure the collectd api
[input_plugins.collectd]
enabled = true
# address = &quot;0.0.0.0&quot; # If not set, is actually set to bind-address.
# port = 25826
database = &quot;collectd&quot;
# types.db can be found in a collectd installation or on github:
# https://github.com/collectd/collectd/blob/master/src/types.db
# typesdb = &quot;/usr/share/collectd/types.db&quot; # The path to the collectd types.db file
...
hanwei@ubuntu-lab:~$ sudo service influxdb restart
</code></pre>
<p>Check the port opened on the server and you will find that the influxdb plug-in has started a 25826 port. If you find that there is no (collected) data in the InfluxDB database, be sure to check whether the 25826 port is started normally</p>
<pre><code>hanwei@ubuntu-lab:~$ sudo netstat -tupln
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 192.168.122.1:53        0.0.0.0:*               LISTEN      1649/dnsmasq    
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1308/sshd       
tcp        0      0 127.0.0.1:5432          0.0.0.0:*               LISTEN      1437/postgres   
tcp        0      0 127.0.0.1:6010          0.0.0.0:*               LISTEN      1740/0          
tcp        0      0 127.0.0.1:6011          0.0.0.0:*               LISTEN      10471/1         
tcp6       0      0 :::8083                 :::*                    LISTEN      10490/influxd   
tcp6       0      0 :::8086                 :::*                    LISTEN      10490/influxd   
tcp6       0      0 :::22                   :::*                    LISTEN      1308/sshd       
tcp6       0      0 :::8088                 :::*                    LISTEN      10490/influxd   
tcp6       0      0 ::1:5432                :::*                    LISTEN      1437/postgres   
tcp6       0      0 ::1:6010                :::*                    LISTEN      1740/0          
tcp6       0      0 ::1:6011                :::*                    LISTEN      10471/1         
udp        0      0 0.0.0.0:53466           0.0.0.0:*                           10239/collectd  
udp        0      0 192.168.122.1:53        0.0.0.0:*                           1649/dnsmasq    
udp        0      0 0.0.0.0:67              0.0.0.0:*                           1649/dnsmasq    
udp        0      0 0.0.0.0:68              0.0.0.0:*                           1250/dhclient   
udp6       0      0 :::25826                :::*                                10490/influxd
</code></pre>
<h1 id="check-if-influxdb-has-data-from-collectd"><a class="header" href="#check-if-influxdb-has-data-from-collectd">Check if InfluxDB has data from Collectd</a></h1>
<p>InfluxDB is ready to accept and process data from Collectd. Use the command line or the web management interface to verify whether there is data in the database</p>
<pre><code>hanwei@ubuntu-lab:~$ influx
Connected to http://localhost:8086 version 1.2.4
InfluxDB shell version: 1.2.4
&gt; show databases;
name: databases
name
----
_internal
mydb
&gt; use collectd
Using database collectd
&gt; show measurements
name: measurements
name
----
cpu_value
df_value
disk_io_time
disk_read
disk_value
disk_weighted_io_time
disk_write
entropy_value
interface_rx
interface_tx
irq_value
load_longterm
load_midterm
load_shortterm
memory_value
processes_value
swap_value
users_value
&gt; SELECT * from cpu_value
...
7435180464451600 ubuntu-lab.localdomain 0        cpu  nice          722
1497435180464452775 ubuntu-lab.localdomain 0        cpu  interrupt     0
1497435180464454083 ubuntu-lab.localdomain 0        cpu  softirq       339
1497435180464454592 ubuntu-lab.localdomain 0        cpu  steal         0
1497435180464455103 ubuntu-lab.localdomain 0        cpu  idle          592750
1497435190462593838 ubuntu-lab.localdomain 0        cpu  user          4281
1497435190462598610 ubuntu-lab.localdomain 0        cpu  system        5599
1497435190462600580 ubuntu-lab.localdomain 0        cpu  wait          7799
1497435190462602296 ubuntu-lab.localdomain 0        cpu  nice          722
1497435190462603649 ubuntu-lab.localdomain 0        cpu  interrupt     0
1497435190462604905 ubuntu-lab.localdomain 0        cpu  softirq       339
1497435190462605510 ubuntu-lab.localdomain 0        cpu  steal         0
1497435190462606004 ubuntu-lab.localdomain 0        cpu  idle          593745
1497435200463552177 ubuntu-lab.localdomain 0        cpu  user          4285
1497435200463557277 ubuntu-lab.localdomain 0        cpu  system        5608
1497435200463558626 ubuntu-lab.localdomain 0        cpu  wait          7799
1497435200463560045 ubuntu-lab.localdomain 0        cpu  nice          722
1497435200463561496 ubuntu-lab.localdomain 0        cpu  interrupt     0
1497435200463562866 ubuntu-lab.localdomain 0        cpu  softirq       339
...
</code></pre>
<p>or web ui</p>
<p><img src="openstack/2023-01-17-11-29-03.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-14 15:25</p>
<blockquote>
<p>nova live-migration --block-migrate</p>
</blockquote>
<h1 id="the-role-of-virtual-machine-live-migration"><a class="header" href="#the-role-of-virtual-machine-live-migration">The role of virtual machine live migration</a></h1>
<p>Every reader may ask such a question, the virtual machine is used well, why should it be migrated? That is, where is the value and purpose of migration. In the daily operation and maintenance of the data center, the following scenarios and requirements are often dealt with. After understanding these requirements, this question will have an answer.</p>
<ul>
<li>Requirement 1: Maintenance, fault repair and upgrade of the physical machine hardware system, but the virtual machine running on this physical machine cannot be shut down, because the user's important services run on it.</li>
<li>Requirement 2: The software system of the physical machine is upgraded and patched. In order not to affect the virtual machine running on it, the virtual machine needs to be migrated to another physical machine before upgrading and patching.</li>
<li>Requirement 3: The load on a physical machine is too heavy, and some virtual machines need to be reduced to release resources.</li>
<li>Requirement 4: In a cluster, some physical machines have too many virtual machines, and some physical machines have too few virtual machines. Resource balance needs to be done.</li>
</ul>
<p>In addition to the above four main requirements, from a service point of view, Live migration has the following two benefits:</p>
<ul>
<li>Benefit 1: The maintenance and upgrade of software and hardware systems will not affect the key services of users, which improves the high availability of services and users satisfaction.</li>
<li>Benefit 2: The system administrator does not need to work overtime, and upgrades the system in the middle of the night. This work can be completed during normal working hours, reducing the company's maintenance costs.</li>
</ul>
<h1 id="basic-concept"><a class="header" href="#basic-concept">basic concept</a></h1>
<p>Before understanding live migration, you must understand the image file format QCOW2. Qcow2 is currently the image format recommended by QEMU. It supports sparse files to save storage space, supports encryption to improve the security of image files, and supports zlib-based compression. Qcow2 image can be used to save the changes of another image file, it does not modify the original image file, which is also called backing_file. Only the image file that is different from the original image file is recorded. This kind of image file is called copy-on-write image. Although it is a separate image file, most of its data comes from the original image, and only based on the original image Only the incremental part of the file will be recorded. The virtual machines mentioned in this article are all created by OpenStack using Qcow2 format image files, as shown in the figure, including two parts.</p>
<ol>
<li>Back-end image (libvirt base)</li>
<li>Separate incremental image file of virtual machine (libvirt instance disks), copy-on-write image</li>
</ol>
<p><img src="openstack/2023-01-17-11-10-35.png" alt="" /></p>
<pre><code>[root@NFJD-TESTN-COMPUTE-1 ~]# cd /var/lib/nova/instances/
[root@NFJD-TESTN-COMPUTE-1 ~]# tree
├── b9530e7b-2309-4eb5-bec3-180241c1e3a2
│   ├── console.log
│   ├── disk
│   ├── disk.config
│   ├── disk.info
│   └── libvirt.xml
├── ...
├── _base
│   ├── 0522bc602608d45758d815b01a6899ff3e1e3e27
│   ├── 05252f6d26980b56fbf93a14c5e70910f18b412c
│   ├── ...
</code></pre>
<p>Use qemu-img to view the information of the virtual machine's separate incremental image file, we can see that his backing file is the image file in the _base directory</p>
<pre><code>[root@NFJD-TESTN-COMPUTE-1 ~]# cd /var/lib/nova/instances/
[root@NFJD-TESTN-COMPUTE-1 instances]# cd 852e1a26-bd49-4149-bd24-552eb4b37034
[root@NFJD-TESTN-COMPUTE-1 852e1a26-bd49-4149-bd24-552eb4b37034]# ls
console.log  disk  disk.config  disk.info  libvirt.xml
[root@NFJD-TESTN-COMPUTE-1 852e1a26-bd49-4149-bd24-552eb4b37034]# qemu-img info disk
image: disk
file format: qcow2
virtual size: 20G (21474836480 bytes)
disk size: 281M
cluster_size: 65536
backing file: /var/lib/nova/instances/_base/731d527f50917ff3364203ebbcf8d4c22dc7919c
Format specific information:
    compat: 1.1
    lazy refcounts: false
    refcount bits: 16
    corrupt: false
</code></pre>
<p>After spending so much time introducing QCOW2, you may wonder, what is the purpose? In fact, the back-end image (libvirt Base) and the incremental image file (libvirt instance disks) of the virtual machine described above are the data to be migrated. The ultimate goal of live migration is to completely migrate them from the source physical host to the target physical host. In addition to the two of them, there is another object that needs to be migrated, which is the data of the virtual machine running in memory.</p>
<p>The transfer of data involves the transmission of data, and the transmission of data needs to go through the network. This article introduces the use of TCP network protocol to realize dynamic migration. Libvirt does not support the TCP protocol by default, you need to modify the configuration of libvirt to enable libvirt to support the TCP protocol, and the following chapters will introduce how to configure it in detail. During the migration process, the libvirtd process running in the destination physical host (Dest Host) needs to create a URI based on the address and port. The URI is used by the destination physical host to receive data and send data back to the source physical host (Source Host). libvirtd process.</p>
<p>On the destination physical host and the source physical host, as long as the following commands can be executed, it means that data can be transferred.</p>
<p><strong>Execute on compute01:</strong></p>
<pre><code>[root@compute01]# virsh -c qemu+tcp://nova@compute02/system
</code></pre>
<p><strong>Execute on compute02:</strong></p>
<pre><code>[root@compute01]# virsh -c qemu+tcp://nova@compute01/system
</code></pre>
<p>Such as:</p>
<pre><code>[root@NFJD-TESTN-COMPUTE-1 instances]# virsh -c qemu+tcp://nova@NFJD-TESTN-COMPUTE-2/system
欢迎使用 virsh，虚拟化的交互式终端。

输入：'help' 来获得命令的帮助信息
    'quit' 退出

virsh # list
Id    名称                         状态
----------------------------------------------------
9     instance-000057c2              暂停
10    instance-000057c6              running
24    instance-0000581a              暂停
25    instance-0000581f              running
31    instance-00005833              running
...
</code></pre>
<h1 id="migration-steps"><a class="header" href="#migration-steps">Migration steps</a></h1>
<p>Now that the basic concept of migration is clarified, let's continue to introduce the steps of migration. A normal process of OpenStack live migration mainly includes four parts: pre-migration condition check, pre-migration preprocessing, migration, and post-migration processing.</p>
<h2 id="condition-checks-before-migration"><a class="header" href="#condition-checks-before-migration">Condition checks before migration</a></h2>
<p>For live migration to execute successfully, some conditions must be met, so some condition checks must be done before performing the migration.</p>
<ol>
<li>Permission check, whether the user performing the migration has sufficient permissions to perform dynamic migration.</li>
<li>Parameter check, whether the parameters passed to the API are sufficient and correct, such as whether the block-migrate parameter is specified.</li>
<li>Check whether the target physical host exists.</li>
<li>Check whether the migrated virtual machine is running.</li>
<li>Check whether the nova-compute service on the source and destination physical hosts is running normally.</li>
<li>Check whether the destination physical host and the source physical host are the same machine.</li>
<li>Check whether the destination physical host has enough memory.</li>
<li>Check whether the hypervisor and hypervisor versions of the destination and source physical host machines are the same.</li>
</ol>
<h2 id="preprocessing-before-migration"><a class="header" href="#preprocessing-before-migration">Preprocessing before migration</a></h2>
<p>Before actually performing the migration, you must do some warm-up and do some preparatory work.</p>
<ol>
<li>Obtain and prepare the block device (volume) mounted by the virtual machine on the target physical host.</li>
<li>Set up the network (networks) of the virtual machine on the destination physical host.</li>
<li>Set up a virtual machine firewall (fireware) on the destination physical host.</li>
</ol>
<h2 id="migrate"><a class="header" href="#migrate">migrate</a></h2>
<p>After the conditions are met and the preprocessing work is completed, live migration can be performed. The main steps are as follows:</p>
<ol>
<li>Call the libvirt python interface migrateToURI to migrate the source host to the destination host.</li>
</ol>
<ul>
<li>dom.migrateToURI(CONF.live_migration_uri % dest,logical_sum,None,CONF.live_migration_bandwidth)</li>
<li>live_migration_uri: This URI is defined by the libvirtd process introduced in 3.2.2.</li>
<li>live_migration_bandwidth: This parameter defines the maximum bandwidth used during the migration.</li>
</ul>
<ol start="2">
<li>Call the wait_for_live_migration method cyclically at a certain time interval (0.5) to detect the status of the virtual machine migration until the virtual machine is successfully migrated.</li>
</ol>
<h2 id="post-migration-processing"><a class="header" href="#post-migration-processing">Post-migration processing</a></h2>
<p>After the virtual machine migration is complete, some aftermath work needs to be done.</p>
<ol>
<li>Detach volume on the source physical host.</li>
<li>Release the security group ingress rule on the source physical host.</li>
<li>Update the status of the virtual machine in the database on the destination physical host.</li>
<li>Delete the virtual machine on the source physical host.</li>
</ol>
<p>After the above four steps are completed normally, the virtual machine is successfully migrated from the source physical host to the destination physical host. The system administrator can then perform the administrative tasks listed in Chapter 2.</p>
<h1 id="configuration-for-live-migration"><a class="header" href="#configuration-for-live-migration">Configuration for Live Migration</a></h1>
<p>This section lists the configurations that support live migration. You must ensure that the configurations on all physical hosts are correct before live migration can be successfully completed.</p>
<h2 id="libvirt"><a class="header" href="#libvirt">libvirt</a></h2>
<p>By default, libvirt supports the TLS protocol for remote connections, but does not support the TCP protocol, so listen_tls=0 listen_tcp=1 enables libvirt to support the TCP protocol.</p>
<ol>
<li>
<p>Modify the /etc/sysconfig/libvirtd file.</p>
<p>LIBVIRTD_ARGS=&quot;--listen&quot;</p>
</li>
<li>
<p>Do the following configuration in the /etc/libvirt/libvirtd.conf file.</p>
<p>listen_tls=0
listen_tcp=1
auth_tcp=&quot;none&quot;</p>
</li>
<li>
<p>Restart the libvirtd service</p>
</li>
</ol>
<h2 id="firewall"><a class="header" href="#firewall">firewall</a></h2>
<p>Configure /etc/sysconfig/iptables to open TCP port 16509.</p>
<pre><code>-A INPUT -p tcp -m multiport --ports 16509 -m comment --comment &quot;libvirt&quot; -j ACCEPT
</code></pre>
<h2 id="openstack-nova"><a class="header" href="#openstack-nova">OpenStack Nova</a></h2>
<p>Configure the live_migration flag in the /etc/nova/nova.conf file.</p>
<pre><code>live_migration_flag=VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-10 03:27</p>
<h1 id="relevant-blueprints"><a class="header" href="#relevant-blueprints">Relevant blueprints:</a></h1>
<ul>
<li>https://blueprints.launchpad.net/nova/+spec/mark-host-down</li>
<li>https://blueprints.launchpad.net/python-novaclient/+spec/support-force-down-service</li>
</ul>
<h1 id="why-add-this-api"><a class="header" href="#why-add-this-api">Why add this API</a></h1>
<p>With this API, external fault monitoring can get the nova-compute service down faster, which will directly improve the fault recovery time of VM HA (the time to evacuate a VM from a faulty node to a normal node).
external fault monitoring system a possibility of telling OpenStack Nova fast that compute host is down. This will immediately enable calling of evacuation of any VM on host and further enabling faster HA actions.</p>
<h1 id="rest-api-for-forced-down"><a class="header" href="#rest-api-for-forced-down">REST API for forced down</a></h1>
<p>request: PUT /v2.1/{tenant_id}/os-services/force-down { “binary”: “nova-compute”, “host”: “compute1”, “forced_down”: true }</p>
<p>response: 200 OK { “service”: { “host”: “compute1”, “binary”: “nova-compute”, “forced_down”: true } }</p>
<p>Example: curl -g -i -X PUT http://{service_host_ip}:8774/v2.1/{tenant_id}/os-services /force-down -H “Content-Type: application/json” -H “Accept: application/json ” -H “X-OpenStack-Nova-API-Version: 2.11” -H “X-Auth-Token: {token}” -d ‘{“b inary”: “nova-compute”, “host”: “compute1”, “forced_down”: true}’</p>
<h1 id="cli-for-forced-down"><a class="header" href="#cli-for-forced-down">CLI for forced down</a></h1>
<p>nova service-force-down nova-compute</p>
<p>Example: nova service-force-down compute1 nova-compute</p>
<h1 id="rest-api-for-disabling-forced-down"><a class="header" href="#rest-api-for-disabling-forced-down">REST API for disabling forced down</a></h1>
<p>request: PUT /v2.1/{tenant_id}/os-services/force-down { “binary”: “nova-compute”, “host”: “compute1”, “forced_down”: false }</p>
<p>response: 200 OK { “service”: { “host”: “compute1”, “binary”: “nova-compute”, “forced_down”: false } }</p>
<p>Example: curl -g -i -X PUT http://{service_host_ip}:8774/v2.1/{tenant_id}/os-services /force-down -H “Content-Type: application/json” -H “Accept: application/json ” -H “X-OpenStack-Nova-API-Version: 2.11” -H “X-Auth-Token: {token}” -d ‘{“b inary”: “nova-compute”, “host”: “compute1”, “forced_down”: false}’</p>
<h1 id="cli-for-disabling-forced-down"><a class="header" href="#cli-for-disabling-forced-down">CLI for disabling forced down</a></h1>
<p>nova service-force-down –unset nova-compute</p>
<p>Example: nova service-force-down –unset compute1 nova-compute</p>
<h1 id="add-fast-evacuation-function-to-vm-ha-procedure"><a class="header" href="#add-fast-evacuation-function-to-vm-ha-procedure">Add fast evacuation function to VM HA procedure</a></h1>
<p>Now add fast evacuation function in the developed VM HA program</p>
<ol>
<li>Modify the configuration file and add the quick_evacuate_level option</li>
</ol>
<ul>
<li>level 1: (fastest) Do not shut down, just force down nova-compute-service</li>
<li>level 2: (faster) (default) Shut down first, then force down nova-compute-service</li>
<li>level 3: (disable quick_evacuate)</li>
</ul>
<ol start="2">
<li>change python-novaclient&gt;=6.0.0 to support force-service-down</li>
<li>Modify the processing logic of the evacuation process and add rapid evacuation</li>
</ol>
<p>The original evacuation process is to shut down the faulty computing node after finding the faulty node, and then wait for the faulty node to cause nova service state down due to heartbeat loss, and then perform evacuation.</p>
<p>Now it is changed to:</p>
<ul>
<li>level 1: (fastest) does not shut down, only force down nova-compute -service</li>
<li>level 2: (faster) (default) Shut down first, then force down after shutdown introduction nova-compute-service</li>
<li>level 3: (disable quick_evacuate) do nothing, same as the original process</li>
</ul>
<p>The following code def _ipmi_handle, before the change, shuts down the faulty node. After the change, at level 1, force down nova service state and skip the shutdown process.</p>
<pre><code class="language-python">def _ipmi_handle(self, node):
    # cmd = 'ssh %s systemctl stop openstack-nova-compute' % node
    # return os.system(cmd)
    ret = 1
################  add  ################
    if CONF.quick_evacuate_level == 1:
        try:
            self.nova_client.services.force_down(host=node,
                                                 binary='nova-compute',
                                                 force_down=True,
                                                )
            service = self.nova_client.services.list(
                host=node,
                binary='nova-compute')
            if service:
                if service[0].state == 'down':
                    return ret
            raise
        except Exception as e:
            content = node + ' nova-compute service force down failed!'
            title = 'force down failed'
            self.db.add_or_update_guardian_log(**{'title': title,
                                                  'detail': content,
                                                  'level': 'ERROR'})
            LOG.error(content)
            LOG.error(e)
################  add  ################
    LOG.info('IPMI enabled,need to check power status of %s' % node)
    ipmi_info = self.db.get_ipmiInfo(node)
    if ipmi_info is None:
        LOG.warning(
            'can not find ipmi info for %s, will ignore evacuate...'
            % node)
        return 0
    LOG.debug('get ipmi info (ip %s user %s, password %s)' % (
        ipmi_info.ip, ipmi_info.username, ipmi_info.password))
    ipmi_util = IPMIUtil(ipmi_info.ip, ipmi_info.username,
                         ipmi_info.password)
    power_status = ipmi_util.get_power_status()
    LOG.info('power status of %s is %s' % (node, power_status))

    if 'off' != power_status:
        LOG.info('power status of %s is not off,power off it...' % node)
        ipmi_util.do_power_off()
        for i in range(1, CONF.ipmi_check_max_count + 1):
            power_status = ipmi_util.get_power_status()
            if 'off' != power_status:
                LOG.info(
                    'power status of %s is not off,wait for %s seconds...'
                    % (node, CONF.ipmi_check_interval))
                time.sleep(CONF.ipmi_check_interval)
            else:
                break
        power_status = ipmi_util.get_power_status()
        if 'off' != power_status:
            LOG.info(
                'after %s check,can not confirm power status of '
                '%s is off,will ignore evacuate...'
                % (node,
                   CONF.ipmi_check_max_count*CONF.ipmi_check_interval))
            ret = 0
    return ret
</code></pre>
<p>When the following code is at level 2, it does not skip the shutdown process of the above code, and force down before checking the nova service state to achieve rapid evacuation (do not check the heartbeat process). Level 3 is to do nothing, the same as before.</p>
<pre><code class="language-python">################  add  ################
if CONF.quick_evacuate_level == 2:
    try:
        self.nova_client.services.force_down(host=node,
                                             binary='nova-compute',
                                             force_down=True,
                                            )
        service = self.nova_client.services.list(
            host=node,
            binary='nova-compute')
        if service:
            if service[0].state != 'down':
                raise
    except Exception as e:
        content = node + ' nova-compute service force down failed!'
        title = 'force down failed'
        self.db.add_or_update_guardian_log(**{'title': title,
                                              'detail': content,
                                              'level': 'ERROR'})
        LOG.error(content)
        LOG.error(e)
################  add  ################

if self._wait_for_service_to_down(node):
    for server in servers:
        can_evacuate = self._evacuate_predict(server)
        if can_evacuate:
            try:
                # just evacuate, let nova scheduler
                # choose one host if not in drbd env
                respone = server.evacuate(
                    host=evacuated_to,
                    on_shared_storage=CONF.on_share_storage)
                LOG.info(
                    'evacuate vm info:[id:%s,name:%s,status:%s],'
                    'reponse is %s'
                    % (server.id, server.name, server.status,
                        respone))
                content = '虚拟机被疏散:虚拟机ID: %s' % server.id
                self.send_snmp(content=content)
            except Exception as e:
                content = ('evacuate vm %s failed' % server.id)
                title = ('疏散虚拟机%s失败' % server.id)
                self.db.add_or_update_guardian_log(
                    **{'title': title,
                       'detail': content,
                       'level': 'ERROR'})
                content = '虚拟机疏散调用API失败:虚拟机ID: %s' % server.id
                self.send_snmp(content=content)
                LOG.error(content)
                LOG.error(e)

            # add to db
            try:
                self.db.add_evacuate_history(
                    instance_id=server.id,
                    instance_name=server.name,
                    host=getattr(server, 'OS-EXT-SRV-ATTR:host'))

                content = (
                    'add_evacuate_history '
                    '[instance_id:%s,instance_name:%s,host:%s]'
                    % (server.id, server.name,
                       getattr(server, 'OS-EXT-SRV-ATTR:host')))
                LOG.info(content)
                title = ('增加疏散记录%s' % server.name)
                self.db.add_or_update_guardian_log(
                    **{'title': title,
                       'detail': content,
                       'level': 'INFO'})
            except Exception as e:
                content = (
                    'add_evacuate_history for vm %s failed'
                    % server.id)
                title = ('增加疏散记录%s失败' % server.id)
                self.db.add_or_update_guardian_log(
                    **{'title': title,
                       'detail': content,
                       'level': 'ERROR'})
                LOG.error(content)
                LOG.error(e)
        else:
            content = '虚拟机没有被疏散:虚拟机ID: %s' % server.id
            self.send_snmp(content=content)
            LOG.info('server %s can not evacuate' % server.id)

    # disable this service
    try:
        self.nova_client.services.disable(node, 'nova-compute')
    except Exception as e:
        content = (
            'failed to disable compute service on node %s' % node)
        title = ('disable节点%s服务失败' % node)
        self.db.add_or_update_guardian_log(
            **{'title': title,
               'detail': content,
               'level': 'ERROR'})
        LOG.error(content)
        LOG.error(e)
        raise

    heart_beat = self.db.get_heartbeat_by_name(node)
    heart_beat.has_evacuated = 1
    heart_beat.status = 'done'
    heart_beat.save(self.db.session)
    LOG.info('node %s update with [has_evacuated: 1]' % node)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-05-04 22:21</p>
<h1 id="question-survey"><a class="header" href="#question-survey">question survey</a></h1>
<h2 id="direct-cause"><a class="header" href="#direct-cause">direct cause</a></h2>
<p>After investigation, we found that due to the cloud environment, There is a deviation between the time of the windows virtual machine and the time of the physical machine, which causes the activation to fail.</p>
<h2 id="specific-time-deviation"><a class="header" href="#specific-time-deviation">Specific time deviation</a></h2>
<p>Windows instance sometimes finds that the operating system time is always 8 hours behind. Even if the time and time zone are manually adjusted, the difference will be 8 hours after the next restart of the instance.</p>
<h2 id="why-is-there-a-time-deviation"><a class="header" href="#why-is-there-a-time-deviation">Why is there a time deviation?</a></h2>
<p>KVM handles system time differently for Linux and Windows virtual machines, and Windows requires some additional settings.</p>
<p>Why do Linux and Windows virtual machines handle system time differently? To understand the following three concepts.</p>
<p>###UTC time and local time</p>
<p>UTC time: Also known as the world standard time and the unified world time, UTC is calibrated by atomic clocks, and other parts of the world use this as the reference time plus their own time zone to set their local time</p>
<p>Local time: Due to different time zones, local time is generally different from UTC, and the conversion method is</p>
<p>Local time = UTC + time zone or UTC = local time - time</p>
<p>zone The east of the time zone is positive, and the west is negative. In China, the time zone is the east eighth district, which is +8 district. The local time uses Beijing time, which is displayed on linux as CST , so CST=UTC+(+8 hours) or UTC=CST-(+8 hours)</p>
<h3 id="guest-os-time-keeping"><a class="header" href="#guest-os-time-keeping">guest OS time keeping</a></h3>
<p>The kvm technology is fully virtualized, and the guest OS can run directly without modification. However, there are problems in timing. One way of guest OS timing is to count through clock interrupts and then convert them, but the clock interrupts generated by the host Can't reach all guest OS in time, because interrupt in guest OS is not really hardware interrupt, it's interrupt injected by host</p>
<p>Many network applications, session verification in the web, etc., will call the system time. If the time in the guest OS is wrong, it will cause a program error. For this reason, kvm provides a paravirtualized clock (kvm-clock) for the guest vms. In RHEL5.5 and above, kvm-clock is used as the default clock source, you can use the following command in the guest to check whether kvm-clock is used</p>
<pre><code>cat /sys/devices/system/clocksource/clocksource0/current_clocksource
kvm-clock
</code></pre>
<p>In the kvm-clock mode, the guest OS cannot directly access the host clock, and the host writes the system time into a memory page that the guest can read, so that the guest can read this memory page to set its own hardware time, but the host does not update it in real time The time is up to this memory page, but it is updated when a vm event (vm shutdown, restart, etc.) occurs, so this method cannot keep the guest time accurate</p>
<h3 id="set-the-virtual-machine-hardware-clock-in-libvirt"><a class="header" href="#set-the-virtual-machine-hardware-clock-in-libvirt">Set the virtual machine hardware clock in libvirt</a></h3>
<p>The kvm virtual machine is generally managed by libvirt. In the xml file of the virtual machine configuration, there are items related to the hardware clock setting of the virtual machine.</p>
<pre><code>&lt;clock offset='localtime'&gt;
&lt;/clock&gt;
</code></pre>
<p>The offset attribute of the clock has four options: &quot;utc&quot;, &quot;localtime&quot;, &quot;timezone&quot;, and &quot;variable&quot;.</p>
<p>If the guest OS is a Linux system, &quot;utc&quot; should be selected. The guest OS will synchronize the utc time with the host once when it starts, and then calculate the system time according to the time zone set in /etc/localtime</p>
<p>If the guest OS is a windows system, &quot;localtime&quot; should be selected, and the guest OS will synchronize the system time with the host once at startup</p>
<h1 id="countermeasures-in-the-openstack-cloud-environment"><a class="header" href="#countermeasures-in-the-openstack-cloud-environment">Countermeasures in the openstack cloud environment</a></h1>
<h2 id="for-new-windows-virtual-machines"><a class="header" href="#for-new-windows-virtual-machines">For new windows virtual machines</a></h2>
<p>It is to add os_type=&quot;windows&quot; information to the existing windows image and the new windows image created in the future</p>
<pre><code>[root@build localhost]# glance image-update  --property os_type=&quot;windows&quot; 1d2afd0f-9162-415c-ace3-97de254b6c9c
1d2afd0f-9162-415c-ace3-97de254b6c9c 是vm id
</code></pre>
<h2 id="for-existing-windows-virtual-machines"><a class="header" href="#for-existing-windows-virtual-machines">For existing windows virtual machines</a></h2>
<p>To modify the database through violence, the specific plan:</p>
<ol>
<li>Find out the uuid information of all the existing windows virtual machines to be modified</li>
<li>Enter the nova database, modify the os_type field of the windows virtual machine in the instances table, and modify the field For windows, for example, modify the os_type field of the vm whose vm id is f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd to windows</li>
<li>update instances set os_type='windows' where uuid='f64bf83d-a9f0-4ec5-b481-e363de19bd' Check whether the os_type field is changed to 'windows', for example, select * from instances where uuid='f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd';</li>
<li>Hard restart the virtual machine, you need to contact the customer in advance
<code># nova reboot f64bf83d-a9f0-4ec5 -b481-e3e76e19b6bd –hard –poll</code>
restart Reason: KVM will obtain the modified database information, update the XML configuration, and ensure time synchronization.</li>
</ol>
<h1 id="test-whether-the-vm-generated-by-nova-boot-will-have-the-keyword-of-image"><a class="header" href="#test-whether-the-vm-generated-by-nova-boot-will-have-the-keyword-of-image">Test: Whether the vm generated by nova boot will have the keyword of image</a></h1>
<pre><code>[root@NFJD-TESTVM-CORE-API-1 ~]# glance image-update --property os_type=&quot;windows&quot; fb5bc637-c8ab-4999-9e88-da28561fed21
+---------------------+--------------------------------------------------+
| Property | Value |
+---------------------+--------------------------------------------------+
| checksum | 8049db08971bec254be86067025d0c32 |
| container_format | bare |
| created_at | 2016-07-12T06:16:50Z |
| direct_url | sheepdog://fb5bc637-c8ab-4999-9e88-da28561fed21 |
| disk_format | qcow2 |
| hw_qemu_guest_agent | yes |
| id | fb5bc637-c8ab-4999-9e88-da28561fed21 |
| image.os_type | windows |
| min_disk | 0 |
| min_ram | 0 |
| name | Windows-Server-2008-R2-Enterprise-64bit-20160706 |
| os_distro | windows |
| os_type | windows |
| owner | 6c149dcd3cf64171b8dd972dd03bbac0 |
| protected | False |
| size | 10015539200 |
| status | active |
| tags | [] |
| updated_at | 2017-05-05T03:42:50Z |
| virtual_size | None |
| visibility | public |
+---------------------+--------------------------------------------------+
[root@NFJD-TESTVM-CORE-API-1 ~]# nova image-show fb5bc637-c8ab-4999-9e88-da28561fed21
+------------------------------+--------------------------------------------------+
| Property | Value |
+------------------------------+--------------------------------------------------+
| OS-EXT-IMG-SIZE:size | 10015539200 |
| created | 2016-07-12T06:16:50Z |
| id | fb5bc637-c8ab-4999-9e88-da28561fed21 |
| metadata hw_qemu_guest_agent | yes |
| metadata image.os_type | windows |
| metadata os_distro | windows |
| metadata os_type | windows |
| minDisk | 0 |
| minRam | 0 |
| name | Windows-Server-2008-R2-Enterprise-64bit-20160706 |
| progress | 100 |
| status | ACTIVE |
| updated | 2017-05-05T03:42:50Z |
+------------------------------+--------------------------------------------------+
[root@NFJD-TESTVM-CORE-API-1 ~]# nova boot --image fb5bc637-c8ab-4999-9e88-da28561fed21 --flavor 3 --nic net-id=ee98ec6a-e1ed-43eb-9d0a-26080d277484 t4 --availability-zone nova:NFJD-TESTN-COMPUTE-3
+--------------------------------------+-----------------------------------------------------------------------------------------+
| Property | Value |
+--------------------------------------+-----------------------------------------------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | nova |
| OS-EXT-SRV-ATTR:host | - |
| OS-EXT-SRV-ATTR:hypervisor_hostname | - |
| OS-EXT-SRV-ATTR:instance_name | instance-000056dc |
| OS-EXT-STS:power_state | 0 |
| OS-EXT-STS:task_state | scheduling |
| OS-EXT-STS:vm_state | building |
| OS-SRV-USG:launched_at | - |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| adminPass | DoLL3d65w7cN |
| config_drive | |
| created | 2017-05-05T03:46:42Z |
| flavor | m1.medium (3) |
| hostId | |
| id | f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd |
| image | Windows-Server-2008-R2-Enterprise-64bit-20160706 (fb5bc637-c8ab-4999-9e88-da28561fed21) |
| key_name | - |
| metadata | {} |
| name | t4 |
| os-extended-volumes:volumes_attached | [] |
| progress | 0 |
| security_groups | default |
| status | BUILD |
| tenant_id | 6c149dcd3cf64171b8dd972dd03bbac0 |
| updated | 2017-05-05T03:46:43Z |
| user_id | 62f52135115f4898bd0d82c1f0cd632b |
+--------------------------------------+-----------------------------------------------------------------------------------------+
[root@NFJD-TESTVM-CORE-API-1 ~]# nova show t4
+--------------------------------------+-----------------------------------------------------------------------------------------+
| Property | Value |
+--------------------------------------+-----------------------------------------------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | nova |
| OS-EXT-SRV-ATTR:host | NFJD-TESTN-COMPUTE-3 |
| OS-EXT-SRV-ATTR:hypervisor_hostname | NFJD-TESTN-COMPUTE-3 |
| OS-EXT-SRV-ATTR:instance_name | instance-000056dc |
| OS-EXT-STS:power_state | 1 |
| OS-EXT-STS:task_state | - |
| OS-EXT-STS:vm_state | active |
| OS-SRV-USG:launched_at | 2017-05-05T03:46:50.000000 |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| config_drive | True |
| created | 2017-05-05T03:46:42Z |
| flavor | m1.medium (3) |
| hostId | c34578f3d9c097be0d91fda237706199673ca7e5a87051f5aad24637 |
| id | f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd |
| image | Windows-Server-2008-R2-Enterprise-64bit-20160706 (fb5bc637-c8ab-4999-9e88-da28561fed21) |
| key_name | - |
| metadata | {} |
| name | t4 |
| net001 network | 192.168.2.92 |
| os-extended-volumes:volumes_attached | [] |
| progress | 0 |
| security_groups | default |
| status | ACTIVE |
| tenant_id | 6c149dcd3cf64171b8dd972dd03bbac0 |
| updated | 2017-05-05T03:46:51Z |
| user_id | 62f52135115f4898bd0d82c1f0cd632b |
+--------------------------------------+-----------------------------------------------------------------------------------------+
MySQL [nova]&gt; select * from instances where uuid='f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd';
+---------------------+---------------------+------------+-------+-------------+----------------------------------+----------------------------------+--------------------------------------+-----------+------------+--------------+----------+----------+-------------+----------+-----------+-------+----------+----------------------+-----------+----------------+--------------+---------------------+---------------+--------------+---------------------+-------------------+--------+---------+----------------------+------------------+---------+--------------------------------------+--------------+------------------+--------------+--------------+--------------+------------+--------------------------+---------------------+----------+------------------+--------------------+-------------------+---------+--------------+-----------+----------------------+---------+-----------+---------+--------------------+
| created_at | updated_at | deleted_at | id | internal_id | user_id | project_id | image_ref | kernel_id | ramdisk_id | launch_index | key_name | key_data | power_state | vm_state | memory_mb | vcpus | hostname | host | user_data | reservation_id | scheduled_at | launched_at | terminated_at | display_name | display_description | availability_zone | locked | os_type | launched_on | instance_type_id | vm_mode | uuid | architecture | root_device_name | access_ip_v4 | access_ip_v6 | config_drive | task_state | default_ephemeral_device | default_swap_device | progress | auto_disk_config | shutdown_terminate | disable_terminate | root_gb | ephemeral_gb | cell_name | node | deleted | locked_by | cleaned | ephemeral_key_uuid |
+---------------------+---------------------+------------+-------+-------------+----------------------------------+----------------------------------+--------------------------------------+-----------+------------+--------------+----------+----------+-------------+----------+-----------+-------+----------+----------------------+-----------+----------------+--------------+---------------------+---------------+--------------+---------------------+-------------------+--------+---------+----------------------+------------------+---------+--------------------------------------+--------------+------------------+--------------+--------------+--------------+------------+--------------------------+---------------------+----------+------------------+--------------------+-------------------+---------+--------------+-----------+----------------------+---------+-----------+---------+--------------------+
| 2017-05-05 03:46:42 | 2017-05-05 03:46:51 | NULL | 22236 | NULL | 62f52135115f4898bd0d82c1f0cd632b | 6c149dcd3cf64171b8dd972dd03bbac0 | fb5bc637-c8ab-4999-9e88-da28561fed21 | | | 0 | NULL | NULL | 1 | active | 4096 | 2 | t4 | NFJD-TESTN-COMPUTE-3 | NULL | r-ae2dhvll | NULL | 2017-05-05 03:46:50 | NULL | t4 | t4 | nova | 0 | windows | NFJD-TESTN-COMPUTE-3 | 3 | NULL | f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd | NULL | /dev/vda | NULL | NULL | True | NULL | NULL | NULL | 0 | 0 | 0 | 0 | 40 | 0 | NULL | NFJD-TESTN-COMPUTE-3 | 0 | NULL | 0 | NULL |
+---------------------+---------------------+------------+-------+-------------+----------------------------------+----------------------------------+--------------------------------------+-----------+------------+--------------+----------+----------+-------------+----------+-----------+-------+----------+----------------------+-----------+----------------+--------------+---------------------+---------------+--------------+---------------------+-------------------+--------+---------+----------------------+------------------+---------+--------------------------------------+--------------+------------------+--------------+--------------+--------------+------------+--------------------------+---------------------+----------+------------------+--------------------+-------------------+---------+--------------+-----------+----------------------+---------+-----------+---------+--------------------+
1 row in set (0.00 sec)

the instances table of database nova os_type='windows'
</code></pre>
<h1 id="similarly-for-virtual-machines-that-cannot-obtain-monitoring-data-use-the-following-methods-to-modify-the-virtual-machine-configuration"><a class="header" href="#similarly-for-virtual-machines-that-cannot-obtain-monitoring-data-use-the-following-methods-to-modify-the-virtual-machine-configuration">Similarly, for virtual machines that cannot obtain monitoring data, use the following methods to modify the virtual machine configuration:</a></h1>
<p>(1) Contact the colleague who provided the image to confirm that qemu-guest-agent has been installed in the virtual machine</p>
<p>(2) Modify the nova database to change the virtual machine field:</p>
<pre><code>MariaDB [(none)]&gt; use nova;
MariaDB [nova]&gt; insert into instance_system_metadata values(NULL,NULL,NULL,NULL,'2cf79497-d1ba-42c3-adf5-3b45c6927dfb','image_hw_qemu_guest_agent','yes',0);
注： 2cf79497-d1ba-42c3-adf5-3b45c6927dfb 为对应instance的uuid
检查：select * from instance_system_metadata where instance_uuid='3ce9d8b0-a0a3-48ed-ba54-415883f717e5';
</code></pre>
<p>(3) Modify the image used by the virtual machine</p>
<pre><code># glance image-update xxx --property hw_qemu_guest_agent=yes --property hw_ovirt_guest_agent=yes
</code></pre>
<p>(4) To hard restart the virtual machine, you need to contact the customer in advance</p>
<pre><code># nova reboot 2cf79497-d1ba-42c3-adf5-3b45c6927dfb --hard --poll
</code></pre>
<h1 id="why-modifying-os_typewindows-will-affect-the-time-of-windows-virtual-machine"><a class="header" href="#why-modifying-os_typewindows-will-affect-the-time-of-windows-virtual-machine">Why modifying os_type='windows' will affect the time of windows virtual machine</a></h1>
<h2 id="openstack-will-determine-the-xml-content-required-for-the-vm-of-the-underlying-kvm-according-to-the-content-of-os_type"><a class="header" href="#openstack-will-determine-the-xml-content-required-for-the-vm-of-the-underlying-kvm-according-to-the-content-of-os_type">openstack will determine the xml content required for the vm of the underlying kvm according to the content of os_type</a></h2>
<p>By querying a windows virtual machine and a linux virtual machine to compare the xml content, it is true that windows vm is clock offset='localtime' linux vm is clock offset='utc'</p>
<p>The <clock> element is used to determine how the guest virtual machine clock is synchronized with the host physical machine clock. The clock element has the following attributes:</p>
<p>offset determines how the guest virtual machine clock is offset from the host physical machine clock.</p>
<h3 id="windows-virtual-machine"><a class="header" href="#windows-virtual-machine">windows virtual machine</a></h3>
<pre><code>[root@NFJD-TESTVM-CORE-API-1 ~]# nova show t4
+--------------------------------------+-----------------------------------------------------------------------------------------+
| Property | Value |
+--------------------------------------+-----------------------------------------------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | nova |
| OS-EXT-SRV-ATTR:host | NFJD-TESTN-COMPUTE-3 |
| OS-EXT-SRV-ATTR:hypervisor_hostname | NFJD-TESTN-COMPUTE-3 |
| OS-EXT-SRV-ATTR:instance_name | instance-000056dc |
| OS-EXT-STS:power_state | 1 |
| OS-EXT-STS:task_state | - |
| OS-EXT-STS:vm_state | active |
| OS-SRV-USG:launched_at | 2017-05-05T03:46:50.000000 |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| config_drive | True |
| created | 2017-05-05T03:46:42Z |
| flavor | m1.medium (3) |
| hostId | c34578f3d9c097be0d91fda237706199673ca7e5a87051f5aad24637 |
| id | f64bf83d-a9f0-4ec5-b481-e3e76e19b6bd |
| image | Windows-Server-2008-R2-Enterprise-64bit-20160706 (fb5bc637-c8ab-4999-9e88-da28561fed21) |
| key_name | - |
| metadata | {} |
| name | t4 |
| net001 network | 192.168.2.92 |
| os-extended-volumes:volumes_attached | [] |
| progress | 0 |
| security_groups | default |
| status | ACTIVE |
| tenant_id | 6c149dcd3cf64171b8dd972dd03bbac0 |
| updated | 2017-05-05T03:46:51Z |
| user_id | 62f52135115f4898bd0d82c1f0cd632b |
+--------------------------------------+-----------------------------------------------------------------------------------------+
[root@NFJD-TESTN-COMPUTE-3 ~]# virsh dumpxml instance-000056dc
...
&lt;clock offset='localtime'&gt;
&lt;timer name='pit' tickpolicy='delay'/&gt;
&lt;timer name='rtc' tickpolicy='catchup'/&gt;
&lt;timer name='hpet' present='no'/&gt;
&lt;/clock&gt;
...
</code></pre>
<h3 id="linux-virtual-machine"><a class="header" href="#linux-virtual-machine">linux virtual machine</a></h3>
<pre><code>[root@NFJD-TESTVM-CORE-API-1 ~]# nova show t1
+--------------------------------------+-------------------------------------------------------------------+
| Property | Value |
+--------------------------------------+-------------------------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | nova |
| OS-EXT-SRV-ATTR:host | NFJD-TESTN-COMPUTE-1 |
| OS-EXT-SRV-ATTR:hypervisor_hostname | NFJD-TESTN-COMPUTE-1 |
| OS-EXT-SRV-ATTR:instance_name | instance-000056d9 |
| OS-EXT-STS:power_state | 1 |
| OS-EXT-STS:task_state | - |
| OS-EXT-STS:vm_state | active |
| OS-SRV-USG:launched_at | 2017-05-05T02:44:20.000000 |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| config_drive | True |
| created | 2017-05-05T02:44:11Z |
| flavor | m1.medium (3) |
| hostId | f091880caebe68654030e5df4825e49548af0037454e5723961bfc3d |
| id | 6316dc6e-2fb0-4d72-8e99-ad5749efc295 |
| image | CentOS-7.1-x86_64-20161018 (8aaf1759-9fb4-4ba9-8cff-1743eb824c5f) |
| key_name | - |
| metadata | {} |
| name | t1 |
| net001 network | 192.168.2.90 |
| os-extended-volumes:volumes_attached | [] |
| progress | 0 |
| security_groups | default |
| status | ACTIVE |
| tenant_id | 6c149dcd3cf64171b8dd972dd03bbac0 |
| updated | 2017-05-05T03:36:09Z |
| user_id | 62f52135115f4898bd0d82c1f0cd632b |
+--------------------------------------+-------------------------------------------------------------------+
[root@NFJD-TESTN-COMPUTE-1 ~]# virsh dumpxml instance-000056d9
...
&lt;clock offset='utc'&gt;
&lt;timer name='pit' tickpolicy='delay'/&gt;
&lt;timer name='rtc' tickpolicy='catchup'/&gt;
&lt;timer name='hpet' present='no'/&gt;
&lt;/clock&gt;
...
</code></pre>
<h1 id="related-nova-code"><a class="header" href="#related-nova-code">Related nova code</a></h1>
<pre><code class="language-python"># nova/virt/libvirt/driver.py

def _set_clock(self, guest, os_type, image_meta, virt_type):
    # NOTE(mikal): Microsoft Windows expects the clock to be in
    # &quot;localtime&quot;. If the clock is set to UTC, then you can use a
    # registry key to let windows know, but Microsoft says this is
    # buggy in http://support.microsoft.com/kb/2687252
    clk = vconfig.LibvirtConfigGuestClock()
    if os_type == 'windows':
        LOG.info(_LI('Configuring timezone for windows instance to '
                     'localtime'))
        clk.offset = 'localtime'  # 在os_type为windows的时候clk.offset设置为localtime
    else:
        clk.offset = 'utc'
    guest.set_clock(clk)  # 在os_type为linux的时候clk.offset设置为utc

    if virt_type == &quot;kvm&quot;:
        self._set_kvm_timers(clk, os_type, image_meta)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="summary-2"><a class="header" href="#summary-2">Summary</a></h1>
<ul>
<li>
<p><a href="./SUMMARY.html">Catalog: Cloud Native</a></p>
<ul>
<li>
<p><a href="./cloud-native/deploy-kubevirt.html">Deploy Kubernetes + KubeVirt and the basic use of KubeVirt</a> ------ release time :2022-05-06 14:17</p>
</li>
<li>
<p><a href="./cloud-native/docker-java-demo.html">docker hello-world project</a> ------ release time :2022-05-02 10:13</p>
</li>
<li>
<p><a href="./cloud-native/deploy-cinder-csi-plugin.html">Several problems encountered in deploying cinder-csi-plugin</a> ------ release time :2022-04-24 11:00</p>
</li>
<li>
<p><a href="./cloud-native/go1.18-workspace.html">New in Go 1.18 - Workspaces</a> ------ release time :2022-04-13 20:17</p>
</li>
<li>
<p><a href="./cloud-native/spring-boot-async-demo.html">spring-boot asynchronous example</a> ------ release time :2022-03-25 11:24</p>
</li>
<li>
<p><a href="./cloud-native/plg.html">PLG implements Kubernetes Pod log collection and display</a> ------ release time :2021-12-01 17:48</p>
</li>
<li>
<p><a href="./cloud-native/fluentd.html">Fluentd implements Kubernetes Pod log collection</a> ------ release time :2021-11-30 16:36</p>
</li>
<li>
<p><a href="./cloud-native/java-dev-memo-1.html">Little records in Java project development (1)</a> ------ release time :2021-11-23 15:04</p>
</li>
<li>
<p><a href="./cloud-native/jdk-bug.html">JDK bug - Encrypt Private Key failed unrecognized algorithm name PBEWithSHA1AndDESede</a> ------ release time :2021-11-23 14:22</p>
</li>
<li>
<p><a href="./cloud-native/tenant-cmp.html">Implementation of the function of creating resource pool tenants on the cloud management platform</a> ------ release time :2021-09-13 14:22</p>
</li>
<li>
<p><a href="./cloud-native/test-service-performance-2.html">iperf3 tests the four-layer performance of Kubernetes Service (Part 2)</a> ------ release time :2021-08-10 07:06</p>
</li>
<li>
<p><a href="./cloud-native/test-service-performance-1.html">iperf3 tests the four-layer performance of Kubernetes Service (Part 1)</a> ------ release time :2021-08-09 17:06</p>
</li>
<li>
<p><a href="./cloud-native/k8s-cmd.html">k8s common command</a> ------ release time :2020-05-27 17:32</p>
</li>
<li>
<p><a href="./cloud-native/cpu-binding.html">k8s supports container core binding</a> ------ release time :2020-05-22 16:24</p>
</li>
<li>
<p><a href="./cloud-native/capability.html">k8s supports Capability mechanism</a> ------ release time :2020-05-22 15:18</p>
</li>
<li>
<p><a href="./cloud-native/oom.html">k8s OOMkilled container exceeding memory limit</a> ------ release time :2020-05-22 13:52</p>
</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Openstack</a></p>
<ul>
<li>
<p><a href="./openstack/deploy-openstack-stein.html">Problems encountered in Openstack Stein deployment</a> ------ release time :2019-09-20 18:49</p>
</li>
<li>
<p><a href="./openstack/sriov2ovs.html">sriov computing node to ovs computing node script</a> ------ release time :2019-04-19 16:50</p>
</li>
<li>
<p><a href="./openstack/placement.html">Openstack Placement</a> ------ release time :2019-04-18 09:14</p>
</li>
<li>
<p><a href="./openstack/power-arch.html">POWER architecture server as computing node</a> ------ release time :2018-10-21 16:21</p>
</li>
<li>
<p><a href="./openstack/sriov.html">OpenStack practices SR-IOV computing nodes</a> ------ release time :2018-10-14 05:15</p>
</li>
<li>
<p><a href="./openstack/openstack-op-4.html">Openstack operation and maintenance records (4)</a> ------ release time :2020-01-01 19:13</p>
</li>
<li>
<p><a href="./openstack/openstack-op-3.html">Openstack operation and maintenance records (3)</a> ------ release time :2018-09-29 20:28</p>
</li>
<li>
<p><a href="./openstack/openstack-op-2.html">Openstack operation and maintenance records (2)</a> ------ release time :2018-08-28 14:48</p>
</li>
<li>
<p><a href="./openstack/openstack-op-1.html">Openstack operation and maintenance records (1)</a> ------ release time :2018-08-26 18:38</p>
</li>
<li>
<p><a href="./openstack/novnc-problem.html">OpenStack Pike dashboard noVNC cannot be accessed problem</a> ------ release time :2018-04-14 05:18</p>
</li>
<li>
<p><a href="./openstack/openstack-local-yum.html">Openstack Pike local yum source construction</a> ------ release time :2017-12-28 22:20</p>
</li>
<li>
<p><a href="./openstack/cpu-binding.html">CPU binding</a> ------ release time :2017-07-20 18:30</p>
</li>
<li>
<p><a href="./openstack/resize-fail.html">Investigation of the cause of resize failure</a> ------ release time :2017-07-04 10:20</p>
</li>
<li>
<p><a href="./openstack/mount-cloud-disk.html">Mount cloud disk</a> ------ release time :2017-06-30 05:16</p>
</li>
<li>
<p><a href="./openstack/ocata-nova-evacuate-bug.html">Ocata nova evacuate bug</a> ------ release time :2017-06-28 18:24</p>
</li>
<li>
<p><a href="./openstack/compute-node-ha.html">compute node ha mainstream open source implementation</a> ------ release time :2017-06-22 09:16</p>
</li>
<li>
<p><a href="./openstack/collectd-influxdb.html">Deployment and usage of Collectd and InfluxDB</a> ------ release time :2017-06-15 06:47</p>
</li>
<li>
<p><a href="./openstack/live-migration-local.html">Live Migration on Local Storage</a> ------ release time :2017-06-14 15:25</p>
</li>
<li>
<p><a href="./openstack/fast-evacuation.html">Add fast evacuation function to VM HA procedure</a> ------ release time :2017-06-10 03:27</p>
</li>
<li>
<p><a href="./openstack/vm-activation-failure.html">Windows virtual machine activation failure problem</a> ------ release time :2017-05-04 22:21</p>
</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Point of view</a></p>
<ul>
<li><a href="./point-of-view/huawei-and-operators.html">Huawei and telecom operators</a> ------ release time :2017-03-10 21:25</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Open source small project</a></p>
<ul>
<li><a href="./open-source-small-project/firewall-tool.html">Open source a node.js firewall tool</a> ------ release time :2022-05-07 09:41</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Tools</a></p>
<ul>
<li>
<p><a href="./tools/github-action.html">Github Action Supplementary Introduction</a> ------ release time :2022-04-12 13:04</p>
</li>
<li>
<p><a href="./tools/blog-cicd.html">Github Action Upgrade backend cloud website CICD process</a> ------ release time :2022-04-03 00:17</p>
</li>
<li>
<p><a href="./tools/github-action-demo.html">Try the Github Action CI/CD process (create a React project and package it for deployment)</a> ------ release time :2022-02-25 11:54</p>
</li>
<li>
<p><a href="./tools/notion-widget.html">Notion's page insert widget Widget</a> ------ release time :2022-01-06 11:06</p>
</li>
<li>
<p><a href="./tools/git-dir.html">Internal structure of .git directory</a> ------ release time :2023-01-18 18:37</p>
</li>
<li>
<p><a href="./tools/xshell-skill.html">Some Skills of using xshell tools</a> ------ release time :2016-07-05 22:57</p>
</li>
<li>
<p><a href="./tools/gerrit-install.html">gerrit install</a> ------ release time :2017-06-28 15:51</p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-03-10 21:25</p>
<h1 id="huawei-and-chinas-telecom-operators"><a class="header" href="#huawei-and-chinas-telecom-operators">Huawei and China's telecom operators</a></h1>
<p>Over the past few decades, operators such as Huawei and China's telecom operators have reached a long-term tacit understanding, that is, operators are responsible for operations, and Huawei provides solutions, equipment, and technologies. </p>
<p>With the development of the times and the popularity of open source, technical barriers have been broken, and operators have also started to develop technology. Operators first broke this harmony, cut off Huawei's financial resources, and gave priority to internal procurement when bidding, destroying the rules of fair competition. Huawei is not a vegetarian either. It also does what operators do, starting a public cloud.</p>
<p>In the past, Huawei did not participate in the operation of the public cloud. Huawei originally wanted to make money from technology, while operators made money from operations. Huawei's technology and global resources are by no means comparable to ordinary companies or even operators. There is nothing Huawei can't do if it wants to do something. As long as Huawei can do it, you need to know how many opponents Huawei has already put down in today's technology-heavy world.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="summary-3"><a class="header" href="#summary-3">Summary</a></h1>
<ul>
<li>
<p><a href="./SUMMARY.html">Catalog: Cloud Native</a></p>
<ul>
<li>
<p><a href="./cloud-native/deploy-kubevirt.html">Deploy Kubernetes + KubeVirt and the basic use of KubeVirt</a> ------ release time :2022-05-06 14:17</p>
</li>
<li>
<p><a href="./cloud-native/docker-java-demo.html">docker hello-world project</a> ------ release time :2022-05-02 10:13</p>
</li>
<li>
<p><a href="./cloud-native/deploy-cinder-csi-plugin.html">Several problems encountered in deploying cinder-csi-plugin</a> ------ release time :2022-04-24 11:00</p>
</li>
<li>
<p><a href="./cloud-native/go1.18-workspace.html">New in Go 1.18 - Workspaces</a> ------ release time :2022-04-13 20:17</p>
</li>
<li>
<p><a href="./cloud-native/spring-boot-async-demo.html">spring-boot asynchronous example</a> ------ release time :2022-03-25 11:24</p>
</li>
<li>
<p><a href="./cloud-native/plg.html">PLG implements Kubernetes Pod log collection and display</a> ------ release time :2021-12-01 17:48</p>
</li>
<li>
<p><a href="./cloud-native/fluentd.html">Fluentd implements Kubernetes Pod log collection</a> ------ release time :2021-11-30 16:36</p>
</li>
<li>
<p><a href="./cloud-native/java-dev-memo-1.html">Little records in Java project development (1)</a> ------ release time :2021-11-23 15:04</p>
</li>
<li>
<p><a href="./cloud-native/jdk-bug.html">JDK bug - Encrypt Private Key failed unrecognized algorithm name PBEWithSHA1AndDESede</a> ------ release time :2021-11-23 14:22</p>
</li>
<li>
<p><a href="./cloud-native/tenant-cmp.html">Implementation of the function of creating resource pool tenants on the cloud management platform</a> ------ release time :2021-09-13 14:22</p>
</li>
<li>
<p><a href="./cloud-native/test-service-performance-2.html">iperf3 tests the four-layer performance of Kubernetes Service (Part 2)</a> ------ release time :2021-08-10 07:06</p>
</li>
<li>
<p><a href="./cloud-native/test-service-performance-1.html">iperf3 tests the four-layer performance of Kubernetes Service (Part 1)</a> ------ release time :2021-08-09 17:06</p>
</li>
<li>
<p><a href="./cloud-native/k8s-cmd.html">k8s common command</a> ------ release time :2020-05-27 17:32</p>
</li>
<li>
<p><a href="./cloud-native/cpu-binding.html">k8s supports container core binding</a> ------ release time :2020-05-22 16:24</p>
</li>
<li>
<p><a href="./cloud-native/capability.html">k8s supports Capability mechanism</a> ------ release time :2020-05-22 15:18</p>
</li>
<li>
<p><a href="./cloud-native/oom.html">k8s OOMkilled container exceeding memory limit</a> ------ release time :2020-05-22 13:52</p>
</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Openstack</a></p>
<ul>
<li>
<p><a href="./openstack/deploy-openstack-stein.html">Problems encountered in Openstack Stein deployment</a> ------ release time :2019-09-20 18:49</p>
</li>
<li>
<p><a href="./openstack/sriov2ovs.html">sriov computing node to ovs computing node script</a> ------ release time :2019-04-19 16:50</p>
</li>
<li>
<p><a href="./openstack/placement.html">Openstack Placement</a> ------ release time :2019-04-18 09:14</p>
</li>
<li>
<p><a href="./openstack/power-arch.html">POWER architecture server as computing node</a> ------ release time :2018-10-21 16:21</p>
</li>
<li>
<p><a href="./openstack/sriov.html">OpenStack practices SR-IOV computing nodes</a> ------ release time :2018-10-14 05:15</p>
</li>
<li>
<p><a href="./openstack/openstack-op-4.html">Openstack operation and maintenance records (4)</a> ------ release time :2020-01-01 19:13</p>
</li>
<li>
<p><a href="./openstack/openstack-op-3.html">Openstack operation and maintenance records (3)</a> ------ release time :2018-09-29 20:28</p>
</li>
<li>
<p><a href="./openstack/openstack-op-2.html">Openstack operation and maintenance records (2)</a> ------ release time :2018-08-28 14:48</p>
</li>
<li>
<p><a href="./openstack/openstack-op-1.html">Openstack operation and maintenance records (1)</a> ------ release time :2018-08-26 18:38</p>
</li>
<li>
<p><a href="./openstack/novnc-problem.html">OpenStack Pike dashboard noVNC cannot be accessed problem</a> ------ release time :2018-04-14 05:18</p>
</li>
<li>
<p><a href="./openstack/openstack-local-yum.html">Openstack Pike local yum source construction</a> ------ release time :2017-12-28 22:20</p>
</li>
<li>
<p><a href="./openstack/cpu-binding.html">CPU binding</a> ------ release time :2017-07-20 18:30</p>
</li>
<li>
<p><a href="./openstack/resize-fail.html">Investigation of the cause of resize failure</a> ------ release time :2017-07-04 10:20</p>
</li>
<li>
<p><a href="./openstack/mount-cloud-disk.html">Mount cloud disk</a> ------ release time :2017-06-30 05:16</p>
</li>
<li>
<p><a href="./openstack/ocata-nova-evacuate-bug.html">Ocata nova evacuate bug</a> ------ release time :2017-06-28 18:24</p>
</li>
<li>
<p><a href="./openstack/compute-node-ha.html">compute node ha mainstream open source implementation</a> ------ release time :2017-06-22 09:16</p>
</li>
<li>
<p><a href="./openstack/collectd-influxdb.html">Deployment and usage of Collectd and InfluxDB</a> ------ release time :2017-06-15 06:47</p>
</li>
<li>
<p><a href="./openstack/live-migration-local.html">Live Migration on Local Storage</a> ------ release time :2017-06-14 15:25</p>
</li>
<li>
<p><a href="./openstack/fast-evacuation.html">Add fast evacuation function to VM HA procedure</a> ------ release time :2017-06-10 03:27</p>
</li>
<li>
<p><a href="./openstack/vm-activation-failure.html">Windows virtual machine activation failure problem</a> ------ release time :2017-05-04 22:21</p>
</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Point of view</a></p>
<ul>
<li><a href="./point-of-view/huawei-and-operators.html">Huawei and telecom operators</a> ------ release time :2017-03-10 21:25</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Open source small project</a></p>
<ul>
<li><a href="./open-source-small-project/firewall-tool.html">Open source a node.js firewall tool</a> ------ release time :2022-05-07 09:41</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Tools</a></p>
<ul>
<li>
<p><a href="./tools/github-action.html">Github Action Supplementary Introduction</a> ------ release time :2022-04-12 13:04</p>
</li>
<li>
<p><a href="./tools/blog-cicd.html">Github Action Upgrade backend cloud website CICD process</a> ------ release time :2022-04-03 00:17</p>
</li>
<li>
<p><a href="./tools/github-action-demo.html">Try the Github Action CI/CD process (create a React project and package it for deployment)</a> ------ release time :2022-02-25 11:54</p>
</li>
<li>
<p><a href="./tools/notion-widget.html">Notion's page insert widget Widget</a> ------ release time :2022-01-06 11:06</p>
</li>
<li>
<p><a href="./tools/git-dir.html">Internal structure of .git directory</a> ------ release time :2023-01-18 18:37</p>
</li>
<li>
<p><a href="./tools/xshell-skill.html">Some Skills of using xshell tools</a> ------ release time :2016-07-05 22:57</p>
</li>
<li>
<p><a href="./tools/gerrit-install.html">gerrit install</a> ------ release time :2017-06-28 15:51</p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2022-05-07 09:41</p>
<p>I took a small open source project on the Internet and changed it. The original project was written by vue on the front end and nodejs on the back end. The implemented function is a small tool for the firewall, which can display the background firewall policy through the front-end web interface, and can also insert a firewall policy through the interface. The front and back end use websocket to communicate.</p>
<p>The original project is an iptables graphical manager with simple addition, deletion, modification and query. Has the following functions:</p>
<ul>
<li>Specify table specify chain add rule</li>
<li>Specified table Specified chain Specified position Insertion rule</li>
<li>Clear the specified table specified chain rule</li>
<li>View the referenced custom chain rule separately, which is more clear at a glance (CTRL key + left mouse button)</li>
</ul>
<p>I made a little extension and changed a few lines of code. The extended functions implemented include:</p>
<ul>
<li>The original project can only operate on the firewall of the node deployed by nodejs, and now it can be extended to operate on the firewall of other nodes</li>
<li>Added getting SSH security logs of other nodes</li>
<li>Added the ability to obtain system logs of other nodes SystemLog</li>
</ul>
<p>Therefore, the front-end and back-end communication formats have changed a little, adding a node field and adding two types, SystemLog and SSHLog</p>
<pre><code>{node: “10.253.199.82”, type: “SystemLog”}
{node: “10.253.199.82”, type: “SSHLog”}
</code></pre>
<blockquote>
<p>Source code: https://github.com/backendcloud/iptables-UI</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="summary-4"><a class="header" href="#summary-4">Summary</a></h1>
<ul>
<li>
<p><a href="./SUMMARY.html">Catalog: Cloud Native</a></p>
<ul>
<li>
<p><a href="./cloud-native/deploy-kubevirt.html">Deploy Kubernetes + KubeVirt and the basic use of KubeVirt</a> ------ release time :2022-05-06 14:17</p>
</li>
<li>
<p><a href="./cloud-native/docker-java-demo.html">docker hello-world project</a> ------ release time :2022-05-02 10:13</p>
</li>
<li>
<p><a href="./cloud-native/deploy-cinder-csi-plugin.html">Several problems encountered in deploying cinder-csi-plugin</a> ------ release time :2022-04-24 11:00</p>
</li>
<li>
<p><a href="./cloud-native/go1.18-workspace.html">New in Go 1.18 - Workspaces</a> ------ release time :2022-04-13 20:17</p>
</li>
<li>
<p><a href="./cloud-native/spring-boot-async-demo.html">spring-boot asynchronous example</a> ------ release time :2022-03-25 11:24</p>
</li>
<li>
<p><a href="./cloud-native/plg.html">PLG implements Kubernetes Pod log collection and display</a> ------ release time :2021-12-01 17:48</p>
</li>
<li>
<p><a href="./cloud-native/fluentd.html">Fluentd implements Kubernetes Pod log collection</a> ------ release time :2021-11-30 16:36</p>
</li>
<li>
<p><a href="./cloud-native/java-dev-memo-1.html">Little records in Java project development (1)</a> ------ release time :2021-11-23 15:04</p>
</li>
<li>
<p><a href="./cloud-native/jdk-bug.html">JDK bug - Encrypt Private Key failed unrecognized algorithm name PBEWithSHA1AndDESede</a> ------ release time :2021-11-23 14:22</p>
</li>
<li>
<p><a href="./cloud-native/tenant-cmp.html">Implementation of the function of creating resource pool tenants on the cloud management platform</a> ------ release time :2021-09-13 14:22</p>
</li>
<li>
<p><a href="./cloud-native/test-service-performance-2.html">iperf3 tests the four-layer performance of Kubernetes Service (Part 2)</a> ------ release time :2021-08-10 07:06</p>
</li>
<li>
<p><a href="./cloud-native/test-service-performance-1.html">iperf3 tests the four-layer performance of Kubernetes Service (Part 1)</a> ------ release time :2021-08-09 17:06</p>
</li>
<li>
<p><a href="./cloud-native/k8s-cmd.html">k8s common command</a> ------ release time :2020-05-27 17:32</p>
</li>
<li>
<p><a href="./cloud-native/cpu-binding.html">k8s supports container core binding</a> ------ release time :2020-05-22 16:24</p>
</li>
<li>
<p><a href="./cloud-native/capability.html">k8s supports Capability mechanism</a> ------ release time :2020-05-22 15:18</p>
</li>
<li>
<p><a href="./cloud-native/oom.html">k8s OOMkilled container exceeding memory limit</a> ------ release time :2020-05-22 13:52</p>
</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Openstack</a></p>
<ul>
<li>
<p><a href="./openstack/deploy-openstack-stein.html">Problems encountered in Openstack Stein deployment</a> ------ release time :2019-09-20 18:49</p>
</li>
<li>
<p><a href="./openstack/sriov2ovs.html">sriov computing node to ovs computing node script</a> ------ release time :2019-04-19 16:50</p>
</li>
<li>
<p><a href="./openstack/placement.html">Openstack Placement</a> ------ release time :2019-04-18 09:14</p>
</li>
<li>
<p><a href="./openstack/power-arch.html">POWER architecture server as computing node</a> ------ release time :2018-10-21 16:21</p>
</li>
<li>
<p><a href="./openstack/sriov.html">OpenStack practices SR-IOV computing nodes</a> ------ release time :2018-10-14 05:15</p>
</li>
<li>
<p><a href="./openstack/openstack-op-4.html">Openstack operation and maintenance records (4)</a> ------ release time :2020-01-01 19:13</p>
</li>
<li>
<p><a href="./openstack/openstack-op-3.html">Openstack operation and maintenance records (3)</a> ------ release time :2018-09-29 20:28</p>
</li>
<li>
<p><a href="./openstack/openstack-op-2.html">Openstack operation and maintenance records (2)</a> ------ release time :2018-08-28 14:48</p>
</li>
<li>
<p><a href="./openstack/openstack-op-1.html">Openstack operation and maintenance records (1)</a> ------ release time :2018-08-26 18:38</p>
</li>
<li>
<p><a href="./openstack/novnc-problem.html">OpenStack Pike dashboard noVNC cannot be accessed problem</a> ------ release time :2018-04-14 05:18</p>
</li>
<li>
<p><a href="./openstack/openstack-local-yum.html">Openstack Pike local yum source construction</a> ------ release time :2017-12-28 22:20</p>
</li>
<li>
<p><a href="./openstack/cpu-binding.html">CPU binding</a> ------ release time :2017-07-20 18:30</p>
</li>
<li>
<p><a href="./openstack/resize-fail.html">Investigation of the cause of resize failure</a> ------ release time :2017-07-04 10:20</p>
</li>
<li>
<p><a href="./openstack/mount-cloud-disk.html">Mount cloud disk</a> ------ release time :2017-06-30 05:16</p>
</li>
<li>
<p><a href="./openstack/ocata-nova-evacuate-bug.html">Ocata nova evacuate bug</a> ------ release time :2017-06-28 18:24</p>
</li>
<li>
<p><a href="./openstack/compute-node-ha.html">compute node ha mainstream open source implementation</a> ------ release time :2017-06-22 09:16</p>
</li>
<li>
<p><a href="./openstack/collectd-influxdb.html">Deployment and usage of Collectd and InfluxDB</a> ------ release time :2017-06-15 06:47</p>
</li>
<li>
<p><a href="./openstack/live-migration-local.html">Live Migration on Local Storage</a> ------ release time :2017-06-14 15:25</p>
</li>
<li>
<p><a href="./openstack/fast-evacuation.html">Add fast evacuation function to VM HA procedure</a> ------ release time :2017-06-10 03:27</p>
</li>
<li>
<p><a href="./openstack/vm-activation-failure.html">Windows virtual machine activation failure problem</a> ------ release time :2017-05-04 22:21</p>
</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Point of view</a></p>
<ul>
<li><a href="./point-of-view/huawei-and-operators.html">Huawei and telecom operators</a> ------ release time :2017-03-10 21:25</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Open source small project</a></p>
<ul>
<li><a href="./open-source-small-project/firewall-tool.html">Open source a node.js firewall tool</a> ------ release time :2022-05-07 09:41</li>
</ul>
</li>
<li>
<p><a href="./SUMMARY.html">Catalog: Tools</a></p>
<ul>
<li>
<p><a href="./tools/github-action.html">Github Action Supplementary Introduction</a> ------ release time :2022-04-12 13:04</p>
</li>
<li>
<p><a href="./tools/blog-cicd.html">Github Action Upgrade backend cloud website CICD process</a> ------ release time :2022-04-03 00:17</p>
</li>
<li>
<p><a href="./tools/github-action-demo.html">Try the Github Action CI/CD process (create a React project and package it for deployment)</a> ------ release time :2022-02-25 11:54</p>
</li>
<li>
<p><a href="./tools/notion-widget.html">Notion's page insert widget Widget</a> ------ release time :2022-01-06 11:06</p>
</li>
<li>
<p><a href="./tools/git-dir.html">Internal structure of .git directory</a> ------ release time :2023-01-18 18:37</p>
</li>
<li>
<p><a href="./tools/xshell-skill.html">Some Skills of using xshell tools</a> ------ release time :2016-07-05 22:57</p>
</li>
<li>
<p><a href="./tools/gerrit-install.html">gerrit install</a> ------ release time :2017-06-28 15:51</p>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2022-04-12 13:04</p>
<h1 id="restrictions-on-the-use-of-github-actions"><a class="header" href="#restrictions-on-the-use-of-github-actions">Restrictions on the use of Github Actions</a></h1>
<p>A total usage time limit of 2000 minutes/month, a job in each Workflow can be executed for a maximum of 6 hours, a Workflow can be executed for a maximum of 72 hours, a job in a Workflow can be queued for a maximum of 24 hours in all Actions in a repository, A maximum of 1,000 API requests can be executed in one hour. Number of concurrent jobs: Linux: 20, Mac: 5.</p>
<blockquote>
<p>These restrictions are sufficient for personal development. Of course, it also restricts the use of Github resources as a server. Github has the action of Github as a server. With this action, you can ssh to the Github server to use the server's computing power. Github's server performance is still very strong.</p>
</blockquote>
<h1 id="what-is-workflow"><a class="header" href="#what-is-workflow">What is Workflow</a></h1>
<p>A Workflow is a configurable automated process consisting of one or more jobs. You can customize the name, and the Github Action page will display the customized name, otherwise use the default naming method.</p>
<p>on can define the name of the event that triggers the execution of Workflow. Below are the two most commonly used.</p>
<pre><code>// single event
on: push

// multiple events
on: [push,pull_request]
</code></pre>
<h1 id="what-is-a-workflow-job-and-how-to-share-data-between-different-jobs"><a class="header" href="#what-is-a-workflow-job-and-how-to-share-data-between-different-jobs">What is a Workflow job? And how to share data between different jobs</a></h1>
<p>A Workflow consists of one or more jobs, which means a continuous integration operation that can complete multiple tasks. Github tasks are called steps, and a step can have multiple actions.</p>
<p>Because a job corresponds to a continuous integration. Different jobs cannot share data. The workflow defined by hexo twice in ci.yaml above is a single job. Github executes multiple jobs in parallel by default.</p>
<p>The job must define the id, and the id is the key. This is different from the step. The id and name of the step are defined by the key value pair, and different steps are in the form of an array, and are executed sequentially in the order of writing. The name of the job will be displayed on the Github Action page.</p>
<pre><code>jobs:
    my_first_job:
        name: My first job
    my_second_job:
        name: My second job
</code></pre>
<p>needs can identify whether the job depends on other jobs - if the job fails, all jobs that require the job will be skipped</p>
<pre><code>jobs:
    job1:
    job2:
        needs: job1
    job3:
        needs: [job1, job2]
</code></pre>
<p>How to share data between different jobs can share data between workflow jobs by cooperating with need to share string variables or artifacts</p>
<h1 id="cooperate-with-need-to-share-string-variables"><a class="header" href="#cooperate-with-need-to-share-string-variables">Cooperate with need to share string variables</a></h1>
<pre><code>jobs:
job1:
    runs-on: ubuntu-latest
    # Map a step output to a job output
    outputs:
    output1: ${{ steps.step1.outputs.test }}
    output2: ${{ steps.step2.outputs.test }}
    steps:
    - id: step1
    run: echo &quot;::set-output name=test::hello&quot;
    - id: step2
    run: echo &quot;::set-output name=test::world&quot;
job2:
    runs-on: ubuntu-latest
    needs: job1
    steps:
    - run: echo ${{needs.job1.outputs.output1}} ${{needs.job1.outputs.output2}}
</code></pre>
<h1 id="artifacts-share-data-between-workflow-jobs"><a class="header" href="#artifacts-share-data-between-workflow-jobs">Artifacts share data between workflow jobs</a></h1>
<p>Github actions Artifact can be used to store the products produced by actions, such as static files generated by npm build. After the upload is successful, the subsequent process can download these files for use.</p>
<p>One of the jobs is to upload files to Github actions Artifact, use actions/upload-artifact@v2</p>
<pre><code>- uses: actions/upload-artifact@v2
with:
name: agileconfig-ui  
path: xxx/yyy/
</code></pre>
<p>Another job needs a job that needs to upload files, use uses actions/download-artifact@v2</p>
<pre><code>- uses: actions/download-artifact@v2
with:
    name: agileconfig-ui  
    path: aaa/bbb   
</code></pre>
<blockquote>
<p>Github actions Artifact can not only share files with different jobs, but also manually download files to Github Action, such as compiling and packaging files.</p>
</blockquote>
<h1 id="define-environment-variables"><a class="header" href="#define-environment-variables">Define environment variables</a></h1>
<p>Job defines environment variables</p>
<pre><code>jobs:
    job1:
        env:
            FIRST_NAME: Mona
</code></pre>
<p>step defines environment variables</p>
<pre><code>steps:
- name: Print a greeting
env:
    MY_VAR: Hi there! My name is
    FIRST_NAME: Mona
    MIDDLE_NAME: The
    LAST_NAME: Octocat
run: |
    echo $MY_VAR $FIRST_NAME $MIDDLE_NAME $LAST_NAME
</code></pre>
<h1 id="what-is-a-matrix"><a class="header" href="#what-is-a-matrix">What is a matrix?</a></h1>
<p>Just sometimes, our code may have multiple compilation environments. We need to compile the dmg compressed package on macos and compile the exe executable file on windows. At this time, we can use the matrix~</p>
<p>For example, in the following code, we use the matrix to specify: 2 operating systems, 3 node versions.</p>
<p>At this time, the following code will be executed 6 times - 2 x 3 = 6! ! !</p>
<pre><code>runs-on: ${{ matrix.os }}
strategy:
matrix:
    os: [ubuntu-16.04, ubuntu-18.04]
    node: [6, 8, 10]
steps:
- uses: actions/setup-node@v1
    with:
    node-version: ${{ matrix.node }}
</code></pre>
<h1 id="skip-github-actions-or-optionally-execute-cicd-process"><a class="header" href="#skip-github-actions-or-optionally-execute-cicd-process">Skip Github Actions or optionally execute CICD process</a></h1>
<p>As long as the following keywords are included in the commit information, CI will be skipped and CI Build will not be triggered</p>
<pre><code>[skip ci]
[ci skip]
[no ci]
[skip actions]
[actions skip]
</code></pre>
<p>Requirement: I don't want to trigger Github Actions build every time I commit, only when the git commit message does not contain the specified content.</p>
<p>Github Actions supports jobs..if (opens new window) syntax During the operation of Github Actions, we can get some current environment information, such as git submission content information, and control the execution of actions through these contents</p>
<p>For example, a build is only triggered when the git message does not contain wip</p>
<pre><code>jobs:
    format:
        runs-on: ubuntu-latest
        if: &quot;! contains(github.event.head_commit.message, 'wip')&quot;
</code></pre>
<p>In the same way, the following workflow indicates that the build will only be triggered if the git message contains [build], otherwise it will be skipped</p>
<pre><code>jobs:
    format:
        runs-on: ubuntu-latest
        if: &quot;contains(github.event.head_commit.message, '[build]')&quot;
</code></pre>
<h1 id="how-to-manually-trigger-a-build"><a class="header" href="#how-to-manually-trigger-a-build">How to manually trigger a build</a></h1>
<p>By default, only push and pull request actions will trigger the build</p>
<pre><code>on:
    push:
        branches: [ main ]
    pull_request:
        branches: [ main ]
</code></pre>
<p>The easiest way is to add workflow_dispatch action</p>
<pre><code>on:
    workflow_dispatch:
    push:
        branches: [ main ]
    pull_request:
        branches: [ main ]
</code></pre>
<p>In this way, you can see the button to execute the build on the actions page, and you can execute a manual build after selecting a branch.</p>
<p><img src="tools/2023-01-19-14-07-32.png" alt="" /></p>
<pre><code>on:
workflow_dispatch:
    inputs:
    name:
        description: 'Person to greet'
        required: true
        default: 'Mona the Octocat'
    home:
        description: 'location'
        required: false

jobs:
say_hello:
    runs-on: ubuntu-latest
    steps:
    - run: |
        echo &quot;Hello ${{ github.event.inputs.name }}!&quot;
        echo &quot;- in ${{ github.event.inputs.home }}!&quot;
</code></pre>
<p><img src="tools/2023-01-19-14-07-47.png" alt="" /></p>
<h1 id="code-inspection-takes-bash-script-inspection-as-an-example"><a class="header" href="#code-inspection-takes-bash-script-inspection-as-an-example">Code inspection takes bash script inspection as an example</a></h1>
<pre><code class="language-yaml">name: shellcheck
'on':
  push:
  pull_request:
  schedule:
    - cron: '0 1 * * *'

jobs:

  shellcheck:
    name: shellcheck
    runs-on: ubuntu-latest

    steps:
      - name: Check out the codebase.
        uses: actions/checkout@v2

      - name: Install shellcheck from GitHub releases
        run: |
          sudo curl -L https://github.com/koalaman/shellcheck/releases/download/v${SHELLCHECK_VERSION}/shellcheck-v${SHELLCHECK_VERSION}.$(uname -s).$(uname -m).tar.xz -o shellcheck.tar.xz
          sudo tar -xvf shellcheck.tar.xz
          sudo mv shellcheck-v${SHELLCHECK_VERSION}/shellcheck /usr/local/bin/shellcheck
          sudo chmod 755 /usr/local/bin/shellcheck
          shellcheck --version
        env:
          SHELLCHECK_VERSION: 0.7.2

      - name: Shellcheck!
        run: |
          shellcheck &lt;code path&gt;
          shellcheck &lt;code path&gt;
</code></pre>
<h1 id="github-actions-build-android"><a class="header" href="#github-actions-build-android">GitHub Actions Build Android</a></h1>
<pre><code class="language-yaml">name: android_build

on:
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout the code
        # 拉取 android_builder 的源代码
        uses: actions/checkout@v2
      - name: Set up JDK
        # 设置 Java 运行环境
        uses: actions/setup-java@v1
        with:
          java-version: 1.8
          # 用 1.8 版本覆盖环境中自带的 Java 11 版本
      - id: get-project
        # 读取项目地址
        name: Get project name
        run: echo &quot;::set-output name=PROJECT::$(cat project-to-build)&quot;
      - name: Clone project
        # 拉取项目源码到虚拟环境
        run: git clone --depth=1 ${{ steps.get-project.outputs.PROJECT }} project
      - name: Build the app
        # 构建调试版 APK
        working-directory: ./project
        run: |
          if [ ! -f &quot;gradlew&quot; ]; then gradle wrapper; fi
          chmod +x gradlew
          ./gradlew assembleDebug --stacktrace
      - name: Upload APK
        # 打包上传生成的 APK 到的网页端
        uses: actions/upload-artifact@v2
        with:
          name: my-build-apk
          path: ./**/*.apk
</code></pre>
<h1 id="docker-build-image-and-push-to-docker-hub"><a class="header" href="#docker-build-image-and-push-to-docker-hub">Docker build image and push to Docker Hub</a></h1>
<p>Go to https://hub.docker.com/settings/security to generate an access token, and remember it well. Then open Github to Settings &gt; Secrets &gt; New secret to add two records:</p>
<ul>
<li>Key name: DOCKER_HUB_USERNAME, value is the username of Docker hub</li>
<li>Key name: DOCKER_HUB_ACCESS_TOKEN, the value is the access token just copied, the value is similar to c292155d-1bd7-xxxx-xxxx-4da75bedb178</li>
</ul>
<pre><code class="language-yaml">name: CI to Docker Hub 

on:
  push:
        branches: [ master ]
   # tags:
   #   - &quot;v*.*.*&quot;

jobs:

  build:
    runs-on: ubuntu-latest
    steps:
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      -
        name: Login to DockerHub
        uses: docker/login-action@v1 
        with:
          username: ${{ secrets.DOCKER_HUB_USERNAME }}
          password: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}
      -
        name: Build and push
        id: docker_build
        uses: docker/build-push-action@v2
        with:
          push: true
          tags: finleyma/simplewhale:latest
          build-args: |
            arg1=value1
            arg2=value2
      -
        name: Image digest
        run: echo ${{ steps.docker_build.outputs.digest }}
</code></pre>
<h1 id="github-action-deploy-openstack-environment"><a class="header" href="#github-action-deploy-openstack-environment">Github Action Deploy Openstack environment</a></h1>
<p>The following two examples are examples in the latest Openstack versions TX 部署nova等基本服务and部署Ironic等服务</p>
<pre><code class="language-yaml">jobs:
  functional-compute:
    strategy:
      fail-fast: false
      matrix:
        name: [&quot;master&quot;]
        openstack_version: [&quot;master&quot;]
        ubuntu_version: [&quot;20.04&quot;]
        include:
          - name: &quot;xena&quot;
            openstack_version: &quot;stable/xena&quot;
            ubuntu_version: &quot;20.04&quot;
          - name: &quot;wallaby&quot;
            openstack_version: &quot;stable/wallaby&quot;
            ubuntu_version: &quot;20.04&quot;
          - name: &quot;victoria&quot;
            openstack_version: &quot;stable/victoria&quot;
            ubuntu_version: &quot;20.04&quot;
          - name: &quot;ussuri&quot;
            openstack_version: &quot;stable/ussuri&quot;
            ubuntu_version: &quot;18.04&quot;
          - name: &quot;train&quot;
            openstack_version: &quot;stable/train&quot;
            ubuntu_version: &quot;18.04&quot;
    runs-on: ubuntu-${{ matrix.ubuntu_version }}
    name: Deploy OpenStack ${{ matrix.name }} with Nova and run compute acceptance tests
    steps:
      - name: Deploy devstack
        uses: EmilienM/devstack-action@v0.6
        with:
          branch: ${{ matrix.openstack_version }}
          conf_overrides: |
            CINDER_ISCSI_HELPER=tgtadm
</code></pre>
<pre><code class="language-yaml">jobs:
  functional-baremetal:
    strategy:
      fail-fast: false
      matrix:
        name: [&quot;master&quot;]
        openstack_version: [&quot;master&quot;]
        ubuntu_version: [&quot;20.04&quot;]
        include:
          - name: &quot;xena&quot;
            openstack_version: &quot;stable/xena&quot;
            ubuntu_version: &quot;20.04&quot;
          - name: &quot;wallaby&quot;
            openstack_version: &quot;stable/wallaby&quot;
            ubuntu_version: &quot;20.04&quot;
          - name: &quot;victoria&quot;
            openstack_version: &quot;stable/victoria&quot;
            ubuntu_version: &quot;20.04&quot;
          - name: &quot;ussuri&quot;
            openstack_version: &quot;stable/ussuri&quot;
            ubuntu_version: &quot;18.04&quot;
          - name: &quot;train&quot;
            openstack_version: &quot;stable/train&quot;
            ubuntu_version: &quot;18.04&quot;
    runs-on: ubuntu-${{ matrix.ubuntu_version }}
    name: Deploy OpenStack ${{ matrix.name }} with Ironic and run baremetal acceptance tests
    steps:
      - name: Deploy devstack
        uses: EmilienM/devstack-action@v0.6
        with:
          branch: ${{ matrix.openstack_version }}
          conf_overrides: |
            enable_plugin ironic https://opendev.org/openstack/ironic ${{ matrix.openstack_version }}
            LIBS_FROM_GIT=pyghmi,virtualbmc
            FORCE_CONFIG_DRIVE=True
            Q_AGENT=openvswitch
            Q_ML2_TENANT_NETWORK_TYPE=vxlan
            Q_ML2_PLUGIN_MECHANISM_DRIVERS=openvswitch
            DEFAULT_INSTANCE_TYPE=baremetal
            OVERRIDE_PUBLIC_BRIDGE_MTU=1400
            VIRT_DRIVER=ironic
            BUILD_TIMEOUT=1800
            SERVICE_TIMEOUT=90
            GLANCE_LIMIT_IMAGE_SIZE_TOTAL=5000
            Q_USE_SECGROUP=False
            API_WORKERS=1
            IRONIC_BAREMETAL_BASIC_OPS=True
            IRONIC_BUILD_DEPLOY_RAMDISK=False
            IRONIC_AUTOMATED_CLEAN_ENABLED=False
            IRONIC_CALLBACK_TIMEOUT=600
            IRONIC_DEPLOY_DRIVER=ipmi
            IRONIC_INSPECTOR_BUILD_RAMDISK=False
            IRONIC_RAMDISK_TYPE=tinyipa
            IRONIC_TEMPEST_BUILD_TIMEOUT=720
            IRONIC_TEMPEST_WHOLE_DISK_IMAGE=False
            IRONIC_VM_COUNT=1
            IRONIC_VM_EPHEMERAL_DISK=1
            IRONIC_VM_LOG_DIR=/opt/stack/new/ironic-bm-logs
            IRONIC_VM_SPECS_RAM=1024
            IRONIC_DEFAULT_DEPLOY_INTERFACE=direct
            IRONIC_ENABLED_DEPLOY_INTERFACES=direct
            SWIFT_ENABLE_TEMPURLS=True
            SWIFT_TEMPURL_KEY=secretkey
          enabled_services: 'ir-api,ir-cond,s-account,s-container,s-object,s-proxy,q-svc,q-agt,q-dhcp,q-l3,q-meta,-cinder,-c-sch,-c-api,-c-vol,-c-bak,-ovn,-ovn-controller,-ovn-northd,-q-ovn-metadata-agent'
</code></pre>
<p><img src="tools/2023-01-19-14-10-44.png" alt="" /></p>
<p><img src="tools/2023-01-19-14-10-53.png" alt="" /></p>
<p>In this way, the parallel preparation of 5 or 6 latest versions of the openstack environment is automatically completed in just one meal.</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2022-04-03 00:17</p>
<h1 id="cicd-program-upgrade"><a class="header" href="#cicd-program-upgrade">CICD program upgrade</a></h1>
<p>In the past, the blog update method was: execute hexo g -d, compile and deploy to Alibaba Cloud, and git version management push to Github.</p>
<p>Since I tried Github Action last time, I feel that I should keep up with the technology trend and use Github Action to complete CICD automation. In fact, the efficiency did not improve afterwards, but dropped, because the hexo g -dcompilation and deployment took a total of one or two seconds. Switching to the new process only saves hexo g -dthis step, leaving only push to Github, but it takes about 30 to 60 seconds for Github Action to automatically execute this process.</p>
<p>It used to take 2 seconds to see the effect of the blog update, but now it takes nearly 1 minute to see it. In terms of convenience and efficiency, there is no optimization, but the process is greatly upgraded. The CICD idea has obvious advantages in large-scale projects. And the node environment can be used locally, and hexo does not need to be installed. It is especially convenient to switch between multiple computers and multiple environments, and it is convenient for multiple people to edit collaboratively.</p>
<p>Three CICD upgrade options:</p>
<ol>
<li>CI process of the Hexo code warehouse: monitor the push to the static directory public, then use rsync to synchronize the nginx html directory of Alibaba Cloud. This approach also requires local execution hexo g.</li>
<li>Hexo code warehouse CI process: monitor the source file folder has a push, then deploy the node environment, install hexo, install the local package, execute hexo g -d, and deploy to the nginx html directory of Alibaba Cloud through hexo rsync.</li>
<li>The ultimate solution, dual code warehouse, hexo code warehouse and static web page code warehouse. CI process of the Hexo code warehouse: Monitor the source file folder for push, then deploy the node environment, install hexo, install the local package, execute hexo g -d, deploy to the Github static web warehouse through hexo git and trigger the CI process of the static web warehouse: deploy through rsync Go to the nginx html directory of Alibaba Cloud.</li>
</ol>
<blockquote>
<p>The advantage of dual warehouses is that the hexo framework is separated from the static web page, and the unblocking of Github may come one day. You can replace a domestic cloud service provider with Github static web page service at any time. The second code warehouse also tosses the CICD process to deploy to Alibaba Cloud for this reason: for domestic acceleration</p>
</blockquote>
<p>Since scheme 3 already includes all the implementations of 1 and 2, scheme 3 will be introduced.</p>
<h1 id="hexo-code-warehouse-cicd-configuration"><a class="header" href="#hexo-code-warehouse-cicd-configuration">Hexo code warehouse CICD configuration</a></h1>
<pre><code class="language-yaml">name: HEXO CI

#Invalid workflow file: .github/workflows/ci.yml#L1
#  you may only define one of `paths` and `paths-ignore` for a single event
on:
  push:
    branches:
      - master 
    paths-ignore: 
      - README.md
      - LICENSE

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: [16.x]

    steps:
      - uses: actions/checkout@v1

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v1
        with:
          node-version: ${{ matrix.node-version }}

      - name: Configuration environment
        env:
          HEXO_DEPLOY_PRI: ${{ secrets.DEPLOY_KEY }}
        run: |
          mkdir -p ~/.ssh/
          echo &quot;$HEXO_DEPLOY_PRI&quot; &gt; ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts
          git config --global user.name &quot;hanwei&quot;
          git config --global user.email &quot;backendcloud@gmail.com&quot;
      - name: Install dependencies
        run: |
          npm i -g hexo-cli
          npm i
          npm install hexo-deployer-git --save
      - name: Deploy hexo
        run: |
          hexo clean &amp;&amp; hexo generate
          mkdir -p ./public/.github/workflows
          cp ./backendcloud.github.io.ci.yml ./public/.github/workflows/ci.yml
          hexo deploy
</code></pre>
<h1 id="static-web-page-code-warehouse-cicd-configuration"><a class="header" href="#static-web-page-code-warehouse-cicd-configuration">Static web page code warehouse CICD configuration</a></h1>
<pre><code class="language-yaml">name: Deploy site files

on:
  push:
    branches:
      - master 
    paths-ignore: 
      - README.md
      - LICENSE

jobs:
  deploy:
    runs-on: ubuntu-latest 

    steps: 
      - uses: actions/checkout@v2

      - name: Deploy to Server 
        uses: AEnterprise/rsync-deploy@v1.0 
        env:
          DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }} 
          ARGS: -avz --delete --exclude='*.pyc'
          SERVER_PORT: &quot;22&quot; # SSH端口
          FOLDER: ./ 
          SERVER_IP: ${{ secrets.SSH_HOST }}
          USERNAME: root 
          SERVER_DESTINATION: /usr/share/nginx/html/backendcloud/www/
      - name: Restart server # 第三步，重启服务
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.SSH_HOST }} 
          username: root
          key: ${{ secrets.DEPLOY_KEY }}
          script: |
            rm -rf /usr/share/nginx/html/backendcloud/www/.git*
</code></pre>
<p>Hexo git deployconfiguration</p>
<pre><code>deploy:
type: git
repo: git@github.com:backendcloud/backendcloud.github.io.git
# example, https://github.com/hexojs/hexojs.github.io
branch: master
ignore_hidden:
    public: false
</code></pre>
<h1 id="run-through-the-pits-encountered-along-the-way"><a class="header" href="#run-through-the-pits-encountered-along-the-way">Run through the pits encountered along the way</a></h1>
<h2 id="paths-ignore-and-paths-cannot-exist-at-the-same-time"><a class="header" href="#paths-ignore-and-paths-cannot-exist-at-the-same-time">paths-ignore and paths cannot exist at the same time.</a></h2>
<pre><code class="language-yaml">on:
  push:
    branches:
      - master 
    paths-ignore: 
      - README.md
      - LICENSE
    paths: 
      - './source/_posts/**'
</code></pre>
<p>/images/blog-cicd</p>
<p><img src="tools/2023-01-19-13-42-56.png" alt="" /></p>
<pre><code>Invalid workflow file: .github/workflows/ci.yml#L1
you may only define one of `paths` and `paths-ignore` for a single event
</code></pre>
<h2 id="the-wrong-open-source-component-was-selected-in-the-rsync-use-market"><a class="header" href="#the-wrong-open-source-component-was-selected-in-the-rsync-use-market">The wrong open source component was selected in the rsync use market</a></h2>
<p>burnett01/rsync-deployments@5.2 I don't know where is the problem</p>
<p><img src="tools/2023-01-19-13-43-22.png" alt="" /></p>
<p>Use changed a: AEnterprise/rsync-deploy@v1.0 immediately normal.</p>
<h2 id="hexo-git-not-found"><a class="header" href="#hexo-git-not-found">hexo git not found</a></h2>
<p><img src="tools/2023-01-19-13-43-45.png" alt="" /></p>
<p>need to be in</p>
<ul>
<li>name: Install dependencies
run: |
npm i -g hexo-cli
npm i
- name: Deploy hexo
run: |
hexo clean &amp;&amp; hexo generate</li>
</ul>
<p>add onenpm install hexo-deployer-git --save</p>
<ul>
<li>name: Install dependencies
run: |
npm i -g hexo-cli
npm i
npm install hexo-deployer-git --save
- name: Deploy hexo
run: |
hexo clean &amp;&amp; hexo generate</li>
</ul>
<h2 id="there-is-no-permission-to-push-to-the-static-web-repository"><a class="header" href="#there-is-no-permission-to-push-to-the-static-web-repository">There is no permission to push to the static web repository</a></h2>
<p><img src="tools/2023-01-19-13-44-24.png" alt="" /></p>
<p>You need to configure the public key in the Github warehouse Settingslabel of the static web page. The public key needs to be consistent with the private key in the container in the github action.</p>
<h2 id="pushing-to-the-static-web-repository-is-normal-github-can-see-the-latest-pushed-code-repository-but-the-ci-configuration-file-of-the-repository-is-always-missing"><a class="header" href="#pushing-to-the-static-web-repository-is-normal-github-can-see-the-latest-pushed-code-repository-but-the-ci-configuration-file-of-the-repository-is-always-missing">Pushing to the static web repository is normal, Github can see the latest pushed code repository, but the CI configuration file of the repository is always missing</a></h2>
<p><img src="tools/2023-01-19-13-44-38.png" alt="" /></p>
<ul>
<li>name: Deploy hexo
run: |
hexo clean &amp;&amp; hexo generate
mkdir -p ./public/.github/workflows
cp ./backendcloud.github.io.ci.yml ./public/.github/workflows/ci.yml
hexo deploy</li>
</ul>
<p>The CI configuration file of the Hexo code warehouse has a copy-write command, but the static web page code warehouse cannot always be found. After debugging several times, it was confirmed that there is no problem with the code. After debugging, it is found that non-hidden files can be seen, but hidden files cannot be seen. I wonder if hexo git will not push to hidden files by default. Go to the open source hexo git to check, sure enough.</p>
<blockquote>
<p>https://github.com/hexojs/hexo-deployer-git</p>
</blockquote>
<p>Hexo git deployconfig from below</p>
<pre><code>deploy:
type: git
repo: git@github.com:backendcloud/backendcloud.github.io.git
# example, https://github.com/hexojs/hexojs.github.io
branch: master
</code></pre>
<p>changed to</p>
<pre><code>deploy:
type: git
repo: git@github.com:backendcloud/backendcloud.github.io.git
# example, https://github.com/hexojs/hexojs.github.io
branch: master
ignore_hidden:
    public: false
</code></pre>
<p>everything is normal.</p>
<p>Another place stepped on a small hole:</p>
<p>these two sentences</p>
<pre><code>mkdir -p ./public/.github/workflows
cp ./backendcloud.github.io.ci.yml ./public/.github/workflows/ci.yml
</code></pre>
<p>It cannot be placed hexo clean &amp;&amp; hexo generatebefore, only after. Because hexo clean will empty the publicfolder.</p>
<p>So far, the three Github Action CICD process of the dual code warehouse has completely run through.</p>
<p><img src="tools/2023-01-19-13-45-49.png" alt="" /></p>
<p><img src="tools/2023-01-19-13-45-55.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2022-02-25 11:54</p>
<p>Create a React projectgithub-action-demo</p>
<p>Will (replace backendcloud with your github account name)</p>
<pre><code>&quot;homepage&quot;: &quot;https://backendcloud.github.io/github-actions-demo&quot;,
</code></pre>
<p>Add to package.json</p>
<p>push to the github repository.</p>
<p>Add the github aciton CI configuration file: the process is generally that once the master branch is monitored to have a push, then checkout the project in the virtual environment, build the React project, and deploy the static file to the gh-pagesbranch of the code warehouse.</p>
<pre><code>hanwei@hanweideMacBook-Air github-actions-demo]$ cat .github/workflows/ci.yml 
name: GitHub Actions Build and Deploy Demo
on:
push:
    branches:
    - master
jobs:
build-and-deploy:
    concurrency: ci-${{ github.ref }} # Recommended if you intend to make multiple deployments in quick succession.
    runs-on: ubuntu-latest
    steps:
    - name: Checkout 🛎️
        uses: actions/checkout@v2

    - name: Install and Build 🔧 # This example project is built using npm and outputs the result to the 'build' folder. Replace with the commands required to build your project, or remove this step entirely if your site is pre-built.
        run: |
        npm ci
        npm run build

    - name: Deploy 🚀
        uses: JamesIves/github-pages-deploy-action@v4.2.5
        with:
        branch: gh-pages # The branch the action should deploy to.
        folder: build # The folder the action should deploy.
hanwei@hanweideMacBook-Air github-actions-demo]$ 
</code></pre>
<p>Push the code to the github code warehouse again, trigger the github action process, and check the github action log (the github website is saved for 30 days):</p>
<pre><code>deploy
succeeded 17 minutes ago in 4s
Search logs
1s
Current runner version: '2.287.1'
Operating System
Virtual Environment
Virtual Environment Provisioner
GITHUB_TOKEN Permissions
Secret source: Actions
Prepare workflow directory
Prepare all required actions
Getting action download info
Download action repository 'actions/deploy-pages@v1' (SHA:479e82243a95585ebee8ab037c4bfd8e6356a47b)
0s
Run actions/deploy-pages@v1
Sending telemetry for run id 1896726019
3s
Run actions/deploy-pages@v1
Actor: github-pages[bot]
Action ID: 1896726019
Artifact URL: https://pipelines.actions.githubusercontent.com/7IW4xoAefje3iJJjmBQg228AYglZEwNoROQmK16mg2iViZ9muv/_apis/pipelines/workflows/1896726019/artifacts?api-version=6.0-preview
{&quot;count&quot;:1,&quot;value&quot;:[{&quot;containerId&quot;:3420907,&quot;size&quot;:614400,&quot;signedContent&quot;:null,&quot;fileContainerResourceUrl&quot;:&quot;https://pipelines.actions.githubusercontent.com/7IW4xoAefje3iJJjmBQg228AYglZEwNoROQmK16mg2iViZ9muv/_apis/resources/Containers/3420907&quot;,&quot;type&quot;:&quot;actions_storage&quot;,&quot;name&quot;:&quot;github-pages&quot;,&quot;url&quot;:&quot;https://pipelines.actions.githubusercontent.com/7IW4xoAefje3iJJjmBQg228AYglZEwNoROQmK16mg2iViZ9muv/_apis/pipelines/1/runs/2/artifacts?artifactName=github-pages&quot;,&quot;expiresOn&quot;:&quot;2022-05-26T03:47:37.6013822Z&quot;,&quot;items&quot;:null}]}
Creating deployment with payload:
{
    &quot;artifact_url&quot;: &quot;https://pipelines.actions.githubusercontent.com/7IW4xoAefje3iJJjmBQg228AYglZEwNoROQmK16mg2iViZ9muv/_apis/pipelines/1/runs/2/artifacts?artifactName=github-pages&amp;%24expand=SignedContent&quot;,
    &quot;pages_build_version&quot;: &quot;f018db912dfb491e86183fbc140fe14e3a256905&quot;,
    &quot;oidc_token&quot;: &quot;***&quot;
}
Created deployment for f018db912dfb491e86183fbc140fe14e3a256905
{&quot;page_url&quot;:&quot;https://backendcloud.github.io/github-actions-demo/&quot;,&quot;status_url&quot;:&quot;https://api.github.com/repos/backendcloud/github-actions-demo/pages/deployment/status/f018db912dfb491e86183fbc140fe14e3a256905&quot;}

Current status: updating_pages
Reported success!

Sending telemetry for run id 1896726019
0s
Evaluate and set environment url
Evaluated environment url: https://backendcloud.github.io/github-actions-demo/
Cleaning up orphan processes
</code></pre>
<p>After the CI/CD process is completed, open it with a browser https://backendcloud.github.io/github-actions-demo/and find that the React project is successfully deployed, as shown below:</p>
<p><img src="tools/2023-01-19-13-15-38.png" alt="" /></p>
<p>GitHub has two recent changes:</p>
<p>1 is used for github.io is no longer the default gh-pagesbranch, which branch to use for static web pages and content needs to be configured in advance. If not configured, an error https://backendcloud.github.io/github-actions-demo/will be reported when opening .404</p>
<p><img src="tools/2023-01-19-13-15-52.png" alt="" /></p>
<p>2 is that the user name and password authentication has been canceled Githubfor developers to call Github API, and only more secure token authentication can be used.</p>
<p><img src="tools/2023-01-19-13-16-06.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2022-01-06 11:06</p>
<h1 id="what-is-widget"><a class="header" href="#what-is-widget">What is Widget</a></h1>
<p>Widget is not a small app, it is a new form of desktop content presentation, mainly used to make up for the main application's inability to display the data that users care about in time.</p>
<p>An excellent Widget needs to have three characteristics: simple and clear (Glanceable), appropriate display (Relevant), and personalized customization (Personalized).</p>
<h1 id="notions-page-insertion-widget"><a class="header" href="#notions-page-insertion-widget">Notion's page insertion widget</a></h1>
<p>The pages of Notion can be easily inserted into widgets like the IOS system and Hongmeng system. As shown below:</p>
<p><img src="tools/2023-01-19-13-04-14.png" alt="" /></p>
<p>There are many controls, you can try them out in the links in the article. Let’s take the 4 small controls in the picture above as examples: the weather for the next 7 days, the current calendar, the remaining battery life, and the current time.</p>
<h2 id="next-7-days-weather-component"><a class="header" href="#next-7-days-weather-component">Next 7 days weather component</a></h2>
<p><a href="https://weatherwidget.io/">WeatherWidget.io</a></p>
<p><img src="tools/2023-01-19-13-04-46.png" alt="" /></p>
<p>Click the Get Code get codebutton</p>
<pre><code>&lt;a class=&quot;weatherwidget-io&quot; href=&quot;https://forecast7.com/en/40d71n74d01/new-york/&quot; data-label_1=&quot;NEW YORK&quot; data-label_2=&quot;WEATHER&quot; data-theme=&quot;original&quot; &gt;NEW YORK WEATHER&lt;/a&gt;
&lt;script&gt;
!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src='https://weatherwidget.io/js/widget.min.js';fjs.parentNode.insertBefore(js,fjs);}}(document,'script','weatherwidget-io-js');
&lt;/script&gt;
</code></pre>
<p>After copying the code, you cannot paste Notion directly, it can only be a large piece of code, go to</p>
<p><a href="https://apption.co/">Apption - Notion friendly Embeddable Widget Apps</a></p>
<p>Generate url, put the url into the embed block of notion</p>
<p><img src="tools/2023-01-19-13-05-47.png" alt="" /></p>
<h2 id="current-calendar-component"><a class="header" href="#current-calendar-component">current calendar component</a></h2>
<p><a href="https://widgetbox.app/">WidgetBox - Notion Widgets</a></p>
<p><img src="tools/2023-01-19-13-08-20.png" alt="" /></p>
<p>After configuring the appearance, copy the url and put the url in the embed block of notion.</p>
<p><img src="tools/2023-01-19-13-08-37.png" alt="" /></p>
<h2 id="the-remaining-power-of-life--components"><a class="header" href="#the-remaining-power-of-life--components">The remaining power of life 🔋 components</a></h2>
<p><a href="https://indify.co/">Indify - Notion Widgets</a></p>
<p><img src="tools/2023-01-19-13-09-01.png" alt="" /></p>
<p>After configuring the appearance, copy the url and put the url in the embed block of notion</p>
<p><img src="tools/2023-01-19-13-09-13.png" alt="" /></p>
<h2 id="current-time-component"><a class="header" href="#current-time-component">current time component</a></h2>
<p><a href="https://widgetbox.app/">WidgetBox - Notion Widgets</a></p>
<p><img src="tools/2023-01-19-13-09-38.png" alt="" /></p>
<p>After configuring the appearance, copy the url and put the url in the embed block of notion.</p>
<p><img src="tools/2023-01-19-13-09-56.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2023-01-18 18:37</p>
<p>This article is not about the use of git, nor the source code of git, but reading this article will help you understand the underlying logic design of git. This article is about a folder, specifically the hidden folder <code>.git</code> of the local repository in the git distributed repository.</p>
<h1 id="the-structure-of-the-git-directory"><a class="header" href="#the-structure-of-the-git-directory">The structure of the .git directory</a></h1>
<pre><code>[root@iZ23nrc95u7Z ~]# mkdir git-dir
[root@iZ23nrc95u7Z ~]# cd git-dir/
[root@iZ23nrc95u7Z git-dir]# git init
Initialized empty Git repository in /root/git-dir/.git/
[root@iZ23nrc95u7Z git-dir]# ls -a
.  ..  .git
</code></pre>
<p><em>This hidden folder has the following folders and files:</em></p>
<p><strong>hooks: is the directory that stores git hooks, which are scripts that are triggered when specific events occur. For example: before submitting, after submitting.</strong></p>
<pre><code>[root@iZ23nrc95u7Z git-dir]# cd .git/hooks/
[root@iZ23nrc95u7Z hooks]# ls
applypatch-msg.sample  commit-msg.sample  post-update.sample  pre-applypatch.sample  pre-commit.sample  prepare-commit-msg.sample  pre-push.sample  pre-rebase.sample  update.sample
</code></pre>
<p><strong>objects: It is an object library that stores various objects and contents of git.</strong></p>
<p>The file name is the SHA-1 hash checksum of the content of the workspace file managed by git, with 40 digits. The first two digits are used as the folder name, and the last 38 digits are used as the file name. The first 2 bits as folders are for quick indexing.</p>
<p>Because the checksum is based on the content of the file, the .git directory will not be saved repeatedly. For example, if a new branch is created based on a certain branch, duplicate files will not be saved repeatedly, and the modified file is also saved incremental information.</p>
<p>There are several types of information stored in the files in the directory objects, commit, tree (workspace folder information), blob (workspace file information).</p>
<p><strong>refs: is the directory that stores various git references, including branches, remote branches and tags.</strong></p>
<p>There are several directories under the refs directory: heads, remotes, tags</p>
<p>The heads directory stores the head information of the local branch. There are several branches in the local area corresponding to the files of several branch names. The content of the file is the commit id corresponding to the head of the branch.</p>
<p>remotes is the branch and head information of the remote repository.</p>
<p>The tags directory stores the tag and head information of the local repository. A tag corresponds to a file with the same name as the tag, and the content of the file is the commit id corresponding to the tag.</p>
<p><strong>config: is the configuration file at the code base level.</strong></p>
<p>The information in this file is exactly the same as that displayed by the git config –local -l command.</p>
<p>The information configured by the git config –local command will be added to this file. For example git config --local user.name &quot;Your Name Here&quot;.</p>
<p>By modifying this file, the same effect as the git config –local command configuration can be achieved.</p>
<p><strong>HEAD: is the branch the codebase is currently pointing to.</strong></p>
<p>This file records the current branch information, eg ref: refs/heads/main.</p>
<p>If you use the git checkout command to switch branches, the information stored in the file will change accordingly.</p>
<p>If you modify this file, you can also achieve the same effect of switching branches as the git checkout command.</p>
<p><strong>description: This file is used by GitWeb.</strong></p>
<p>GitWeb is a CGI script (Common Gateway Interface, which is simply a program running on a web server, but triggered by browser input), allowing users to view git content on a web page. If we want to start GitWeb, we can use the following command:</p>
<p>git instaweb --start</p>
<p>Serve and open the browser http://127.0.0.1:1234, the page directly displays the current git repository name and description, the default description is as follows:</p>
<p>Unnamed repository; edit this file 'description' to name the repository.</p>
<p>The above output info is the content of the default description file. Editing this file will make the GitWeb description more friendly.</p>
<h1 id="the-objects-folder-in-the-git-directory-changes-after-common-git-command-operations"><a class="header" href="#the-objects-folder-in-the-git-directory-changes-after-common-git-command-operations">The objects folder in the .git directory changes after common git command operations</a></h1>
<p><strong>Add a file to the temporary storage area, changes in the objects folder</strong></p>
<pre><code>[root@iZ23nrc95u7Z git-dir]# ls
[root@iZ23nrc95u7Z git-dir]# tree .git/objects/
.git/objects/
├── info
└── pack

2 directories, 0 files
[root@iZ23nrc95u7Z git-dir]# echo &quot;this is file1&quot; &gt; file1.md
[root@iZ23nrc95u7Z git-dir]# ls
file1.md
[root@iZ23nrc95u7Z git-dir]# cat file1.md
this is file1
[root@iZ23nrc95u7Z git-dir]# git add file1.md
[root@iZ23nrc95u7Z git-dir]# tree .git/objects/
.git/objects/
├── 43
│   └── 3eb172726bc7b6d60e8d68efb0f0ef4e67a667
├── info
└── pack

3 directories, 1 file
[root@iZ23nrc95u7Z git-dir]# git ls-files --stage
100644 433eb172726bc7b6d60e8d68efb0f0ef4e67a667 0	file1.md
[root@iZ23nrc95u7Z git-dir]# git cat-file -t 433eb172726bc7b6d60e8d68efb0f0ef4e67a667
blob
[root@iZ23nrc95u7Z git-dir]# git cat-file -p 433eb172726bc7b6d60e8d68efb0f0ef4e67a667
this is file1
</code></pre>
<p><strong>Add a folder and create a new file in the folder, and save it to the temporary storage area, the changes in the objects folder</strong></p>
<pre><code>[root@iZ23nrc95u7Z git-dir]# ls
file1.md
[root@iZ23nrc95u7Z git-dir]# mkdir dir2
[root@iZ23nrc95u7Z git-dir]# echo &quot;this is file2&quot; &gt; dir2/file2.md
[root@iZ23nrc95u7Z git-dir]# tree
.
├── dir2
│   └── file2.md
└── file1.md

1 directory, 2 files
[root@iZ23nrc95u7Z git-dir]# git add dir2
[root@iZ23nrc95u7Z git-dir]# tree .git/objects/
.git/objects/
├── 43
│   └── 3eb172726bc7b6d60e8d68efb0f0ef4e67a667
├── f1
│   └── 38820097c8ef62a012205db0b1701df516f6d5
├── info
└── pack

4 directories, 2 files
[root@iZ23nrc95u7Z git-dir]# git ls-files --stage
100644 f138820097c8ef62a012205db0b1701df516f6d5 0	dir2/file2.md
100644 433eb172726bc7b6d60e8d68efb0f0ef4e67a667 0	file1.md
[root@iZ23nrc95u7Z git-dir]# git cat-file -t 433eb172726bc7b6d60e8d68efb0f0ef4e67a667
blob
[root@iZ23nrc95u7Z git-dir]# git cat-file -p 433eb172726bc7b6d60e8d68efb0f0ef4e67a667
this is file1
[root@iZ23nrc95u7Z git-dir]# git cat-file -t f138820097c8ef62a012205db0b1701df516f6d5
blob
[root@iZ23nrc95u7Z git-dir]# git cat-file -p f138820097c8ef62a012205db0b1701df516f6d5
this is file2
</code></pre>
<p><strong>Submit content:</strong></p>
<p>git commit Submits the contents of the temporary storage area to the local warehouse.</p>
<pre><code>[root@iZ23nrc95u7Z git-dir]# git commit -m 'add file1.md and dir2/file2.md'
[master (root-commit) b2d176a] add file1.md and dir2/file2.md
2 files changed, 2 insertions(+)
create mode 100644 dir2/file2.md
create mode 100644 file1.md
[root@iZ23nrc95u7Z git-dir]# tree .git/objects/
.git/objects/
├── 01
│   └── 54de200dfc71f16a7847f293d79b42b43995e6
├── 20
│   └── d1d441485e8a194976cba515754890bffb32ad
├── 43
│   └── 3eb172726bc7b6d60e8d68efb0f0ef4e67a667
├── b2
│   └── d176a8c3f1d4f608b722cc3b79bdeeaebbdf06
├── f1
│   └── 38820097c8ef62a012205db0b1701df516f6d5
├── info
└── pack

7 directories, 5 files
</code></pre>
<p>It is found that after the git commit command, in addition to the original two file blob objects, there are three more objects, namely the commit object, the commit as a whole tree object, and the tree object of the folder dir2 where file2.md is located.</p>
<pre><code>[root@iZ23nrc95u7Z git-dir]# git cat-file -t 0154de200dfc71f16a7847f293d79b42b43995e6
tree
[root@iZ23nrc95u7Z git-dir]# git cat-file -p 0154de200dfc71f16a7847f293d79b42b43995e6
040000 tree 20d1d441485e8a194976cba515754890bffb32ad	dir2
100644 blob 433eb172726bc7b6d60e8d68efb0f0ef4e67a667	file1.md
[root@iZ23nrc95u7Z git-dir]# git cat-file -t 20d1d441485e8a194976cba515754890bffb32ad
tree
[root@iZ23nrc95u7Z git-dir]# git cat-file -p 20d1d441485e8a194976cba515754890bffb32ad
100644 blob f138820097c8ef62a012205db0b1701df516f6d5	file2.md
[root@iZ23nrc95u7Z git-dir]# git cat-file -t b2d176a8c3f1d4f608b722cc3b79bdeeaebbdf06
commit
[root@iZ23nrc95u7Z git-dir]# git cat-file -p b2d176a8c3f1d4f608b722cc3b79bdeeaebbdf06
tree 0154de200dfc71f16a7847f293d79b42b43995e6
author gitusername &lt;gitusermail&gt; 1674019979 +0800
committer gitusername &lt;gitusermail&gt; 1674019979 +0800

add file1.md and dir2/file2.md
[root@iZ23nrc95u7Z git-dir]# git log
commit b2d176a8c3f1d4f608b722cc3b79bdeeaebbdf06
Author: gitusername &lt;gitusermail&gt;
Date:   Wed Jan 18 13:32:59 2023 +0800

    add file1.md and dir2/file2.md
[root@iZ23nrc95u7Z git-dir]# cat .git/COMMIT_EDITMSG
add file1.md and dir2/file2.md
</code></pre>
<p>After commit, continue to modify the file content, or add new files and folders. Whether it is content modification or new creation, if the checksum changes, there will be more corresponding objects in the .git/objects directory, and if you commit again, there will be more commit objects and the entire tree objects of the commit.</p>
<p>So even if there are multiple versions, git will not save multiple copies of the same file content, but only the original file and incremental content. Each commit version has a clear structural snapshot, which can be restored to any commit. Create a new branch and commit in other branches. Through the branch information file and the HEAD information file of the branch, the commit history of different branches can be calculated.</p>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2016-07-05 22:57</p>
<h1 id="paste"><a class="header" href="#paste">Paste</a></h1>
<p>For example, openstack vm_id:</p>
<pre><code>424288e4-23a7-45de-bb5d-742bd6c54561
</code></pre>
<p>If the default delimiter is used, double-clicking can only select a part, you need to press and hold the mouse to drag, or more or less, or it will take a little time.</p>
<p>After changing the settings:</p>
<p>In the &quot;Keyboard and Mouse&quot; tab of &quot;Options&quot;</p>
<ol>
<li>Remove the &quot;-&quot; in the delimiter.</li>
<li>Check &quot;Automatically copy the selected text to the clipboard&quot; and vm_id can be selected by double-clicking. There is no need to select copy and paste. It has been copied at the same time as it is selected. At this time, only the middle mouse button is needed to complete the paste.</li>
</ol>
<h1 id="split-screen"><a class="header" href="#split-screen">Split Screen</a></h1>
<p>Only 5.0 or above is supported, and it can be realized by dragging the label to a certain position on the screen with the mouse. It is useful in some cases, such as analyzing networks and comparing jos.</p>
<h1 id="multi-level-jump"><a class="header" href="#multi-level-jump">multi-level jump</a></h1>
<p>In the internal environment of an enterprise, not every node has an external network ip, often through the Jump Server, and then the Jump Server login to other nodes. In a complex environment, there may be more than two levels of login, or even three or more levels of login. It can be easily achieved with xshell.</p>
<p>When creating a new session, or click the properties of the created session, select &quot;Login Script&quot; in &quot;Connection&quot; in &quot;Category&quot;</p>
<p>Select the &quot;Execute the following wait and send rules&quot; check box to activate the two columns below Expect and Send to display interactive functions similar to tcl's expect or python pexpect package.</p>
<table><thead><tr><th>Expect</th><th>Send</th></tr></thead><tbody>
<tr><td>$</td><td>ssh deployer@xx.xx.xx.xx</td></tr>
<tr><td>password:</td><td>xxxxxx</td></tr>
</tbody></table>
<p>The above is two levels of login, and more levels of login can be added later.</p>
<h1 id="tunnel-forwarding"><a class="header" href="#tunnel-forwarding">tunnel forwarding</a></h1>
<p>Select &quot;Tunnel&quot; in &quot;SSH&quot; in &quot;Connection&quot; in &quot;Category&quot; in the properties of the session.</p>
<p>Two commonly used methods Local (Outgoing) and Dynamic (SOCKS4/5)</p>
<p>Take access to the openstack dashboard on the intranet as an example:</p>
<h2 id="localoutgoing"><a class="header" href="#localoutgoing">Local(Outgoing)</a></h2>
<pre><code>(http)
Source host: localhost
Listening port: xx
Destination host: xx.xx.xx.xx
Destination port: 80
(novnc)
Source host: localhost
Listening port: xx
Destination host: xx.xx.xx.xx
Destination port : 6080
</code></pre>
<p>The browser does not need to set up a proxy when accessing, just need to enter <code>http://localhost:port</code> in the address bar.</p>
<h2 id="dynamicsocks45"><a class="header" href="#dynamicsocks45">Dynamic(SOCKS4/5)</a></h2>
<p>Listening port: xx</p>
<p>You need to set SOCKS4 or SOCKS5 proxy when accessing the browser, and you need to enter the url address of the intranet in the address bar.</p>
<p>General browser supports SOCKS4/5 proxy, also you can use some proxy tool such as the Proxy SwitchyOmega plug-in of chrome.</p>
<p>The two tunnel forwarding methods have their own characteristics. I always use the latter, because there are few settings. If you want to access other ports or other services, you only need to set up the jump server.</p>
<h1 id="application-scenario-1"><a class="header" href="#application-scenario-1">Application Scenario 1</a></h1>
<p>Use the XShell tunnel to connect to the intranet machine through the springboard machine. The springboard machine can be accessed from the public network or through the local area network, but the internal network node public network or local area network cannot be directly accessed.</p>
<p>In this case, access can be achieved by establishing a tunnel through Local (Outgoing) and Dynamic (SOCKS4/5) methods.</p>
<h1 id="application-scenario-2"><a class="header" href="#application-scenario-2">Application Scenario 2</a></h1>
<p>A company is connected to another company's network, or a laptop in a location needs to be connected to a computer in a closed network (a city's office location is connected to another city's closed data center), and it can be remotely transmitted through the Remote (Incoming) method Establish a tunnel to achieve access.
Of course, the second application scenario is often realized by using teamviewer software. Using teamviewer has many disadvantages, such as charging software, charging traffic, and occupying 100% of the screen resources of a computer. Use the Remote (Incoming) method to establish a tunnel, free of charge, save traffic, run in the background, and the computer can be used for other work. The disadvantage is that it is not as comprehensive as teamviewer. The connection established by the tunnel is mainly used for ssh purposes.</p>
<p>Remote incoming Remote (Incoming)
'Local' is used to map the service on the server to a certain port of the local machine, while 'Remote' is the opposite, it maps the local port to a certain port of the server, and accessing the port on the server is actually accessing the local port. machine service. It is similar to this: the port mapping mode of the firewall, and the difference is that the former is to open a hole in the company's firewall, which is a legal operation; the latter is to dig a hole, which is an illegal operation without the need for network administrators . The latter may bring the risk of leaks to the company, especially if the ssh port mapping is done, the entire internal network of the company can be accessed from the external server!</p>
<p>After the tunnel is established through Remote (Incoming), the computer of Company A can access the resources of Company B.</p>
<p>Specific steps:</p>
<ol>
<li>Tunnel establishment</li>
</ol>
<ol>
<li>From the computer of company B, ssh to the cloud host or physical machine where the office computers of both companies are reachable</li>
<li>Configure the tunnel</li>
</ol>
<ol start="2">
<li>
<p>144.85.93 is a node in company B LAN</p>
</li>
<li>
<p>Access process</p>
</li>
</ol>
<ol>
<li>From the computer of company A, ssh to the cloud host or physical machine where the office computers of both companies are reachable</li>
<li>After successful login, execute ssh 127.0.0.1 -p 2345 to complete the login from the computer of company A The whole process of company B LA</li>
</ol>
<h1 id="application-scenario-3"><a class="header" href="#application-scenario-3">Application Scenario 3</a></h1>
<p>data center. This scenario is similar to Scenario 2, except that Company B is replaced by a data center.
It's just that the conditions of the data center may not be as good as the company's office environment, and the network cannot be pulled casually. So at this time, you can use a notebook (the role of the notebook is equivalent to the office computer of company B in scenario 2). Use the dual network cards of the notebook, connect the wireless network card to the hotspot of the mobile phone, and connect the wired network card to the computer room of the data center. Other steps are exactly the same as Scenario 2. It can achieve the purpose of remotely connecting to the data center in other regions or other companies.</p>
<p><strong>Notebook dual network card configuration</strong></p>
<p>Need to modify the route of windows
route add 10.220.0.0 mask 255.255.0.0 192.168.1.1 -p
route delete 0.0.0.0 mask 0.0.0.0 192.168.1.1
The above two commands are to delete the default route through the wired network card, and the default route uses the wireless network card; Access 10.220.<em>.</em> (the network segment of the server in the data center) via a wired network card</p>
<p>In this way, the notebook can connect to the Internet and the data center at the same time, but during use, it is found that the default route through the wired network card will be automatically added after a period of time, resulting in the inability to access the Internet. Copy the following content into the newly created file, the suffix of the file name is bat, and then execute the bat file (used to periodically delete the default route of the wired network card)</p>
<pre><code>@echo off 

route delete 10.220.0.0 mask 255.255.0.0
route add 10.220.0.0 mask 255.255.0.0 192.168.1.1

:start
route delete 0.0.0.0 mask 0.0.0.0 192.168.1.1
choice /t 5 /d y /n &gt;nul

goto start</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>release time :2017-06-28 15:51</p>
<p><img src="tools/2023-01-17-13-39-31.png" alt="" /></p>
<p>Gerrit is the current mainstream code review tool. It directly adds a layer of code review verification to code push and code merge, and it is also an important link and tool in the continuous integration tool chain.</p>
<p>The gerrit deployment process is as follows:</p>
<ol>
<li>Install jdk1.8</li>
<li>install httpd</li>
</ol>
<p>Download gerrit, such as: gerrit-2.12.4.war</p>
<pre><code>Gerrit 2.12.4 https://www.gerritcodereview.com/download/gerrit-2.12.4.war
</code></pre>
<p>gerrit management account</p>
<pre><code>sudo adduser gerrit
sudo passwd gerrit
</code></pre>
<p>And add gerrit to sudo permission</p>
<pre><code>sudo visudo
gerrit  ALL=(ALL:ALL) ALL
</code></pre>
<p>install gerrit</p>
<pre><code>java -jar gerrit-2.11.3.war init
</code></pre>
<p>Start the gerrit service</p>
<pre><code>[gerrit@promote review2]$ /etc/init.d/gerrit.sh start
** ERROR: GERRIT_SITE not set
[gerrit@promote review2]$ pwd
/home/gerrit/review2
[gerrit@promote review2]$ /etc/init.d/gerrit.sh start
** ERROR: GERRIT_SITE not set
[gerrit@promote review2]$ export GERRIT_SITE=/home/gerrit/review2
[gerrit@promote review2]$ /etc/init.d/gerrit.sh status
Checking arguments to Gerrit Code Review:
GERRIT_SITE     =  /home/gerrit/review2
GERRIT_CONFIG   =  /home/gerrit/review2/etc/gerrit.config
GERRIT_PID      =  /home/gerrit/review2/logs/gerrit.pid
GERRIT_TMP      =  /home/gerrit/review2/tmp
GERRIT_WAR      =  /home/gerrit/review2/bin/gerrit.war
GERRIT_FDS      =  1024
GERRIT_USER     =  gerrit
JAVA            =  /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.141-2.6.10.1.el7_3.x86_64/jre/bin/java
JAVA_OPTIONS    =  
RUN_EXEC        =  /usr/bin/perl -e '$x=$ENV{JAVA};exec $x @ARGV;die $!' -- GerritCodeReview
RUN_ARGS        =  -jar /home/gerrit/review2/bin/gerrit.war daemon -d /home/gerrit/review2

[gerrit@promote review2]$ /etc/init.d/gerrit.sh start
Starting Gerrit Code Review: OK
[root@promote etc]# cd /etc/httpd/
[root@promote httpd]# ls
conf  conf.d  conf.modules.d  logs  modules  run
[root@promote httpd]# cd conf.d/
[root@promote conf.d]# ls
autoindex.conf  gerrit.conf  README  userdir.conf  welcome.conf
</code></pre>
<p>Modify the configuration file as needed</p>
<pre><code>[root@promote conf.d]# vim gerrit.conf 
</code></pre>
<p>Configure the gerrit account password</p>
<pre><code>[root@promote ~]# htpasswd -m /etc/gerrit.passwd hanwei
htpasswd: cannot modify file /etc/gerrit.passwd; use '-c' to create it
[root@promote ~]# touch /etc/gerrit.passwd
[root@promote ~]# htpasswd -m /etc/gerrit.passwd hanwei
New password: 
Re-type new password: 
Adding password for user hanwei
</code></pre>
<p>Open the browser, open the main interface of gerrit, and log in to gerrit with the account just created.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="sidebar.js"></script>
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>